<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[transient]]></title>
    <url>%2F2019%2F06%2F01%2FJava%2Ftransient%2F</url>
    <content type="text"><![CDATA[对于实现Serializable接口的类，属性前添加关键字transient，序列化对象的时候，这个属性就不会被序列化 1transient Object[] elementData; 注意点 要使transient生效，类需要实现Serializable接口 transient关键字只能修饰变量，不能修饰方法和类 静态变量不管是否被transient修饰，均不能被序列化，反序列化后对象中static型变量的值为当前JVM中对应类中static变量的值]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>transient</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F28%2F%E5%88%86%E5%B8%83%E5%BC%8F%2Fweb%2F</url>
    <content type="text"><![CDATA[cookie通过 Document.cookie 属性可创建新的 Cookie，也可通过该属性访问非 HttpOnly 标记的 Cookie。 12document.cookie = "token=123";console.log(document.cookie); 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。跨站脚本攻击 (XSS) 常常使用 JavaScript 的 Document.cookie API 窃取用户的 Cookie 信息，因此使用 HttpOnly 标记可以在一定程度上避免 XSS 攻击。 除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。 session Session 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。 session就是一种保存上下文信息的机制，它是针对每一个用户的，变量的值保存在服务器端，通过SessionID来区分不同的客户,session是以cookie或URL重写为基础的，默认使用cookie来实现，系统会创造一个名为JSESSIONID的输出cookie，我们叫做session cookie,以区别persistent cookies,也就是我们通常所说的cookie，当我们把浏览器的cookie禁止后，web服务器会采用URL重写的方式传递Sessionid 通常session cookie是不能跨窗口使用的，当你新开了一个浏览器窗口进入相同页面时，系统会赋予你一个新的sessionid，这样我们信息共享的目的就达不到了，此时我们可以先把sessionid保存在persistent cookie中，然后在新窗口中读出来，就可以得到上一个窗口SessionID了，这样通过session cookie和persistent cookie的结合我们就实现了跨窗口的session tracking（会话跟踪）。 使用 Session 维护用户登录状态的过程如下： 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID；通过SessionID来区分不同的客户 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。 应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fspark%2F</url>
    <content type="text"><![CDATA[分布式 内存 迭代计算 RDD是Spark提供的核心抽象，全称为Resillient DistributedDataset，即弹性分布式数据集 也是副本机制 坏了能还原 默认放内存 不够放磁盘]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[《利用zookeeper实现分布式leader节点选举》 《基于zookeeper实现统一配置管理》 Zookeeper是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应 zoo.cfg 1）tickTime：通信心跳数，Zookeeper服务器心跳时间，单位毫秒Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit：LF初始通信时限集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。投票选举新leader的初始化时间Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。Leader允许F在initLimit时间内完成这个工作。 3）syncLimit：LF同步通信时限集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 1）Znode有两种类型： 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode有四种形式的目录节点（默认是persistent ） （1）持久化目录节点（PERSISTENT） ​ 客户端与zookeeper断开连接后，该节点依旧存在 （2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） ​ 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 （3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除 （4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 3）创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 半数以上就能对外服务 实现 hadoop -ha]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fhbase%2F</url>
    <content type="text"><![CDATA[HDFS12vi etc/hadoop/hadoop-env.sh export JAVA_HOME=vi etc/hadoop/core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node:8020&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;&lt;/property&gt; 1vi etc/hadoop/hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/data/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 12345# 格式化HDFSbin/hdfs namenode -format# 启动HDFS 查看 http://node:50070/sbin/start-dfs.shvi /etc/profile 123export HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin YARN架构 ResourceManager的职责： 一个集群active状态的RM只有一个，负责整个集群的资源管理和调度1）处理客户端的请求(启动/杀死)2）启动/监控ApplicationMaster(一个作业对应一个AM)3）监控NM4）系统的资源分配和调度 NodeManager：整个集群中有N个，负责单个节点的资源管理和使用以及task的运行情况1）定期向RM汇报本节点的资源使用请求和各个Container的运行状态2）接收并处理RM的container启停的各种命令3）单个节点的资源管理和任务管理 ApplicationMaster：每个应用/作业对应一个，负责应用程序的管理1）数据切分2）为应用程序向RM申请资源(container)，并分配给内部任务3）与NM通信以启停task， task是运行在container中的4）task的监控和容错 Container：对任务运行情况的描述：cpu、memory、环境变量 YARN执行流程1）用户向YARN提交作业2）RM为该作业分配第一个container(AM)3）RM会与对应的NM通信，要求NM在这个container上启动应用程序的AM4) AM首先向RM注册，然后AM将为各个任务申请资源，并监控运行情况5）AM采用轮训的方式通过RPC协议向RM申请和领取资源6）AM申请到资源以后，便和相应的NM通信，要求NM启动任务7）NM启动我们作业对应的task YARN12cp mapred-site.xml.template mapred-site.xmlvi mapred-site.xml 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 1vi yarn-site.xml 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 12# 启动 访问http://192.168.10.104:8088/clustersbin/start-yarn.sh HiveHive底层的执行引擎有：MapReduce、Tez、Spark 12cp hive-default.xml.template hive-site.xmlvi hive-site.xml &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/steambuy?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; 上传mysql驱动到lib 12cp hive-env.sh.template hive-env.shvi hive-config.sh 123export HADOOP_HOME=/usr/local/hadoopexport JAVA_HOME=/usr/java/jdkexport HIVE_HOME=/usr/local/hive /usr/local/hadoop/sbin/start-all.sh 启动hadoop集群 bin/hive create table tb_user(id int, name string) ; load data local inpath ‘/usr/local/userdata.txt’ into table tb_user; select name from tb_user; Spark1vi /etc/profile 123export SPARK_HOME=/usr/local/sparkexport PATH=$SPARK_HOME/binexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib 1cp spark-env.sh.template spark-env.sh 1234export JAVA_HOME=/usr/local/jdkexport SPARK_MASTER_IP=192.168.10.104export SPARK_WORKER_MEMORY=1gexport HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop 12# 启动 访问http://192.168.10.104:8080/sbin/start-all.sh]]></content>
  </entry>
  <entry>
    <title><![CDATA[简单的JDBC]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2F%E7%AE%80%E5%8D%95%E7%9A%84JDBC%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536public class Conn &#123; static Connection connection; static Statement stat; static ResultSet rs; public Connection getConnection() &#123; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; Connection conn=null; try &#123; conn=DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/mall&quot;, &quot;root&quot;, &quot;Gepoint&quot;); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return conn; &#125; public static void main(String[] args) &#123; Conn c=new Conn(); connection=c.getConnection(); try &#123; stat=connection.createStatement(); rs=stat.executeQuery(&quot;select * from spbrands limit 10&quot;); while(rs.next()) &#123; String brand=rs.getString(&quot;Brand&quot;); System.out.println(&quot;品牌:&quot;+brand); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数组]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[数组初始化123int[] arr1= new int[5];int[] arr2= &#123;1,2,3,4,5&#125;;int[] arr3= new int[] &#123;1,2,3,4,5&#125;; Arrays method description Arrays.fill(arr, val); 替换数组的值 Arrays.fill(arr, fromIndex, toIndex, val); 替换数组下标[fromIndex,toIndex)的值 Arrays.copyOf(arr, newLength); 拷贝数组，先数组长度为newLength Arrays.copyOfRange(arr, from, to); 拷贝数组下标[from,to)的值 Arrays.toString(arr); 打印数组 Arrays.binarySearch(arr, val); 二分查找，返回下标 System.arraycopy(src, srcPos, dest, destPos, length); src原数组，srcPos起始index，dest目标数组，despos起始位置，length拷贝长度，属于浅拷贝 Arrays.sort(arr); 排序 Arrays.equals(arr1, arr2); 比较两个数据是否相等 Collections.sort方法底层就是调用的Array.sort 123456789101112131415161718192021222324252627282930313233343536373839404142434445static &lt;T&gt; void sort(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c, T[] work, int workBase, int workLen) &#123; assert c != null &amp;&amp; a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) return; // array的大小为0或者1就不用排了 // 当数组大小小于MIN_MERGE(32)的时候，就用一个"mini-TimSort"的方法排序 if (nRemaining &lt; MIN_MERGE) &#123; // 将最长的递减序列，找出来，然后倒过来 int initRunLen = countRunAndMakeAscending(a, lo, hi, c); // 长度小于32的时候，是使用binarySort的 binarySort(a, lo, hi, lo + initRunLen, c); return; &#125; // 先扫描一次array，找到已经排好的序列，然后再用刚才的mini-TimSort，然后合并 TimSort&lt;T&gt; ts = new TimSort&lt;&gt;(a, c, work, workBase, workLen); int minRun = minRunLength(nRemaining); do &#123; // Identify next run int runLen = countRunAndMakeAscending(a, lo, hi, c); // If run is short, extend to min(minRun, nRemaining) if (runLen &lt; minRun) &#123; int force = nRemaining &lt;= minRun ? nRemaining : minRun; binarySort(a, lo, lo + force, lo + runLen, c); runLen = force; &#125; // Push run onto pending-run stack, and maybe merge ts.pushRun(lo, runLen); ts.mergeCollapse(); // Advance to find next run lo += runLen; nRemaining -= runLen; &#125; while (nRemaining != 0); // Merge all remaining runs to complete sort assert lo == hi; ts.mergeForceCollapse(); assert ts.stackSize == 1; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java8 feature]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FJava8%20feature%2F</url>
    <content type="text"><![CDATA[LambdaLambda 表达式是Java 8最重要的新特性。Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中），可以使代码变的更加简洁紧凑。 1(参数列表) -&gt; &#123;代码块&#125; tips: 参数类型可省略，编译器可以自己推断 如果只有一个参数，圆括号可以省略 代码块如果只是一行代码，大括号也可以省略 如果代码块是一行，且是有结果的表达式，return可以省略 事实上，把Lambda表达式可以看做是匿名内部类的一种简写方式。当然，前提是这个匿名内部类对应的必须是接口，而且接口中必须只有一个函数！Lambda表达式就是直接编写函数的：参数列表、代码体、返回值等信息。 用法示例 12345678910111213141516public class Lambda &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list=new ArrayList&lt;&gt;(Arrays.asList(14,213,45)); list.forEach(System.out::println);//遍历 list.sort(Comparator.comparingInt(Integer::intValue));//升序排序 list.sort((e1,e2)-&gt;&#123;return e2-e1;&#125;);//降序排序 list.sort((e1,e2)-&gt; e2-e1);//简化 list.forEach(System.out::println); final String name="world"; new Thread(() -&gt; System.out.println("hello"+name)).start();//当成匿名内部类赋值赋值 //name="zhangsan";//Variable used in lambda expression should be final or effectively final &#125;&#125; Lambda表达式的实质其实还是匿名内部类，而匿名内部类在访问外部局部变量时，要求变量必须声明为final。不过我们在使用Lambda表达式时无需声明final，因为Lambda底层会隐式的把变量设置为final，在后续的操作中，一定不能修改该变量。// 在Lambda表达式中使用局部变量num，num会被隐式声明为final，不能进行任何修改操作 Runnable、Comparator都是函数式接口的典型代表。但是在实践中，函数接口是非常脆弱的，只要有人在接口里添加多一个方法，那么这个接口就不是函数接口了，就会导致编译失败。Java 8提供了一个特殊的注解@FunctionalInterface来克服上面提到的脆弱性并且显示地表明函数接口。而且jdk8版本中，对很多已经存在的接口都添加了@FunctionalInterface注解，例如Runnable接口，另外，Jdk8默认提供了一些函数式接口供我们使用 接口 用法 Function 接收一个参数T，返回一个结果R Consumer 接收T类型参数，不返回结果 Predicate 接收T类型参数，返回boolean类型结果 Supplier 不接收参数，返回一个T类型结果 StreamJava8的新增的Stream API（java.util.stream）将生成环境的函数式编程引入了Java库中。这是目前为止最大的一次对Java库的完善，以便开发者能够写出更加有效、更加简洁和紧凑的代码。Steam API极大得简化了集合操作，Stream流配合Lambda表达式使用起来真是美滋滋，steam的另一个价值是创造性地支持并行处理。 优点：性能高，灵活，表达清楚 api分为两大类，中间操作相当于流水线，能对流水线上的东西进行各种处理，不会消耗流；终端操作消耗流，不能再对流进行操作 中间操作(流水线) filter 过滤 map 抽取流 limit 限制个数 skip 跳过个数 sorted 排序 distinct 去重 flagmap 合并到一起 终端操作(消耗流) collect 收集 foreach 遍历 count 计数 anyMatch 至少匹配1个 allMatch 匹配所有 noneMatch 没有元素匹配 findAny 返回流中任意元素 reduce 举例12345678910111213141516171819202122232425import java.util.ArrayList;import java.util.Comparator;import java.util.List;import java.util.Map;import java.util.stream.Collectors;public class StreamTest &#123; public static void main(String[] args) &#123; List&lt;Apple&gt; appleList=new ArrayList&lt;&gt;(); appleList.parallelStream().filter((e)-&gt;e.getWeight()&gt;50) // 筛选重量&gt;50的苹果 .sorted(Comparator.comparing(Apple::getColor)) // 根据苹果的颜色进行排序 .map(Apple::getWeight) // 抽取苹果中的重量 .limit(3) // 限定选3个 .distinct() // 去掉相同的元素 .collect(Collectors.toList()); // 转化成list appleList.forEach(System.out::println); // 打印每个苹果 appleList.stream().count(); // 计算苹果的总个数 // 对list根据苹果的颜色进行分组 Map&lt;String,List&lt;Apple&gt;&gt; map=appleList.parallelStream().collect(Collectors.groupingBy(Apple::getColor)); &#125;&#125; flagmap合并流 123456789101112public class StreamTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; names = new ArrayList&lt;&gt;(); names.add("hello"); names.add("world"); List&lt;String&gt; a=names.stream().map((String e)-&gt;e.split("")) .flatMap(Arrays::stream) .collect(Collectors.toList());// 将两个单词，拆成单个字母并存为list System.out.println(a.get(9)); &#125;&#125; reduce 123456789101112public class StreamTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; names = new ArrayList&lt;&gt;(); names.add("hello"); names.add("world"); String newstr=names.stream().map((String e)-&gt;e.split("")) .flatMap(Arrays::stream) .reduce("",String::concat);// 拆成字母后，拼接成一个新的字符串，相当于.reduce("",(String a ,String b)-&gt;a+b); System.out.println(newstr); &#125;&#125; collect 123List&lt;Apple&gt; appleList=new ArrayList&lt;&gt;();appleList.stream().collect(Collectors.averagingInt(Apple::getWeight));// 计算重量的平均值appleList.stream().map(Apple::getColor).collect(Collectors.joining());// 连接字符串 创建流的方式值创建流 1Stream&lt;String&gt; strstream=Stream.of(&quot;dasd&quot;,&quot;dsa&quot;,&quot;das&quot;); 数据创建流 12String[] nums=&#123;&quot;qwe&quot;,&quot;ee&quot;,&quot;ds&quot;&#125;;Stream&lt;String&gt; aaa=Arrays.stream(nums); 文件创建流 123456try &#123; Stream&lt;String&gt; lines=Files.lines(Paths.get(&quot;C:\\Users\\maruami\\Desktop\\readbook\\book.md&quot;), Charset.defaultCharset()); lines.forEach(System.out::println);&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 函数生成流 1Stream.generate(Math::random).limit(10).forEach(System.out::println); OptionalJava应用中最常见的bug就是空指针异常 Optional仅仅是一个容器，可以存放T类型的值或者null。它提供了一些有用的接口来避免显式的null检查，可以参考Java 8官方文档了解更多细节。 method description isPresent 有值返回true 否则返回false get() 有值时返回值 没有抛出异常 orElse 有值时返回值 没有返回默认值 orElseGet 有值时返回值 没有返回一个supplier接口生成的值 map 只存在就执行mapping函数调用,以将现有Optional实例的值转换成新的值 创建Optional1234567Apple apple = new Apple();// 空的optionalOptional&lt;Apple&gt; optApple=Optional.empty(); // 值为nul抛出异常Optional&lt;Apple&gt; optApple1=Optional.of(apple); // 值为null 返回空的optionalOptional&lt;Apple&gt; optApple2=Optional.ofNullable(apple); 并行数组1234567891011public class ParallelArrays &#123; public static void main(String[] args) &#123; int[] arrays=new int[20]; Arrays.parallelSetAll(arrays,e-&gt; ThreadLocalRandom.current().nextInt(100)); System.out.println(Arrays.toString(arrays)); Arrays.parallelSort(arrays); Arrays.stream(arrays).limit(10).forEach(System.out::println); &#125;&#125; 接口的默认方法和静态方法123456789101112131415public interface UserService &#123; /** * 静态方法 */ static void print()&#123; System.out.println("print"); &#125; /** * 默认方法 */ default void eat()&#123; System.out.println("eat rice"); &#125;&#125; 使得可以向现存接口中添加新的默认方法，不强制那些实现了该接口的类也同时实现这个新加的方法。实现了该接口的类可以重写改方法 接口中定义的静态方法，可以直接用接口调用]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[在《阿里巴巴Java开发手册》“并发处理”这一章节，明确指出线程资源必须通过线程池提供，不允许在应用中自行显示创建线程。 为什么要使用线程池？ 线程是稀缺资源，创建开销大，不能频繁的创建，而线程池可以重复利用线程减少开销，提升系统响应速度，减去创建线程的时间 解耦，线程的创建、执行完全分开，便于维护 线程池定义的状态 123456// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 运行状态 可以接受队列里的任务private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// 不在接受新的任务，且队列里的任务要执行完private static final int STOP = 1 &lt;&lt; COUNT_BITS;// 不再接受新任务，且队列里的任务和正在执行的任务都中断private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// 所有任务都执行完毕private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// 终止状态 工作队列 PriorityBlockingQueue优先队列，优先级低的可能永远不能被执行 ArrayBlockingQueue基于数组的先进先出队列，此队列创建时必须指定大小； LinkedBlockingQueue基于链表的先进先出队列，如果创建时没有指定，则默认为Integer.MAX_VALUE； SynchronousQueue这个队列不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 一般如果线程池任务队列采用LinkedBlockingQueue队列的话，那么不会拒绝任何任务（因为队列大小没有限制），这种情况下，ThreadPoolExecutor最多仅会按照最小线程数来创建线程，也就是说线程池大小被忽略了。 如果线程池任务队列采用ArrayBlockingQueue队列的话，那么ThreadPoolExecutor将会采取一个非常负责的算法，比如假定线程池的最小线程数为4，最大为8所用的ArrayBlockingQueue最大为10。随着任务到达并被放到队列中，线程池中最多运行4个线程（即最小线程数）。即使队列完全填满，也就是说有10个处于等待状态的任务，ThreadPoolExecutor也只会利用4个线程。如果队列已满，而又有新任务进来，此时才会启动一个新线程，这里不会因为队列已满而拒接该任务，相反会启动一个新线程。新线程会运行队列中的第一个任务，为新来的任务腾出空间。 拒绝处理任务时的策略 ThreadPoolExecutor.AbortPolicy:直接拒绝所提交的任务并抛出RejectedExecutionException异常 ThreadPoolExecutor.DiscardPolicy：不处理直接丢弃掉任务，不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务 ThreadPoolExecutor.CallerRunsPolicy：只用调用者所在的线程来执行任务 主要方法execute() 12345678910111213141516171819public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); // 获得线程池状态 if (workerCountOf(c) &lt; corePoolSize) &#123;// 小于线程池大小创建一个新的线程运行 if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123;// 线程在运行，且成功进入阻塞队列 int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command))// 双重校验，线程状态，如果线程状态变了，就 reject(command); // 从队列移除任务 else if (workerCountOf(recheck) == 0)// 线程池为空，创建一个新的线程执行 addWorker(null, false); &#125; else if (!addWorker(command, false)) // 尝试新建线程，失败就执行拒绝策略 reject(command); &#125; submit() 也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果 ，利用了Future来获取任务执行结果 shutdown() 用来关闭线程池的 如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕； shutdownNow() 如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试停止所有的正在执行和未执行任务的线程，并返回等待执行任务的列表 线程池任务执行流程 Executors 线程池种类newCachedThreadPool 底层：返回ThreadPoolExecutor实例，corePoolSize为0；maximumPoolSize为Integer.MAX_VALUE；keepAliveTime为60L；unit为TimeUnit.SECONDS；workQueue为SynchronousQueue(同步队列) 通俗：当有新任务到来，则插入到SynchronousQueue中，由于SynchronousQueue是同步队列，因此会在池中寻找可用线程来执行，若有可以线程则执行，若没有可用线程则创建一个线程来执行该任务；若池中线程空闲时间超过指定大小，则该线程会被销毁。 适用：执行很多短期异步的小程序 newFixedThreadPool 底层：返回ThreadPoolExecutor实例，接收参数为所设定线程数量nThread，corePoolSize为nThread，maximumPoolSize为nThread；keepAliveTime为0L(不限时)；unit为：TimeUnit.MILLISECONDS；WorkQueue为：new LinkedBlockingQueue() 无界阻塞队列 通俗：创建可容纳固定数量线程的池子，每隔线程的存活时间是无限的，当池子满了就不在添加线程了；如果池中的所有线程均在繁忙状态，对于新任务会进入阻塞队列中(无界的阻塞队列) 适用：执行长期的任务，性能好很多，适用于可以预测线程数量的业务中，或者服务器负载较重，对当前线程数量进行限制。 newSingleThreadExecutor 底层：FinalizableDelegatedExecutorService包装的ThreadPoolExecutor实例，corePoolSize为1；maximumPoolSize为1；keepAliveTime为0L；unit为：TimeUnit.MILLISECONDS；workQueue为：new LinkedBlockingQueue() 无解阻塞队列 通俗：创建只有一个线程的线程池，且线程的存活时间是无限的；当该线程正繁忙时，对于新任务会进入阻塞队列中(无界的阻塞队列) 适用：一个任务一个任务执行的场景 newScheduledThreadPool 底层：创建ScheduledThreadPoolExecutor实例，corePoolSize为传递来的参数，maximumPoolSize为Integer.MAX_VALUE；keepAliveTime为0；unit为：TimeUnit.NANOSECONDS；workQueue为：new DelayedWorkQueue() 一个按超时时间升序排序的队列 通俗：创建一个固定大小的线程池，线程池内线程存活时间无限制，线程池可以支持定时及周期性任务执行，如果所有线程均处于繁忙状态，对于新任务会进入DelayedWorkQueue队列中，这是一种按照超时时间排序的队列结构 适用：周期性执行任务的场景 newWorkStealingPool 获取当前可用的线程数量进行创建作为并行级别 使用ForkJoinPool 使用一个无限队列来保存需要执行的任务，可以传入线程的数量，不传入，则默认使用当前计算机中可用的cpu数量，使用分治法来解决问题，使用fork()和join()来进行调用 适用：适用于大耗时的操作，可以并行来执行 合理配置线程池参数《阿里巴巴Java开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,如果工作线程数目太少，导致处理跟不上入队的速度，这就很有可能占用大量系统内存，可能堆积大量的请求，从而导致OOM。诊断时，可以使用jmap之类的工具，查看是否有大量的任务对象入队。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，通常在处理大量短时任务时，使用缓存的线程池可能会创建大量线程，如果线程数目不断增长（可以使用jstack等工具检查）因为任务逻辑有问题，导致工作线程迟迟不能被释放（线程泄漏），从而导致OOM。 要想合理的配置线程池，就必须首先分析任务特性，可以从以下几个角度来进行分析 任务性质不同：CPU密集型任务配置尽可能少的线程数量，如配置cpu+1个线程的线程池，减少上下文切换。IO密集型任务则由于需要等待IO操作，线程并不是一直在执行任务，则配置尽可能多的线程，如cpu x 2个，让CPU处理更多的业务。混合型的任务，如果可以拆分，则将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐率要高于串行执行的吞吐率，如果这两个任务执行时间相差太大，则没必要进行分解。可以通过Runtime.getRuntime().availableProcessors()方法获得当前设备的CPU个数 优先级不同的任务：可以使用优先级队列PriorityBlockingQueue来处理。它可以让优先级高的任务先得到执行，需要注意的是如果一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行 任务的执行时间：执行时间不同的任务可以交给不同规模的线程池来处理，或者也可以使用优先级队列，让执行时间短的任务先执行 任务的依赖性：任务依赖其他系统资源，如数据库连接，因为线程提交SQL后需要等待数据库返回结果，如果等待的时间越长CPU空闲时间就越长，那么线程数应该设置越大，这样才能更好的利用CPU 如果提交任务时，线程池队列已满 如果使用的是无界队列LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue可以近乎认为是一个无穷大的队列，可以无限存放任务 如果使用的是有界队列比如ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue满了，会根据maximumPoolSize的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue继续满，那么则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy 注意避免死锁等同步问题，对于死锁的场景和排查尽量避免在使用线程池时操作ThreadLocal 推荐阅读线程池详解 http://www.cnblogs.com/dolphin0520/p/3932921.html 对于线程池感兴趣的可以查看我的这篇文章：《Java多线程学习（八）线程池与Executor 框架》 点击阅读原文即可查看到该文章的最新版。、深入理解Java线程池]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F15%2FJava%2FThreadLocal%2F</url>
    <content type="text"><![CDATA[简介在多线程编程中我们通常会利用synchronzed或者lock控制线程对临界区资源的访问顺序从而解决线程安全的问题，但是这种加锁的方式会让未获取到锁的线程进行阻塞等待。如果每个线程都使用自己的共享资源，不影响到彼此，这样就不会出现线程安全的问题。每个Thread里面维护了一个ThreadLocal.ThreadLocalMap，把数据进行隔离，不共享，自然就没有线程安全方面的问题。这就是一种用空间换时间的做法，每个线程都会都拥有自己的“共享资源”无疑内存会大很多，但是由于不需要同步也就减少了线程可能存在的阻塞等待的情况从而提高的时间效率。 ThreadLocal这个类名可以顾名思义的进行理解，表示线程的“本地变量”，即每个线程都拥有该变量副本，达到人手一份的效果，各用各的这样就可以避免共享资源的竞争。 核心方法void set(T value)123456789101112public void set(T value) &#123; // 获取当前线程实例对象 Thread t = Thread.currentThread(); // 通过当前线程实例获取到ThreadLocalMap对象 ThreadLocalMap map = getMap(t); // 如果map不为null,则以当前threadLocl实例为key,值为value进行存入 if (map != null) map.set(this, value); // map为null,则新建ThreadLocalMap并存入value else createMap(t, value);&#125; 数据value是真正的存放在了ThreadLocalMap这个容器中了，并且是以当前threadLocal实例为key 1234ThreadLocalMap getMap(Thread t) &#123; // 返回的就是当前线程对象t的一个成员变量ThreadLocal.ThreadLocalMap threadLocals = null; return t.threadLocals;&#125; 1234void createMap(Thread t, T firstValue) &#123; // new一个ThreadLocalMap实例对象,当前threadLocal实例作为key,值为value存放到threadLocalMap中 t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; T get()get方法是获取当前线程中threadLocal变量的值 123456789101112131415161718public T get() &#123; // 获取当前线程实例对象 Thread t = Thread.currentThread(); // 通过当前线程实例获取到ThreadLocalMap对象 ThreadLocalMap map = getMap(t); if (map != null) &#123; // 如果map不为null,则获取map中当前threadLocal实例为key的值的entry ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") // 如果entry不为null，就返回相应的值value T result = (T)e.value; return result; &#125; &#125; // 如果map为null或者entry为null，就通过该方法初始化，并返回该方法返回的value return setInitialValue();&#125; 12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 123protected T initialValue() &#123; return null;&#125; 这个方法是protected修饰的，也就是说继承ThreadLocal的子类可重写该方法，实现赋值为其他的初始值 123protected T initialValue() &#123; return null;&#125; void remove()1234567public void remove() &#123; // 获取当前线程的threadLocalMap ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) // 从map中删除以当前threadLocal实例为key的键值对 m.remove(this);&#125; 1234567public void remove() &#123; //1. 获取当前线程的threadLocalMap ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) //2. 从map中删除以当前threadLocal实例为key的键值对 m.remove(this);&#125; ThreadLocalMapThreadLocalMap是ThreadLocal的一个静态内部类，内部维护了一个Entry类型的table数组 Entry123456789101112131415161718192021static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125;/** * The initial capacity -- MUST be a power of two. */private static final int INITIAL_CAPACITY = 16;/** * The table, resized as necessary. * table.length MUST always be a power of two. */// 通过注释可以看出，table数组的长度为2的幂次方private Entry[] table; Entry是一个以ThreadLocal为key,Object为value的键值对，因为Entry继承了WeakReference，在Entry的构造方法中，调用了super(k)方法就会将threadLocal实例包装成一个WeakReferenece。 thread,threadLocal,threadLocalMap，Entry之间的关系图： 每个线程实例中可以通过threadLocals获取到threadLocalMap，而threadLocalMap实际上就是一个以threadLocal实例为key，object为value的Entry数组。当我们为threadLocal变量赋值，实际上就是以当前threadLocal实例为key，值为value的Entry往这个threadLocalMap中存放。Entry中的key是弱引用，当threadLocal外部强引用被置为null(threadLocalInstance=null),那么系统 GC 的时候，根据可达性分析，这个threadLocal实例就没有任何一条链路能够引用到它，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。当然，如果当前thread运行结束，threadLocal，threadLocalMap,Entry没有引用链可达，在垃圾回收的时候都会被系统进行回收。在实际开发中，会使用线程池去维护线程的创建和复用，比如固定大小的线程池，线程为了复用是不会主动结束的，所以，threadLocal的内存泄漏问题，是应该值得我们思考和注意的问题。 1234567891011121314151617181920212223242526272829303132private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 与concurrentHashMap，hashMap等容器一样，threadLocalMap也是采用散列表进行实现的。在了解set方法前，我们先来回顾下关于散列表相关的知识（摘自这篇的threadLocalMap的讲解部分以及这篇文章的hash）。为了解决散列冲突，主要采用下面两种方式： 分离链表法（separate chaining）和开放定址法（open addressing） 拉链法 使用链表解决冲突，将散列值相同的元素都保存到一个链表中。当查询的时候，首先找到元素所在的链表，然后遍历链表查找对应的元素，典型实现为hashMap，concurrentHashMap的拉链法 开放定址法 开放定址法不会创建链表，当关键字散列到的数组单元已经被另外一个关键字占用的时候，就会尝试在数组中寻找其他的单元，直到找到一个空的单元。探测数组空单元的方式有很多，最简单的为线性探测法。线性探测法就是从冲突的数组单元开始，依次往后搜索空单元，如果到数组尾部，再从头开始搜索（环形查找）。ThreadLocalMap 中使用开放地址法来处理散列冲突，因为在 ThreadLocalMap 中的散列值分散的十分均匀，很少会出现冲突。并且 ThreadLocalMap 经常需要清除无用的对象，使用纯数组更加方便 在了解这些相关知识后我们再回过头来看一下set方法。set方法的源码为： 12345678910111213141516171819202122232425262728293031323334353637private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don&apos;t use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; //根据threadLocal的hashCode确定Entry应该存放的位置 int i = key.threadLocalHashCode &amp; (len-1); //采用开放地址法，hash冲突的时候使用线性探测 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); //覆盖旧Entry if (k == key) &#123; e.value = value; return; &#125; //当key为null时，说明threadLocal强引用已经被释放掉，那么就无法 //再通过这个key获取threadLocalMap中对应的entry，这里就存在内存泄漏的可能性 if (k == null) &#123; //用当前插入的值替换掉这个key为null的“脏”entry replaceStaleEntry(key, value, i); return; &#125; &#125; //新建entry并插入table中i处 tab[i] = new Entry(key, value); int sz = ++size; //插入后再次清除一些key为null的“脏”entry,如果大于阈值就需要扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; set方法的关键部分请看上面的注释，主要有这样几点需要注意： threadLocal的hashcode? private final int threadLocalHashCode = nextHashCode();​ private static final int HASH_INCREMENT = 0x61c88647;​ private static AtomicInteger nextHashCode =new AtomicInteger();​ /*​ Returns the next hash code.​ */​ private static int nextHashCode() {​ return nextHashCode.getAndAdd(HASH_INCREMENT);​ }从源码中我们可以清楚的看到threadLocal实例的hashCode是通过nextHashCode()方法实现的，该方法实际上总是用一个AtomicInteger加上0x61c88647来实现的。0x61c88647这个数是有特殊意义的，它能够保证hash表的每个散列桶能够均匀的分布，这是Fibonacci Hashing，关于更多介绍可以看这篇文章的threadLocal散列值部分。也正是能够均匀分布，所以threadLocal选择使用开放地址法来解决hash冲突的问题。 怎样确定新值插入到哈希表中的位置？ 该操作源码为：key.threadLocalHashCode &amp; (len-1)，同hashMap和ConcurrentHashMap等容器的方式一样，利用当前key(即threadLocal实例)的hashcode与哈希表大小相与，因为哈希表大小总是为2的幂次方，所以相与等同于一个取模的过程，这样就可以通过Key分配到具体的哈希桶中去。而至于为什么取模要通过位与运算的原因就是位运算的执行效率远远高于了取模运算。 怎样解决hash冲突？ 源码中通过nextIndex(i, len)方法解决hash冲突的问题，该方法为((i + 1 &lt; len) ? i + 1 : 0);，也就是不断往后线性探测，当到哈希表末尾的时候再从0开始，成环形。 怎样解决“脏”Entry？ 在分析threadLocal,threadLocalMap以及Entry的关系的时候，我们已经知道使用threadLocal有可能存在内存泄漏（对象创建出来后，在之后的逻辑一直没有使用该对象，但是垃圾回收器无法回收这个部分的内存），在源码中针对这种key为null的Entry称之为“stale entry”，直译为不新鲜的entry，我把它理解为“脏entry”，自然而然，Josh Bloch and Doug Lea大师考虑到了这种情况,在set方法的for循环中寻找和当前Key相同的可覆盖entry的过程中通过replaceStaleEntry方法解决脏entry的问题。如果当前table[i]为null的话，直接插入新entry后也会通过cleanSomeSlots来解决脏entry的问题，关于cleanSomeSlots和replaceStaleEntry方法，会在详解threadLocal内存泄漏中讲到，具体可看那篇文章 如何进行扩容？ threshold的确定 也几乎和大多数容器一样，threadLocalMap会有扩容机制，那么它的threshold又是怎样确定的了？​ 1234567891011121314151617181920private int threshold; // Default to 0/** * The initial capacity -- MUST be a power of two. */ private static final int INITIAL_CAPACITY = 16; ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125;/** * Set the resize threshold to maintain at worst a 2/3 load factor. */ private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; 根据源码可知，在第一次为threadLocal进行赋值的时候会创建初始大小为16的threadLocalMap,并且通过setThreshold方法设置threshold，其值为当前哈希数组长度乘以（2/3），也就是说加载因子为2/3(加载因子是衡量哈希表密集程度的一个参数，如果加载因子越大的话，说明哈希表被装载的越多，出现hash冲突的可能性越大，反之，则被装载的越少，出现hash冲突的可能性越小。同时如果过小，很显然内存使用率不高，该值取值应该考虑到内存使用率和hash冲突概率的一个平衡，如hashMap,concurrentHashMap的加载因子都为0.75)。这里threadLocalMap初始大小为16，加载因子为2/3，所以哈希表可用大小为：16*2/3=10，即哈希表可用容量为10。 扩容resize 从set方法中可以看出当hash表的size大于threshold的时候，会通过resize方法进行扩容。 123456789101112131415161718192021222324252627282930313233/** * Double the capacity of the table. */private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; //新数组为原数组的2倍 int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); //遍历过程中如果遇到脏entry的话直接另value为null,有助于value能够被回收 if (k == null) &#123; e.value = null; // Help the GC &#125; else &#123; //重新确定entry在新数组的位置，然后进行插入 int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; //设置新哈希表的threshHold和size属性 setThreshold(newLen); size = count; table = newTab;&#125; 方法逻辑请看注释，新建一个大小为原来数组长度的两倍的数组，然后遍历旧数组中的entry并将其插入到新的hash数组中，主要注意的是，在扩容的过程中针对脏entry的话会令value为null，以便能够被垃圾回收器能够回收，解决隐藏的内存泄漏的问题。 getEntrygetEntry方法源码为： 123456789101112private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; //1. 确定在散列数组中的位置 int i = key.threadLocalHashCode &amp; (table.length - 1); //2. 根据索引i获取entry Entry e = table[i]; //3. 满足条件则返回该entry if (e != null &amp;&amp; e.get() == key) return e; else //4. 未查找到满足条件的entry，额外在做的处理 return getEntryAfterMiss(key, i, e);&#125; 方法逻辑很简单，若能当前定位的entry的key和查找的key相同的话就直接返回这个entry，否则的话就是在set的时候存在hash冲突的情况，需要通过getEntryAfterMiss做进一步处理。getEntryAfterMiss方法为： 12345678910111213141516171819private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) //找到和查询的key相同的entry则返回 return e; if (k == null) //解决脏entry的问题 expungeStaleEntry(i); else //继续向后环形查找 i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125; 这个方法同样很好理解，通过nextIndex往后环形查找，如果找到和查询的key相同的entry的话就直接返回，如果在查找过程中遇到脏entry的话使用expungeStaleEntry方法进行处理。到目前为止，为了解决潜在的内存泄漏的问题，在set，resize,getEntry这些地方都会对这些脏entry进行处理，可见为了尽可能解决这个问题几乎无时无刻都在做出努力。 remove12345678910111213141516171819/** * Remove the entry for key. */private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; //将entry的key置为null e.clear(); //将该entry的value也置为null expungeStaleEntry(i); return; &#125; &#125;&#125; 该方法逻辑很简单，通过往后环形查找到与指定key相同的entry后，先通过clear方法将key置为null后，使其转换为一个脏entry，然后调用expungeStaleEntry方法将其value置为null，以便垃圾回收时能够清理，同时将table[i]置为null。 使用场景ThreadLocal 不是用来解决共享对象的多线程访问问题的，数据实质上是放在每个thread实例引用的threadLocalMap,也就是说每个不同的线程都拥有专属于自己的数据容器（threadLocalMap），彼此不影响。因此threadLocal只适用于 共享对象会造成线程安全 的业务场景。比如hibernate中通过threadLocal管理Session就是一个典型的案例，不同的请求线程（用户）拥有自己的session,若将session共享出去被多线程访问，必然会带来线程安全问题。下面，我们自己来写一个例子，SimpleDateFormat.parse方法会有线程安全的问题，我们可以尝试使用threadLocal包装SimpleDateFormat，将该实例不被多线程共享即可。 1234567891011121314151617181920212223242526272829303132public class ThreadLocalDemo &#123; private static ThreadLocal&lt;SimpleDateFormat&gt; sdf = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; 100; i++) &#123; executorService.submit(new DateUtil(&quot;2019-11-25 09:00:&quot; + i % 60)); &#125; &#125; static class DateUtil implements Runnable &#123; private String date; public DateUtil(String date) &#123; this.date = date; &#125; @Override public void run() &#123; if (sdf.get() == null) &#123; sdf.set(new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)); &#125; else &#123; try &#123; Date date = sdf.get().parse(this.date); System.out.println(date); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 如果当前线程不持有SimpleDateformat对象实例，那么就新建一个并把它设置到当前线程中，如果已经持有，就直接使用。另外，从if (sdf.get() == null){....}else{.....}可以看出为每一个线程分配一个SimpleDateformat对象实例是从应用层面（业务代码逻辑）去保证的。 在上面我们说过threadLocal有可能存在内存泄漏，在使用完之后，最好使用remove方法将这个变量移除，就像在使用数据库连接一样，及时关闭连接。 参考资料 《java高并发程序设计》这篇文章的threadLocalMap讲解和threadLocal的hashCode讲解不错这篇文章讲解了hash，不错解决hash冲突 链地址法和开放地址法的比较 造成内存泄漏的原因threadLocal是为了解决对象不能被多线程共享访问的问题，通过threadLocal.set方法将对象实例保存在每个线程自己所拥有的threadLocalMap中，这样每个线程使用自己的对象实例，彼此不会影响达到隔离的作用，从而就解决了对象在被共享访问带来线程安全问题。如果将同步机制和threadLocal做一个横向比较的话，同步机制就是通过控制线程访问共享对象的顺序，而threadLocal就是为每一个线程分配一个该对象，各用各的互不影响。打个比方说，现在有100个同学需要填写一张表格但是只有一支笔，同步就相当于A使用完这支笔后给B，B使用后给C用……老师就控制着这支笔的使用顺序，使得同学之间不会产生冲突。而threadLocal就相当于，老师直接准备了100支笔，这样每个同学都使用自己的，同学之间就不会产生冲突。很显然这就是两种不同的思路，同步机制以“时间换空间”，由于每个线程在同一时刻共享对象只能被一个线程访问造成整体上响应时间增加，但是对象只占有一份内存，牺牲了时间效率换来了空间效率即“时间换空间”。而threadLocal，为每个线程都分配了一份对象，自然而然内存使用率增加，每个线程各用各的，整体上时间效率要增加很多，牺牲了空间效率换来时间效率即“空间换时间”。 关于threadLocal,threadLocalMap更多的细节可以看这篇文章，给出了很详细的各个方面的知识（很多也是面试高频考点）。threadLocal,threadLocalMap,entry之间的关系如下图所示： 上图中，实线代表强引用，虚线代表的是弱引用，如果threadLocal外部强引用被置为null(threadLocalInstance=null)的话，threadLocal实例就没有一条引用链路可达，很显然在gc(垃圾回收)的时候势必会被回收，因此entry就存在key为null的情况，无法通过一个Key为null去访问到该entry的value。同时，就存在了这样一条引用链：threadRef-&gt;currentThread-&gt;threadLocalMap-&gt;entry-&gt;valueRef-&gt;valueMemory,导致在垃圾回收的时候进行可达性分析的时候,value可达从而不会被回收掉，但是该value永远不能被访问到，这样就存在了内存泄漏。当然，如果线程执行结束后，threadLocal，threadRef会断掉，因此threadLocal,threadLocalMap，entry都会被回收掉。可是，在实际使用中我们都是会用线程池去维护我们的线程，比如在Executors.newFixedThreadPool()时创建线程的时候，为了复用线程是不会结束的，所以threadLocal内存泄漏就值得我们关注。 已经做出了哪些改进？实际上，为了解决threadLocal潜在的内存泄漏的问题，Josh Bloch and Doug Lea大师已经做了一些改进。在threadLocal的set和get方法中都有相应的处理。下文为了叙述，针对key为null的entry，源码注释为stale entry，直译为不新鲜的entry，这里我就称之为“脏entry”。比如在ThreadLocalMap的set方法中： 1234567891011121314151617181920212223242526272829303132private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don&apos;t use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 在该方法中针对脏entry做了这样的处理： 如果当前table[i]！=null的话说明hash冲突就需要向后环形查找，若在查找过程中遇到脏entry就通过replaceStaleEntry进行处理； 如果当前table[i]==null的话说明新的entry可以直接插入，但是插入后会调用cleanSomeSlots方法检测并清除脏entry cleanSomeSlots该方法的源码为： 123456789101112131415161718192021222324252627282930/* @param i a position known NOT to hold a stale entry. The * scan starts at the element after i. * * @param n scan control: &#123;@code log2(n)&#125; cells are scanned, * unless a stale entry is found, in which case * &#123;@code log2(table.length)-1&#125; additional cells are scanned. * When called from insertions, this parameter is the number * of elements, but when from replaceStaleEntry, it is the * table length. (Note: all this could be changed to be either * more or less aggressive by weighting n instead of just * using straight log n. But this version is simple, fast, and * seems to work well.) * * @return true if any stale entries have been removed. */private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; i = nextIndex(i, len); Entry e = tab[i]; if (e != null &amp;&amp; e.get() == null) &#123; n = len; removed = true; i = expungeStaleEntry(i); &#125; &#125; while ( (n &gt;&gt;&gt;= 1) != 0); return removed;&#125; 入参： i表示：插入entry的位置i，很显然在上述情况2（table[i]==null）中，entry刚插入后该位置i很显然不是脏entry; 参数n 2.1. n的用途 主要用于扫描控制（scan control），从while中是通过n来进行条件判断的说明n就是用来控制扫描趟数（循环次数）的。在扫描过程中，如果没有遇到脏entry就整个扫描过程持续log2(n)次，log2(n)的得来是因为n &gt;&gt;&gt;= 1，每次n右移一位相当于n除以2。如果在扫描过程中遇到脏entry的话就会令n为当前hash表的长度（n=len），再扫描log2(n)趟，注意此时n增加无非就是多增加了循环次数从而通过nextIndex往后搜索的范围扩大，示意图如下 按照n的初始值，搜索范围为黑线，当遇到了脏entry，此时n变成了哈希数组的长度（n取值增大），搜索范围log2(n)增大，红线表示。如果在整个搜索过程没遇到脏entry的话，搜索结束，采用这种方式的主要是用于时间效率上的平衡。 2.2. n的取值​ 如果是在set方法插入新的entry后调用（上述情况2），n位当前已经插入的entry个数size；如果是在replaceSateleEntry方法中调用n为哈希表的长度len。 expungeStaleEntry如果对输入参数能够理解的话，那么cleanSomeSlots方法搜索基本上清除了，但是全部搞定还需要掌握expungeStaleEntry方法，当在搜索过程中遇到了脏entry的话就会调用该方法去清理掉脏entry。源码为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Expunge a stale entry by rehashing any possibly colliding entries * lying between staleSlot and the next null slot. This also expunges * any other stale entries encountered before the trailing null. See * Knuth, Section 6.4 * * @param staleSlot index of slot known to have null key * @return the index of the next null slot after staleSlot * (all between staleSlot and this slot will have been checked * for expunging). */private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; //清除当前脏entry // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; //2.往后环形继续查找,直到遇到table[i]==null时结束 for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); //3. 如果在向后搜索过程中再次遇到脏entry，同样将其清理掉 if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; //处理rehash的情况 int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125; 该方法逻辑请看注释（第1,2,3步），主要做了这么几件事情： 清理当前脏entry，即将其value引用置为null，并且将table[staleSlot]也置为null。value置为null后该value域变为不可达，在下一次gc的时候就会被回收掉，同时table[staleSlot]为null后以便于存放新的entry; 从当前staleSlot位置向后环形（nextIndex）继续搜索，直到遇到哈希桶（tab[i]）为null的时候退出； 若在搜索过程再次遇到脏entry，继续将其清除。 也就是说该方法，清理掉当前脏entry后，并没有闲下来继续向后搜索，若再次遇到脏entry继续将其清理，直到哈希桶（table[i]）为null时退出。因此方法执行完的结果为 从当前脏entry（staleSlot）位到返回的i位，这中间所有的entry不是脏entry。为什么是遇到null退出呢？原因是存在脏entry的前提条件是 当前哈希桶（table[i]）不为null,只是该entry的key域为null。如果遇到哈希桶为null,很显然它连成为脏entry的前提条件都不具备。 现在对cleanSomeSlot方法做一下总结，其方法执行示意图如下： 如图所示，cleanSomeSlot方法主要有这样几点： 从当前位置i处（位于i处的entry一定不是脏entry）为起点在初始小范围（log2(n)，n为哈希表已插入entry的个数size）开始向后搜索脏entry，若在整个搜索过程没有脏entry，方法结束退出 如果在搜索过程中遇到脏entryt通过expungeStaleEntry方法清理掉当前脏entry，并且该方法会返回下一个哈希桶(table[i])为null的索引位置为i。这时重新令搜索起点为索引位置i，n为哈希表的长度len，再次扩大搜索范围为log2(n’)继续搜索。 下面，以一个例子更清晰的来说一下，假设当前table数组的情况如下图。 如图当前n等于hash表的size即n=10，i=1,在第一趟搜索过程中通过nextIndex,i指向了索引为2的位置，此时table[2]为null，说明第一趟未发现脏entry,则第一趟结束进行第二趟的搜索。 第二趟所搜先通过nextIndex方法，索引由2的位置变成了i=3,当前table[3]!=null但是该entry的key为null，说明找到了一个脏entry，先将n置为哈希表的长度len,然后继续调用expungeStaleEntry方法，该方法会将当前索引为3的脏entry给清除掉（令value为null，并且table[3]也为null）,但是该方法可不想偷懒，它会继续往后环形搜索，往后会发现索引为4,5的位置的entry同样为脏entry，索引为6的位置的entry不是脏entry保持不变，直至i=7的时候此处table[7]位null，该方法就以i=7返回。至此，第二趟搜索结束； 由于在第二趟搜索中发现脏entry，n增大为数组的长度len，因此扩大搜索范围（增大循环次数）继续向后环形搜索； 直到在整个搜索范围里都未发现脏entry，cleanSomeSlot方法执行结束退出。 replaceStaleEntry先来看replaceStaleEntry 方法，该方法源码为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/* * @param key the key * @param value the value to be associated with key * @param staleSlot index of the first stale entry encountered while * searching for key. */private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; // Back up to check for prior stale entry in current run. // We clean out whole runs at a time to avoid continual // incremental rehashing due to garbage collector freeing // up refs in bunches (i.e., whenever the collector runs). //向前找到第一个脏entry int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null)1. slotToExpunge = i; // Find either the key or trailing null slot of run, whichever // occurs first for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); // If we find key, then we need to swap it // with the stale entry to maintain hash table order. // The newly stale slot, or any other stale slot // encountered above it, can then be sent to expungeStaleEntry // to remove or rehash all of the other entries in run. if (k == key) &#123; //如果在向后环形查找过程中发现key相同的entry就覆盖并且和脏entry进行交换2. e.value = value;3. tab[i] = tab[staleSlot];4. tab[staleSlot] = e; // Start expunge at preceding stale entry if it exists //如果在查找过程中还未发现脏entry，那么就以当前位置作为cleanSomeSlots //的起点 if (slotToExpunge == staleSlot)5. slotToExpunge = i; //搜索脏entry并进行清理6. cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; // If we didn&apos;t find stale entry on backward scan, the // first stale entry seen while scanning for key is the // first still present in the run. //如果向前未搜索到脏entry，则在查找过程遇到脏entry的话，后面就以此时这个位置 //作为起点执行cleanSomeSlots if (k == null &amp;&amp; slotToExpunge == staleSlot)7. slotToExpunge = i; &#125; // If key not found, put new entry in stale slot //如果在查找过程中没有找到可以覆盖的entry，则将新的entry插入在脏entry8. tab[staleSlot].value = null;9. tab[staleSlot] = new Entry(key, value); // If there are any other stale entries in run, expunge them10. if (slotToExpunge != staleSlot) //执行cleanSomeSlots11. cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 该方法的逻辑请看注释，下面我结合各种情况详细说一下该方法的执行过程。首先先看这一部分的代码： 123456int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; 这部分代码通过PreIndex方法实现往前环形搜索脏entry的功能，初始时slotToExpunge和staleSlot相同，若在搜索过程中发现了脏entry，则更新slotToExpunge为当前索引i。另外，说明replaceStaleEntry并不仅仅局限于处理当前已知的脏entry，它认为在出现脏entry的相邻位置也有很大概率出现脏entry，所以为了一次处理到位，就需要向前环形搜索，找到前面的脏entry。那么根据在向前搜索中是否还有脏entry以及在for循环后向环形查找中是否找到可覆盖的entry，我们分这四种情况来充分理解这个方法： 1.前向有脏entry 1.1后向环形查找找到可覆盖的entry 该情形如下图所示。 ​ 123456 如图，slotToExpunge初始状态和staleSlot相同，当前向环形搜索遇到脏entry时，在第1行代码中slotToExpunge会更新为当前脏entry的索引i，直到遇到哈希桶（table[i]）为null的时候，前向搜索过程结束。在接下来的for循环中进行后向环形查找，若查找到了可覆盖的entry，第2,3,4行代码先覆盖当前位置的entry，然后再与staleSlot位置上的脏entry进行交换。交换之后脏entry就更换到了i处，最后使用cleanSomeSlots方法从slotToExpunge为起点开始进行清理脏entry的过程- 1.2后向环形查找未找到可覆盖的entry 该情形如下图所示。 ![前向环形搜索到脏entry,向后环形未搜索可覆盖entry.png](http://upload-images.jianshu.io/upload_images/2615789-423c8c8dfb2e9557.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 如图，slotToExpunge初始状态和staleSlot相同，当前向环形搜索遇到脏entry时，在第1行代码中slotToExpunge会更新为当前脏entry的索引i，直到遇到哈希桶（table[i]）为null的时候，前向搜索过程结束。在接下来的for循环中进行后向环形查找，若没有查找到了可覆盖的entry，哈希桶（table[i]）为null的时候，后向环形查找过程结束。那么接下来在8,9行代码中，将插入的新entry直接放在staleSlot处即可，最后使用cleanSomeSlots方法从slotToExpunge为起点开始进行清理脏entry的过程 2.前向没有脏entry 2.1后向环形查找找到可覆盖的entry该情形如下图所示。如图，slotToExpunge初始状态和staleSlot相同，当前向环形搜索直到遇到哈希桶（table[i]）为null的时候，前向搜索过程结束，若在整个过程未遇到脏entry，slotToExpunge初始状态依旧和staleSlot相同。在接下来的for循环中进行后向环形查找，若遇到了脏entry，在第7行代码中更新slotToExpunge为位置i。若查找到了可覆盖的entry，第2,3,4行代码先覆盖当前位置的entry，然后再与staleSlot位置上的脏entry进行交换，交换之后脏entry就更换到了i处。如果在整个查找过程中都还没有遇到脏entry的话，会通过第5行代码，将slotToExpunge更新当前i处，最后使用cleanSomeSlots方法从slotToExpunge为起点开始进行清理脏entry的过程。 2.2后向环形查找未找到可覆盖的entry该情形如下图所示。 ​ 1如图，slotToExpunge初始状态和staleSlot相同，当前向环形搜索直到遇到哈希桶（table[i]）为null的时候，前向搜索过程结束，若在整个过程未遇到脏entry，slotToExpunge初始状态依旧和staleSlot相同。在接下来的for循环中进行后向环形查找，若遇到了脏entry，在第7行代码中更新slotToExpunge为位置i。若没有查找到了可覆盖的entry，哈希桶（table[i]）为null的时候，后向环形查找过程结束。那么接下来在8,9行代码中，将插入的新entry直接放在staleSlot处即可。另外，如果发现slotToExpunge被重置，则第10行代码if判断为true,就使用cleanSomeSlots方法从slotToExpunge为起点开始进行清理脏entry的过程。 下面用一个实例来有个直观的感受，示例代码就不给出了，代码debug时table状态如下图所示： 如图所示，当前的staleSolt为i=4，首先先进行前向搜索脏entry，当i=3的时候遇到脏entry，slotToExpung更新为3，当i=2的时候tabel[2]为null，因此前向搜索脏entry的过程结束。然后进行后向环形查找，知道i=7的时候遇到table[7]为null，结束后向查找过程，并且在该过程并没有找到可以覆盖的entry。最后只能在staleSlot（4）处插入新entry，然后从slotToExpunge（3）为起点进行cleanSomeSlots进行脏entry的清理。是不是上面的1.2的情况。 这些核心方法，通过源码又给出示例图，应该最终都能掌握了，也还挺有意思的。若觉得不错，对我的辛劳付出能给出鼓励欢迎点赞，给小弟鼓励，在此谢过 :)。 当我们调用threadLocal的get方法时，当table[i]不是和所要找的key相同的话，会继续通过threadLocalMap的getEntryAfterMiss方法向后环形去找，该方法为： 12345678910111213141516private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125; 当key==null的时候，即遇到脏entry也会调用expungeStleEntry对脏entry进行清理。 当我们调用threadLocal.remove方法时候，实际上会调用threadLocalMap的remove方法，该方法的源码为： 1234567891011121314private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; e.clear(); expungeStaleEntry(i); return; &#125; &#125;&#125; 同样的可以看出，当遇到了key为null的脏entry的时候，也会调用expungeStaleEntry清理掉脏entry。 从以上set,getEntry,remove方法看出，在threadLocal的生命周期里，针对threadLocal存在的内存泄漏的问题，都会通过expungeStaleEntry，cleanSomeSlots,replaceStaleEntry这三个方法清理掉key为null的脏entry。 为什么使用弱引用？从文章开头通过threadLocal,threadLocalMap,entry的引用关系看起来threadLocal存在内存泄漏的问题似乎是因为threadLocal是被弱引用修饰的。那为什么要使用弱引用呢？ 如果使用强引用 假设threadLocal使用的是强引用，在业务代码中执行threadLocalInstance==null操作，以清理掉threadLocal实例的目的，但是因为threadLocalMap的Entry强引用threadLocal，因此在gc的时候进行可达性分析，threadLocal依然可达，对threadLocal并不会进行垃圾回收，这样就无法真正达到业务逻辑的目的，出现逻辑错误 如果使用弱引用 假设Entry弱引用threadLocal，尽管会出现内存泄漏的问题，但是在threadLocal的生命周期里（set,getEntry,remove）里，都会针对key为null的脏entry进行处理。 从以上的分析可以看出，使用弱引用的话在threadLocal生命周期里会尽可能的保证不出现内存泄漏的问题，达到安全的状态。 Thread.exit()当线程退出时会执行exit方法： 1234567891011121314private void exit() &#123; if (group != null) &#123; group.threadTerminated(this); group = null; &#125; /* Aggressively null out all reference fields: see bug 4006245 */ target = null; /* Speed the release of some of these resources */ threadLocals = null; inheritableThreadLocals = null; inheritedAccessControlContext = null; blocker = null; uncaughtExceptionHandler = null;&#125; 从源码可以看出当线程结束时，会令threadLocals=null，也就意味着GC的时候就可以将threadLocalMap进行垃圾回收，换句话说threadLocalMap生命周期实际上thread的生命周期相同。 threadLocal最佳实践在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，应该尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险。 参考资料 《java高并发程序设计》http://blog.xiaohansong.com/2016/08/06/ThreadLocal-memory-leak/]]></content>
  </entry>
  <entry>
    <title><![CDATA[多线程]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FThread%2F</url>
    <content type="text"><![CDATA[多线程的优势在于可以充分发挥多核CPU的计算能力，方便进行业务拆分，提高性能。CPU是通过给线程分配时间片实现线程的切换，失去时间片时，线程需要记录当前执行的位置，等到分配到时间片时再恢复到之前的状态，线程上下文切换非常耗费性能，这也导致了公平锁和非公平锁之间的性能差距；还会导致线程安全问题，如死锁、读过期数据。 线程的状态NEW ：线程创建后进入NEW状态 RUNNABLE ：线程创建后调用start()方法进入可运行态，分配到时间片后进入运行态 WAITING ：调用wait()、join()、LockSupport.lock()后线程进入WAITING状态 TIMED_WAITING ：调用sleep(millis)、join(millis)、LockSupport.parkNanos(nanos)、LockSupport.parkUntil(deadline)、wait(timeout)后线程进入TIMED_WAITING状态，比如java.util.concurrent.locks的lock，线程切换到的是WAITING状态或TIMED_WAITING状态，lock调用LockSupport的方法 BLOCKED ：使用synchronized线程出现锁竞争，竞争失败的线程会进入BLOCKED状态 TERMINATED ：线程正常运行结束或线程抛出一个未被捕捉的Exception或Error后会释放对象的监视器，线程进入TERMINATED状态 常用方法join()可以用于线程的顺序执行，threadA调用threadB的join()方法后，线程A会一直阻塞，直到线程B终止后隐式调用notifyAll()方法唤醒所有等待线程后，threadA才会继续执行 123while (isAlive()) &#123; wait(0);&#125; sleep()与wait() sleep()是Thread类的静态方法，wait()是Object类的实例方法 wait()必须要在同步方法或同步块内才能调用，前提是已经获取对象锁，sleep()没有限制 wait()会释放已占有的对象锁，sleep()不会释放 sleep()在休眠指定时间后分配到时间片后就会继续执行，wait()必须等待notify()或notifyAll()通知后，才会离开等待队列，分配到时间片后才会继续执行 两者都可以暂停线程的执行。 Wait通常被用于线程间交互/通信，sleep通常被用于暂停执行。 yield()与sleep() 调用yield()后，当前线程会让出CPU，回到可运行状态等待时间片的分配，让出的时间片只会分配给与当前线程相同优先级或者更高优先级的线程；调用sleep()后，当前线程会让出CPU，其他线程都可以去竞争 stop()、suspend()、resume()suspend()线程挂起，不会释放锁 resume()线程继续执行，如果先于suspend()将会冻结 wait()、notify()、notifyAll()JDK强制wait()、notify()、notifyAll()方法在调用前都必须先获得对象的锁 wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器 interrupt()、interrupted()、isInterrupted()线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，通过抛出InterruptedException来唤醒它 interrupt()对当前线程进行中断操作，如果线程调用了wait()、join()方法时会抛出InterruptedException并将中断标志位清除 interrupted()测试线程是否被中断，会清除中断标志位当抛出InterruptedException时候，会清除中断标志位，也就是说在调用isInterrupted()会返回false 结束线程时通过中断标志位或者标志位的方式可以有机会去清理资源，相对于武断而直接的结束线程，这种方式要优雅和安全 isInterrupted()测试线程是否被中断，不会清除中断标志位 void interrupt() 向线程发送中断请求，线程的中断状态将被设置为true，如果线程被一个sleep调用阻塞，那么将会抛出异常 static boolean interrupted() 测试当前线程是否被中断，他会将当前线程的中断状态重置为false boolean isInterrupted 测试线程是否被终止 不改变线程的中断状态 常见问题 Thread.holdsLock(obj) 检测一个线程是否持有对象监视器锁 通过setDaemon(true)可以将线程设置为守护线程，需要先于start()执行，否则会抛出一个异常，但线程还是会执行，只不过还是当作普通线程执行，当Java应用只有守护线程时，虚拟机就会自然退出，这时守护线程退出的时候并不会执行finally里的代码，所以将释放资源等操作放在finally块里时不安全的 避免死锁 避免一个线程同时获得多个锁，减少每个锁占用的资源数 尝试使用定时锁tryLock(timeout)，超过等待时间不会阻塞 数据库的加锁、解锁必须放在同一个数据库连接里 Java多线程中的死锁死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 1.互斥（Mutual exclusion）：存在这样一种资源，它在某个时刻只能被分配给一个执行绪（也称为线程）使用； 2.持有（Hold and wait）：当请求的资源已被占用从而导致执行绪阻塞时，资源占用者不但无需释放该资源，而且还可以继续请求更多资源； 3.不可剥夺（No preemption）：执行绪获得到的互斥资源不可被强行剥夺，换句话说，只有资源占用者自己才能释放资源； 4.环形等待（Circular wait）：若干执行绪以不同的次序获取互斥资源，从而形成环形等待的局面，想象在由多个执行绪组成的环形链中，每个执行绪都在等待下一个执行绪释放它持有的资源。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。这篇教程有代码示例和避免死锁的讨论细节。 一个线程执行完毕之后会自动结束，如果在运行过程中发生异常未被捕获，会提前结束。 InterruptedException通过调用一个线程的 interrupt() 来中断该线程，如果该线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 InterruptedException，从而提前结束该线程。但是不能中断 I/O 阻塞和 synchronized 锁阻塞。 对于以下代码，在 main() 中启动一个线程之后再中断它，由于线程中调用了 Thread.sleep() 方法，因此会抛出一个 InterruptedException，从而提前结束线程，不执行之后的语句。 1234567891011121314public class InterruptExample &#123; private static class MyThread1 extends Thread &#123; @Override public void run() &#123; try &#123; Thread.sleep(2000); System.out.println("Thread run"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 123456public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new MyThread1(); thread1.start(); thread1.interrupt(); System.out.println("Main run");&#125; 123456Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at InterruptExample.lambda$main$0(InterruptExample.java:5) at InterruptExample$$Lambda$1/713338599.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) interrupted()如果一个线程的 run() 方法执行一个无限循环，并且没有执行 sleep() 等会抛出 InterruptedException 的操作，那么调用线程的 interrupt() 方法就无法使线程提前结束。 但是调用 interrupt() 方法会设置线程的中断标记，此时调用 interrupted() 方法会返回 true。因此可以在循环体中使用 interrupted() 方法来判断线程是否处于中断状态，从而提前结束线程。 123456789101112public class InterruptExample &#123; private static class MyThread2 extends Thread &#123; @Override public void run() &#123; while (!interrupted()) &#123; // .. &#125; System.out.println("Thread end"); &#125; &#125;&#125; 12345public static void main(String[] args) throws InterruptedException &#123; Thread thread2 = new MyThread2(); thread2.start(); thread2.interrupt();&#125; 1Thread end 20) Java中interrupted 和 isInterruptedd方法的区别？interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted()来检查中断状态时，中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 37）如果你提交任务时，线程池队列已满。会时发会生什么？这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常。 Thread.DefaultUncaughtExceptionHandler设置该线程由于未捕获到异常而突然终止时调用的处理程序。通过明确设置未捕获到的异常处理程序，线程可以完全控制它对未捕获到的异常作出响应的方式。如果没有设置这样的处理程序，则该线程的 ThreadGroup 对象将充当其处理程序。 Thread.UncaughtExceptionHandler设置当线程由于未捕获到异常而突然终止，并且没有为该线程定义其他处理程序时所调用的默认处理程序。未捕获到的异常处理首先由线程控制，然后由线程的 ThreadGroup 对象控制，最后由未捕获到的默认异常处理程序控制。如果线程不设置明确的未捕获到的异常处理程序，并且该线程的线程组（包括父线程组）未特别指定其 uncaughtException 方法，则将调用默认处理程序的 uncaughtException 方法。通过设置未捕获到的默认异常处理程序，应用程序可以为那些已经接受系统提供的任何“默认”行为的线程改变未捕获到的异常处理方式请注意，未捕获到的默认异常处理程序通常不应顺从该线程的 ThreadGroup 对象，因为这可能导致无限递归。 1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] args) &#123; set(); test(); &#125; private static void test()&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("子线程异常"); System.out.println(1/0); &#125; &#125;).start(); System.out.println("当前线程异常"); System.out.println(1/0); &#125; private static void set()&#123; Thread.UncaughtExceptionHandler uncaughtExceptionHandler = new Thread.UncaughtExceptionHandler() &#123; @Override public void uncaughtException(Thread t, Throwable e) &#123; System.out.println("current"+t.toString()+":"+e.getMessage()); &#125; &#125;; Thread.UncaughtExceptionHandler defaultHandler = new Thread.UncaughtExceptionHandler() &#123; @Override public void uncaughtException(Thread t, Throwable e) &#123; System.out.println("default"+t.toString()+":"+e.getMessage()); &#125; &#125;; Thread.currentThread().setUncaughtExceptionHandler(uncaughtExceptionHandler); Thread.setDefaultUncaughtExceptionHandler(defaultHandler); &#125;&#125;]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String与StrinBuilder与StringBuffer]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FString%E4%B8%8EStrinBuilder%E4%B8%8EStringBuffer%2F</url>
    <content type="text"><![CDATA[StringString的创建机理由于String在Java世界中使用过于频繁，Java为了避免在一个系统中产生大量的String对象，引入了字符串常量池。其运行机制是： 创建一个字符串时，首先检查池中是否有值相同的字符串对象，如果有则不需要创建直接从池中刚查找到的对象引用；如果没有则新建字符串对象，返回对象引用，并且将新创建的对象放入池中。但是，通过new方法创建的String对象是不检查字符串池的，而是直接在堆区或栈区创建一个新的对象，也不会把对象放入池中。上述原则只适用于通过直接量给String对象引用赋值的情况。 举例： 12String str1 = "123"; //通过直接量赋值方式，放入字符串常量池String str2 = new String(“123”);//通过new方式赋值方式，不放入字符串常量池 注意：String提供了intern()方法。调用该方法时，如果常量池中包括了一个等于此String对象的字符串（由equals方法确定），则返回池中的字符串。否则，将此String对象添加到池中，并且返回此池中对象的引用。 Intern()是一种显式地排重机制，但是它也有一定的副作用，因为需要开发者写代码时明确调用，一是不方便，每一个都显式调用是非常麻烦的；另外就是我们很难保证效率，应用开发阶段很难清楚地预计字符串的重复情况，有人认为这是一种污染代码的实践。幸好在Oracle JDK 8u20之后，推出了一个新的特性，也就是G1 GC下的字符串排重。它是通过将相同数据的字符串指向同一份数据来做到的，是JVM底层的改变，并不需要Java类库做什么修改。注意这个功能目前是默认关闭的，你需要使用下面参数开启，并且记得指定使用G1 GC：-XX:+UseStringDeduplication String的特性 不可变。是指String对象一旦生成，则不能再对它进行改变。不可变的主要作用在于当一个对象需要被多线程共享，并且访问频繁时，可以省略同步和锁等待的时间，从而大幅度提高系统性能。不可变模式是一个可以提高多线程程序的性能，降低多线程程序复杂度的设计模式。 针对常量池的优化。当2个String对象拥有相同的值时，他们只引用常量池中的同一个拷贝。当同一个字符串反复出现时，这个技术可以大幅度节省内存空间。 StringBufer/StringBuilderStringBuffer和StringBuilder都实现了AbstractStringBuilder抽象类，拥有几乎一致对外提供的调用接口；其底层在内存中的存储方式与String相同，都是以一个有序的字符序列（char类型的数组）进行存储，不同点是StringBufer/StringBuilder对象的值是可以改变的，并且值改变以后，对象引用不会发生改变;两者对象在构造过程中，首先按照默认大小申请一个字符数组，由于会不断加入新数据，当超过默认大小后，会创建一个更大的数组，并将原先的数组内容复制过来，再丢弃旧的数组。因此，对于较大对象的扩容会涉及大量的内存复制操作，如果能够预先评估大小，可提升性能。 应用场景 在字符串内容不经常发生变化的业务场景优先使用String类。例如：常量声明、少量的字符串拼接操作等。如果有大量的字符串内容拼接，避免使用String与String之间的“+”操作，因为这样会产生大量无用的中间对象，耗费空间且执行效率低下（新建对象、回收对象花费大量时间）。 在频繁进行字符串的运算（如拼接、替换、删除等），并且运行在多线程环境下，建议使用StringBuffer(synchronized关键字)，例如XML解析、HTTP参数解析与封装。 在频繁进行字符串的运算（如拼接、替换、删除等），并且运行在单线程环境下，建议使用StringBuilder，例如SQL语句拼装、JSON封装等。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FSet%2F</url>
    <content type="text"><![CDATA[特点：不会存储重复的元素 HashSetObject的hashCode方法的话，hashCode会返回每个对象特有的序号（Java是依据对象的内存地址计算出的此序号），所以两个不同的对象的hashCode值是不可能相等的。 如果想要让两个不同的Object对象视为相等的，就必须重写类的hashCode方法和equals方法，同时也需要两个不同对象比较equals方法会返回true HashSet和ArrayList都有判断元素是否相同的方法 ArrayList使用boolean contains(Object o)–&gt;indexOf(Object o)–&gt;equals HashSet使用hashCode和equals方法 通过hashCode方法和equals方法来保证元素的唯一性，add()返回的是boolean类型 判断两个元素是否相同，先判断元素的hashCode值是否一致，如果相同才会去判断equals方法 TreeSet基于红黑树实现，不能重复存储元素，还能实现排序 有两种自定义排序规则的方法 让存入的元素自身具有比较性 实现Comparable接口，重写compareTo(Object o)方法 给TreeSet指定排序规则 定义一个类实现接口Comparator，重写compare(Object o1, Object o2）方法，该接口的子类实例对象作为参数传递给TreeMap集合的构造方法 如果Comparable和Comparator同时存在，优先Comparator LinkedHashSet会保存与元素插入的顺序 总结看到array，就要想到下标 看到link，就要想到first，last 看到hash，就要想到hashCode,equals 看到tree，就要想到两个接口。Comparable，Comparator]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F15%2FJava%2FReadWriteLock%2F</url>
    <content type="text"><![CDATA[在并发场景中用于解决线程安全的问题，我们几乎会高频率的使用到独占式锁，通常使用java提供的关键字synchronized（关于synchronized可以看这篇文章）或者concurrents包中实现了Lock接口的ReentrantLock。它们都是独占式获取锁，也就是在同一时刻只有一个线程能够获取锁。而在一些业务场景中，大部分只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。在分析WirteLock和ReadLock的互斥性时可以按照WriteLock与WriteLock之间，WriteLock与ReadLock之间以及ReadLock与ReadLock之间进行分析。更多关于读写锁特性介绍大家可以看源码上的介绍（阅读源码时最好的一种学习方式，我也正在学习中，与大家共勉），这里做一个归纳总结： 公平性选择：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平； 重入性：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁； 锁降级：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁 要想能够彻底的理解读写锁必须能够理解这样几个问题：1. 读写锁是怎样实现分别记录读写状态的？2. 写锁是怎样获取和释放的？3.读锁是怎样获取和释放的？我们带着这样的三个问题，再去了解下读写锁。 写锁的获取同步组件的实现聚合了同步器（AQS），并通过重写重写同步器（AQS）中的方法实现同步组件的同步语义（关于同步组件的实现层级结构可以看这篇文章，AQS的底层实现分析可以看这篇文章）。因此，写锁的实现依然也是采用这种方式。在同一时刻写锁是不能被多个线程所获取，很显然写锁是独占式锁，而实现写锁的同步语义是通过重写AQS中的tryAcquire方法实现的。源码为: 12345678910111213141516171819202122232425262728293031323334353637protected final boolean tryAcquire(int acquires) &#123; /* * Walkthrough: * 1. If read count nonzero or write count nonzero * and owner is a different thread, fail. * 2. If count would saturate, fail. (This can only * happen if count is already nonzero.) * 3. Otherwise, this thread is eligible for lock if * it is either a reentrant acquire or * queue policy allows it. If so, update state * and set owner. */ Thread current = Thread.currentThread(); // 1. 获取写锁当前的同步状态 int c = getState(); // 2. 获取写锁获取的次数 int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) // 3.1 当读锁已被读线程获取或者当前线程不是已经获取写锁的线程的话 // 当前线程获取写锁失败 if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // Reentrant acquire // 3.2 当前线程获取写锁，支持可重复加锁 setState(c + acquires); return true; &#125; // 3.3 写锁未被任何线程获取，当前线程可获取写锁 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125; 这段代码的逻辑请看注释，这里有一个地方需要重点关注，exclusiveCount(c)方法，该方法源码为：​ 1static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; 其中EXCLUSIVE_MASK为: static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数。同时还有一个方法值得我们注意： 1static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; 该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。现在还记得我们开篇说的需要弄懂的第一个问题吗？读写锁是怎样实现分别记录读锁和写锁的状态的，现在这个问题的答案就已经被我们弄清楚了，其示意图如下图所示： 现在我们回过头来看写锁获取方法tryAcquire，其主要逻辑为：当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。 写锁的释放写锁释放通过重写AQS的tryRelease方法，源码为： 12345678910111213protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //1. 同步状态减去写状态 int nextc = getState() - releases; //2. 当前写状态是否为0，为0则释放写锁 boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); //3. 不为0则更新同步状态 setState(nextc); return free;&#125; 源码的实现逻辑请看注释，不难理解与ReentrantLock基本一致，这里需要注意的是，减少写状态int nextc = getState() - releases;只需要用当前同步状态直接减去写状态的原因正是我们刚才所说的写状态是由同步状态的低16位表示的。 读锁的获取看完了写锁，现在来看看读锁，读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。按照之前对AQS介绍，实现共享式同步组件的同步语义需要通过重写AQS的tryAcquireShared方法和tryReleaseShared方法。读锁的获取实现方法为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected final int tryAcquireShared(int unused) &#123; /* * Walkthrough: * 1. If write lock held by another thread, fail. * 2. Otherwise, this thread is eligible for * lock wrt state, so ask if it should block * because of queue policy. If not, try * to grant by CASing state and updating count. * Note that step does not check for reentrant * acquires, which is postponed to full version * to avoid having to check hold count in * the more typical non-reentrant case. * 3. If step 2 fails either because thread * apparently not eligible or CAS fails or count * saturated, chain to version with full retry loop. */ Thread current = Thread.currentThread(); int c = getState(); //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前 // 线程获取读锁失败返回-1 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; int r = sharedCount(c); if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; //2. 当前线程获取读锁 compareAndSetState(c, c + SHARED_UNIT)) &#123; //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法 //返回当前获取读锁的次数 if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; //4. 处理在第二步中CAS操作失败的自旋已经实现重入性 return fullTryAcquireShared(current);&#125; 代码的逻辑请看注释，需要注意的是 当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。另外，当前同步状态需要加上SHARED_UNIT（(1 &lt;&lt; SHARED_SHIFT)即0x00010000）的原因这是我们在上面所说的同步状态的高16位用来表示读锁被获取的次数。如果CAS失败或者已经获取读锁的线程再次获取读锁时，是靠fullTryAcquireShared方法实现的，这段代码就不展开说了，有兴趣可以看看。 读锁的释放读锁释放的实现主要通过方法tryReleaseShared，源码如下，主要逻辑请看注释： 1234567891011121314151617181920212223242526272829303132protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); // 前面还是为了实现getReadHoldCount等新功能 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; --rh.count; &#125; for (;;) &#123; int c = getState(); // 读锁释放 将同步状态减去读状态即可 int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. return nextc == 0; &#125;&#125; 锁降级读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中： 123456789101112131415161718192021222324252627void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // Must release read lock before acquiring write lock rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // Recheck state because another thread might have // acquired write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // Downgrade by acquiring read lock before releasing write lock rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[ReentrantLock]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FReentrantLock%2F</url>
    <content type="text"><![CDATA[ReentrantLock重入锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞 ReenTrantLock独有的能力： ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。 ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。 当使用非公平锁的时候，会立刻尝试配置状态，成功了就会插队执行，失败了就会和公平锁的机制一样，调用acquire()方法，以排他的方式来获取锁，成功了立刻返回，否则将线程加入队列，知道成功调用为止。 重入性的实现原理要想支持重入性，就要解决两个问题： 在线程获取锁的时候，如果已经获取锁的线程是当前线程的话则直接再次获取成功； 由于锁会被获取n次，那么只有锁在被释放同样的n次之后，该锁才算是完全释放成功。 通过重写AQS的几个protected方法来表达自己的同步语义。针对第一个问题，我们来看看ReentrantLock是怎样实现的，以非公平锁为例，判断当前线程能否获得锁为例，核心方法为nonfairTryAcquire： 123456789101112131415161718192021final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //1. 如果该锁未被任何线程占有，该锁能被当前线程获取 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //2.若被占有，检查占有线程是否是当前线程 else if (current == getExclusiveOwnerThread()) &#123; // 3. 再次获取，计数加一 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false;&#125; 这段代码的逻辑也很简单，具体请看注释。为了支持重入性，在第二步增加了处理逻辑，如果该锁已经被线程所占有了，会继续检查占有线程是否为当前线程，如果是的话，同步状态加1返回true，表示可以再次获取成功。每次重新获取都会对同步状态进行加一的操作，那么释放的时候处理思路是怎样的了？（依然还是以非公平锁为例）核心方法为tryRelease： 123456789101112131415protected final boolean tryRelease(int releases) &#123; //1. 同步状态减1 int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; //2. 只有当同步状态为0时，锁成功被释放，返回true free = true; setExclusiveOwnerThread(null); &#125; // 3. 锁未被完全释放，返回false setState(c); return free;&#125; 代码的逻辑请看注释，需要注意的是，重入锁的释放必须得等到同步状态为0时锁才算成功释放，否则锁仍未释放。如果锁被获取n次，释放了n-1次，该锁未完全释放返回false，只有被释放n次才算成功释放，返回true。到现在我们可以理清ReentrantLock重入性的实现了，也就是理解了同步语义的第一条。 公平锁与公平锁ReentrantLock支持两种锁：公平锁和非公平锁。 公平锁：每次获取到锁为同步队列中的第一个节点，保证请求资源时间上的绝对顺序非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，造成“饥饿”现象 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。ReentrantLock默认非公平锁，为了减少一部分上下文切换，保证了系统更大的吞吐量 公平锁的处理逻辑是怎样的，核心方法为： 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 唯一的不同在于增加了hasQueuedPredecessors的逻辑判断，方法名就可知道该方法用来判断当前节点在同步队列中是否有前驱节点的判断，如果有前驱节点说明有线程比当前线程更早的请求资源，根据公平性，当前线程请求资源失败。如果当前节点没有前驱节点的话，再才有做后面的逻辑判断的必要性。公平锁每次都是从同步队列中的第一个节点获取到锁，而非公平性锁则不一定，有可能刚释放锁的线程能再次获取到锁。 使用场景tryLock 比如一个定时任务,第一次定时任务未完成,重复发起了第二次,直接返回flase; 用在界面交互时点击执行较长时间请求操作时，防止多次点击导致后台重复执行 123456789101112131415161718192021public class LockTest&#123; private static final ReentrantLock LOCK=new ReentrantLock(); public static void tryLockTest()&#123; if(LOCK.tryLock())&#123;// 如果已经被lock，则立即返回false不会等待 try &#123; // ... method body System.out.println(Thread.currentThread().getName()+"lock"); Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(Thread.currentThread().getName()+"unlock"); LOCK.unlock(); &#125; &#125; &#125;&#125; lock 同步操作 类似于synchronized 如果被其它资源锁定，会在此等待锁释放，达到暂停的效果 ReentrantLock存在公平锁与非公平锁 而且synchronized都是公平的 123456789101112131415public static void lockTest()&#123; LOCK.lock();// //如果被其它资源锁定，会在此等待锁释放，达到暂停的效果 try &#123; // ... method body System.out.println(Thread.currentThread().getName()+"lock"); Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(Thread.currentThread().getName()+"unlock"); LOCK.unlock(); &#125;&#125; tryLock(timeout, unit) 如果发现该操作正在执行,等待一段时间，如果规定时间未得到锁,放弃。防止资源处理不当，线程队列溢出,出现死锁 123456789101112131415161718public static void tryLockTimeTest()&#123; try &#123; if(LOCK.tryLock(5000, TimeUnit.SECONDS))&#123;// 如果已经被lock，尝试等待5s，看是否可以获得锁，如果5s后仍然无法获得锁则返回false继续执行 try &#123; // ... method body System.out.println(Thread.currentThread().getName()+"lock"); Thread.sleep(6000); &#125; finally &#123; System.out.println(Thread.currentThread().getName()+"unlock"); LOCK.unlock(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; lockInterruptibly 中断正在进行的操作立刻释放锁继续下一操作.比如 取消正在同步运行的操作，来防止不正常操作长时间占用造成的阻塞 1234567891011121314public static void lockInterruptiblyTest()&#123; try &#123; LOCK.lockInterruptibly(); // ... method body System.out.println(Thread.currentThread().getName()+"lock"); Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; System.out.println(Thread.currentThread().getName()+"unlock"); LOCK.unlock(); &#125;&#125; 测试： 12345public static void main(String[] args) &#123; for (int i = 0; i &lt; 1000; i++) &#123; new Thread(()-&gt;LockTest.lockTest()).start(); &#125; &#125; 对比synchronized、ReentrantLockReentrantLock使用起来比较灵活，但是必须手动获取与释放锁，而synchronized不需要手动释放和开启锁ReentrantLock只适用于代码块锁，而synchronized可用于修饰方法、代码块ReentrantLock的优势体现在：具备尝试非阻塞地获取锁的特性：当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁能被中断地获取锁的特性：与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放超时获取锁的特性：在指定的时间范围内获取锁；如果截止时间到了仍然无法获取锁，则返回3 注意事项在使用ReentrantLock类的时，一定要注意三点：在fnally中释放锁，目的是保证在获取锁之后，最终能够被释放不要将获取锁的过程写在try块内，因为如果在获取锁时发生了异常，异常抛出的同时，也会导致锁无故被释放。ReentrantLock提供了一个newCondition的方法，以便用户在同一锁的情况下可以根据不同的情况执行等待或唤醒的动作 锁降级确实是会发生的，当JVM进入安全点（SafePoint）的时候，会检查是否有闲置的Monitor，然后试图进行降级。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Map]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FMap%2F</url>
    <content type="text"><![CDATA[HashMap从结构实现来讲，HashMap是数组+链表+红黑树实现的。通过拉链法解决hash冲突的问题，为了避免链表过长影响HashMap的性能，当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 put(K key, V value)12345678910111213141516171819202122232425262728293031323334353637383940414243444546public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null)// 判断落在哪个桶中 如果桶为空 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))// 如果key已存在，覆盖 e = p; else if (p instanceof TreeNode)// 红黑树 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123;// 链表 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123;// 到链表底部 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st 节点数量超过阈值转化为红黑树 treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key key值与已有的重复的 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value;// 旧值替换新值 afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold))// 实际大小超过阈值就resize resize(); afterNodeInsertion(evict); return null;&#125; get(Object key)123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))// 与第一个节点相等 return first; if ((e = first.next) != null) &#123;// 不止一个节点 if (first instanceof TreeNode)// 在红黑树里找 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123;// 在链表中找 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize()扩容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;// 超过最大值不扩容 threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // 扩容成原来的两倍 &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE);// 计算新的resize上限 &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];// 初始化新数组 table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123;// 把每个bucket都移动到新的buckets中 Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null;// 释放旧Entry数组的对象引用（for循环后，旧的数组不再引用任何对象） if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e;// 重新计算每个元素在数组中的位置 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123;// 原索引 if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123;// 原索引+oldCap if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123;// 原索引放到bucket里 loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123;// 原索引+oldCap放到bucket里 hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 扩容是一个特别耗性能的操作，所以在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 HashMap 长度是2的幂次方数组下标的计算方法是(n - 1) &amp; hash,如果n是2的次方使用 &amp;相对于%能够提高运算效率 假设hash 为 00010110 2的3次方 00000111 &amp; 00010110 =00000110=6 2的4次方 00001111 &amp; 00010110 =00000110=6 假设hash 为 00001110 00000111 &amp; 00001110=00000110=6 00001111 &amp; 00001110=00001110=14=6+2的3次方 (newTab[j + oldCap] = hiHead;) 总结HashMap基于哈希思想实现对数据的读写。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。当两个不同的键对象的hashcode相同时，它们会储存在同一个bucket位置的链表中，可通过键对象的equals()方法用来找到键值对。如果链表大小超过阈值（TREEIFY_THRESHOLD, 8），链表就会被改造为树形结构 扩容时：Hashtable将容量变为原来的2倍加1；HashMap扩容将容量变为原来的2倍newCap = oldCap &lt;&lt; 1 HashTable中的key、value都不能为null；HashMap中的key、value可以为null，很显然只能有一个key为null的键值对，但是允许有多个值为null的键值对； LinkedHashMapLinkedHashMap 继承了 HashMap ，其中 Entry 继承于 HashMap 的 Entry，并新增了上下节点的指针，也就形成了双向链表，来保证了顺序性。通过顺序性来解决有排序需求的场景。有一个 accessOrder 成员变量，默认是 false，默认按照写入顺序排序，为 true 时按照访问顺序排序，其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。可以用来做LRU算法。 put() 方法 看 LinkedHashMap 的 put() 方法之前先看看 HashMap 的 put 方法，其中有两个空方法afterNodeAccess和afterNodeInsertion 1234567891011121314151617181920212223242526272829303132void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; get 方法 12345678public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125; LinkedHashMap通常提供的是遍历顺序符合插入顺序，它的实现是通过为条目（键值对）维护一个双向链表。注意，通过特定构造函数，我们可以创建反映访问顺序的实例，所谓的put、get、compute等，都算作“访问”。这种行为适用于一些特定应用场景，例如，我们构建一个空间占用敏感的资源池，希望可以自动将最不常被访问的对象释放掉，这就可以利用LinkedHashMap提供的机制来实现 LRUHashMap 基于访问的最近最少使用算法 构造函数accessOrder=true时,当get（Object key）时，将最新访问的元素放到双向链表的第一位 12345678910111213public class ParallelArrays &#123; public static void main(String[] args) &#123; LinkedHashMap&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true)&#123; @Override protected boolean removeEldestEntry(Map.Entry&lt;String,String&gt; eldest) &#123; return size()&gt;3; &#125; &#125;; map.forEach((K,V)-&gt;System.out.println(K+" "+V)); &#125;&#125; WeakHashMapWeakHashMap 的 Entry 继承自 WeakReference，被 WeakReference 关联的对象在下一次垃圾回收时会被回收。 WeakHashMap 主要用来实现缓存，通过使用 WeakHashMap 来引用缓存对象，由 JVM 对这部分缓存进行回收。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; ConcurrentCacheTomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能，ConcurrentCache 采取的是分代缓存： 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园）； 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收。 12345678910111213141516171819202122232425262728293031323334353637383940public final class ConcurrentCache&lt;K, V&gt; &#123; private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap(size); this.longterm = new WeakHashMap(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; Map var3 = this.longterm; synchronized(this.longterm) &#123; v = this.longterm.get(k); &#125; if (v != null) &#123; this.eden.put(k, v); &#125; &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= this.size) &#123; Map var3 = this.longterm; synchronized(this.longterm) &#123; this.longterm.putAll(this.eden); &#125; this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; TreeMapTreeMap是利用红黑树来实现的（树中的每个节点的值，都会大于或等于它的左子树种的所有节点的值，并且小于或等于它的右子树中的所有节点的值），实现了SortMap接口，能够对保存的记录根据key进行排序。所以一般需要排序的情况下是选择TreeMap来进行，默认为升序排序方式（深度优先搜索），可自定义实现Comparator接口实现排序方式。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>LinkedHashMap</tag>
        <tag>WeakHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java List]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FList%2F</url>
    <content type="text"><![CDATA[ArrayListArrayList 的底层是数组默认大小10，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。ArrayList 中的操作不是线程安全的！在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 ArrayList核心源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量, 在 add 大量元素之前用 ensureCapacity 方法，以减少增量从新分配的次数 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125;//判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为ArrayList定义的最大容量，否则，新容量大小则为 minCapacity。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); &#125; &#125; /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() &#123; return Arrays.copyOf(elementData, size); &#125; /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings("unchecked") public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // 新建一个运行时类型的数组，但是ArrayList数组的内容 return (T[]) Arrays.copyOf(elementData, size, a.getClass()); //调用System提供的arraycopy()方法实现数组之间的复制 System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; &#125; /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; &#125; /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; /** * 从列表中删除所有元素。 */ public void clear() &#123; modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; &#125; /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) &#123; modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i &lt; size; i++) &#123; elementData[i] = null; &#125; size = newSize; &#125; /** * 检查给定的索引是否在范围内。 */ private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * 返回IndexOutOfBoundsException细节信息 */ private String outOfBoundsMsg(int index) &#123; return "Index: "+index+", Size: "+size; &#125; /** * 从此列表中删除指定集合中包含的所有元素。 */ public boolean removeAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); &#125; /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true); &#125; /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator(int index) &#123; if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException("Index: "+index); return new ListItr(index); &#125; /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator() &#123; return new ListItr(0); &#125; /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; System.arraycopy()和Arrays.copyOf()方法 通过上面源码我们发现这两个实现数组复制的方法被广泛使用而且很多地方都特别巧妙。比如下面add(int index, E element)方法就很巧妙的用到了arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置: 12345678910111213141516&gt; /**&gt; * 在此列表中的指定位置插入指定的元素。 &gt; *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大；&gt; *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。&gt; */&gt; public void add(int index, E element) &#123;&gt; rangeCheckForAdd(index);&gt; &gt; ensureCapacityInternal(size + 1); // Increments modCount!!&gt; //arraycopy()方法实现数组自己复制自己&gt; //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量；&gt; System.arraycopy(elementData, index, elementData, index + 1, size - index);&gt; elementData[index] = element;&gt; size++;&gt; &#125;&gt; 又如toArray()方法中用到了copyOf()方法 12345678910&gt; /**&gt; *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 &gt; *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。&gt; *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。&gt; */&gt; public Object[] toArray() &#123;&gt; //elementData：要复制的数组；size：要复制的长度&gt; return Arrays.copyOf(elementData, size);&gt; &#125;&gt; 两者联系与区别联系：​ 看两者源代码可以发现copyOf()内部调用了System.arraycopy()方法区别： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf()是系统自动在内部新建一个数组，并返回该数组。 ArrayList 核心扩容技术1234567891011121314151617181920212223242526272829303132333435363738//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容,上面两个方法都要调用 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 如果说minCapacity也就是所需的最小容量大于保存ArrayList数据的数组的长度的话，就需要调用grow(minCapacity)方法扩容。 //这个minCapacity到底为多少呢？举个例子在添加元素(add)方法中这个minCapacity的大小就为现在数组的长度加1 if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 12345678910111213141516171819202122/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; //elementData为保存ArrayList数据的数组 ///elementData.length求数组长度elementData.size是求数组中的元素个数 // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为ArrayList定义的最大容量，否则，新容量大小则为 minCapacity。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; java 中的length 属性是数组属性 java 中的length()方法是字符串方法 .java 中的size()方法是集合方法 内部类1234(1)private class Itr implements Iterator&lt;E&gt; (2)private class ListItr extends Itr implements ListIterator&lt;E&gt; (3)private class SubList extends AbstractList&lt;E&gt; implements RandomAccess (4)static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; ArrayList有四个内部类，其中的Itr是实现了Iterator接口，同时重写了里面的hasNext()，next()，remove()等方法；其中的ListItr继承Itr，实现了ListIterator接口，同时重写了hasPrevious()，nextIndex()，previousIndex()，previous()，set(E e)，add(E e)等方法，所以这也可以看出了Iterator和ListIterator的区别:ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 LinkedList LinkedList 底层是基于双向链表实现的。 add(E e) 123456789101112131415161718192021222324252627282930transient Node&lt;E&gt; last;private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125;public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; add(int index, E element) 12345678910111213141516171819202122232425262728293031323334353637public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 由此可以看出是使用二分查找，看 index距离两端的距离来判断是从头结点正序查还是从尾节点倒序查，这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 remove(Object o) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public boolean remove(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; // 从头开始遍历 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; // 找到元素 if (o.equals(x.item)) &#123; // 从链表中移除找到的元素 unlink(x); return true; &#125; &#125; &#125; return false;&#125;E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // 删除前驱指针 if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; // 删除后继指针 if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; ArrayListArrayList 是基于数组实现的，实现了 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; ArrayList相当于动态数据，其中最重要的两个属性分别是:elementData 数组，以及 size 大小。在调用 add() 方法的时候： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话： 12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码: 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，减少扩容操作的次数。更要减少在指定位置插入数据的操作。 删除元素时需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 序列化由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 123456789101112131415161718192021222324252627282930313233343536373839404142 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125;// modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125; &#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 VectorVector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 以下是 add() 方法： 123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据: 1234567891011121314public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125; LinkedList基于双向链表实现，使用 Node 存储链表节点信息。 12345private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; 每个链表存储了 first 和 last 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 总结 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，LinkedList 不支持； LinkedList 在任意位置添加删除元素更快。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>ArrayList</tag>
        <tag>LinkedList</tag>
        <tag>Vector</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F15%2FJVM%2FJVM%2F</url>
    <content type="text"><![CDATA[简单介绍一下Class类文件结构（常量池主要存放的是那两大常量？Class文件的继承关系是如何确定的？字段表、方法表、属性表主要包含那些信息？） 字面量和符号引用 接口索引集合]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F15%2FJVM%2FJMM%2F</url>
    <content type="text"><![CDATA[线程安全问题一般是因为主内存和本地内存数据不一致和重排序导致的 JMM java内存模型是共享内存的并发模型，线程之间主要通过读/写共享变量来完成隐式通信 在JVM中所有实例域，静态域和数组元素都是放在堆内存中是共享的，而局部变量、方法定义参数、异常处理器参数是线程私有的 重排序 在执行程序时，为了提高性能，编译器和处理器常常会对指令进行重排序 1源代码→编译器优化重排序→指令集并行重排序→内存系统重排序→最终执行的指令序列 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序 as-if-serial语义保证不管怎么重排序，单线程内程序的执行结果不被改变 happens-before关系保证正确同步的多线程程序的执行结果不被改变.JMM可以通过happens-before提供跨线程的内存可见性保证 happens-before关系并不代表了最终的执行顺序 出现线程安全的问题一般是因为主内存和工作内存数据不一致性和重排序导致的 happens-before 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 程序中断规则：对线程interrupted()方法的调用先行于被中断线程的代码检测到中断时间的发生。 对象finalize规则：一个对象的初始化完成（构造函数执行结束）先行于发生它的finalize()方法的开始。 在Java中线程之间通过写-读共享内存来隐式进行通信。 JVM中堆、方法区是线程共享的，虚拟机栈、本地方法栈、程序计数器是非线程共享的。堆内存中存放类的实例、静态变量和数组。虚拟机栈中存放局部变量。 为了提高性能，编译器和处理器常常会对指令做重排序。 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，重新安排语句的执行顺序。 指令集并行的重排序：语句不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序：由于处理器使用缓存和读写缓冲区，加载和存储操作乱序。 读写顺序不一致 happens-before前一个操作的执行结果对后一个操作可见，且前一个操作顺序排在后一个操作之前，并不意味着前一个操作必要要在后一个操作之前执行？？？？ 程序顺序规则：一个线程中每个操作，happens-before于该线程中的任意后续操作 监视器锁规则：对一个监视器的解锁，happens-before于随后对这个监视器的加锁 volatile变量规则：对一个volatile的写，happens-before于任意后续对这个volatile域的读 传递性：如果A happens-before B，B happens-before C，那么A happens-before C 当代码中存在控制依赖性时，会影响指令序列执行的并行度。编译器和处理器会采用猜测执行，将计算结果临时保存到一个名为重排序缓冲区的硬件缓存中，猜对了就把计算结果写入。 64位的long型和double型变量的写操作拆分为两个32位的写操作来操作 volatile实现原理在对被volatile修饰的共享变量进行写操作时，会多出Lock前缀的指令。作用是： 将当前处理器缓存行的数据写回到系统内存； 使得其他CPU里缓存了该内存地址的数据无效； 当处理器发现本地缓存失效后，就会从内存中重新读取该变量数据。 通过这样的机制就使得每个线程都能获得该变量的最新值，从而避免出现数据脏读的现象。 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，操作完成后不知道何时将数据写回到内存。如果对声明了volatile的变量进行写操作，虚拟机就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。在多处理器下为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议（每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期），当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 volatile与重排序为了实现volatile的内存语义，JMM会通过插入内存屏障指令来限制特定类型的编译器和处理器重排序 在每个volatile读后面插入一个LoadLoad屏障：禁止下面所有的普通读操作和上面的volatile读重排序 在每个volatile读后面插入一个LoadStore屏障：禁止下面所有的普通写操作和上面的volatile读重排序 在每个volatile写前面插入一个StoreStore屏障：禁止上面的普通写和下面的volatile写重排序 在每个volatile写后面插入一个StoreLoad屏障：防止上面的volatile写与下面可能有的volatile读/写重排序 可见性：对一个volatile变量的读，总是能看到任意线程对这个volatile变量最后的写入 原子性：对任意单个volatile变量的读写具有原子性，但类似于i++这种复合操作不具有原子性 concurrent包的源代码实现 声明共享变量volatile 使用CAS更新来实现线程之间的同步 配合volatile的读写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信 使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据 使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率 final 在构造函数内对一个final域的写入，与随后把这个构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序 初次读一个包含final域的对象的引用与随后读这个final域，这两个操作之间不能重排序]]></content>
  </entry>
  <entry>
    <title><![CDATA[JDK中的设计模式]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FJDK%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[结构型模式： 适配器：用来把一个接口转化成另一个接口，如 java.util.Arrays#asList()。java.io.InputStreamReader(InputStream)java.io.OutputStreamWriter(OutputStream) 桥接模式：这个模式将抽象和抽象操作的实现进行了解耦，这样使得抽象和实现可以独立地变化，如JDBC； 组合模式：使得客户端看来单个对象和对象的组合是同等的。换句话说，某个类型的方法同时也接受自身类型作为参数，如 Map.putAll，List.addAll、Set.addAll。 装饰者模式：动态的给一个对象附加额外的功能，这也是子类的一种替代方式，如 java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap。java.io.BufferedInputStream(InputStream)java.io.DataInputStream(InputStream)java.io.BufferedOutputStream(OutputStream) 享元模式：使用缓存来加速大量小对象的访问时间，如 valueOf(int)。 代理模式：代理模式是用一个简单的对象来代替一个复杂的或者创建耗时的对象，如 java.lang.reflect.Proxy 门面模式：给一组组件，接口，抽象，或者子系统提供一个简单的接口。 java.lang.Class javax.faces.webapp.FacesServlet 创建模式: 抽象工厂模式：抽象工厂模式提供了一个协议来生成一系列的相关或者独立的对象，而不用指定具体对象的类型，如 java.util.Calendar#getInstance()。java.util.Arrays#asList()java.util.ResourceBundle#getBundle()java.sql.DriverManager#getConnection()java.sql.Connection#createStatement()java.sql.Statement#executeQuery() 建造模式(Builder)：定义了一个新的类来构建另一个类的实例，以简化复杂对象的创建，如：java.lang.StringBuilder#append()。 工厂方法：就是 一个返 回具体对象的方法，而不是多个，如 java.lang.Object#toString()、java.lang.Class#newInstance()。java.lang.Class#newInstance()java.lang.Class#forName() 原型模式：使得类的实例能够生成自身的拷贝、如：java.lang.Object#clone()。java.lang.Object#clone()java.lang.Cloneable 单例模式 java.lang.Runtime#getRuntime() 行为模式： 责任链模式：通过把请求从一个对象传递到链条中下一个对象的方式，直到请求被处理完毕，以实现对象间的解耦。如 javax.servlet.Filter#doFilter()。 java.util.logging.Logger#log() 命令模式：将操作封装到对象内，以便存储，传递和返回，如：java.lang.Runnable。 解释器模式：定义了一个语言的语法，然后解析相应语法的语句，如，java.text.Format，java.text.Normalizer。 迭代器模式：提供一个一致的方法来顺序访问集合中的对象，如 java.util.Iterator。 中介者模式：通过使用一个中间对象来进行消息分发以及减少类之间的直接依赖，java.lang.reflect.Method#invoke()。java.util.Timerjava.util.concurrent.Executor#execute()java.util.concurrent.ExecutorService#submit()java.lang.reflect.Method#invoke() 空对象模式：如 java.util.Collections#emptyList()。 观察者模式：它使得一个对象可以灵活的将消息发送给感兴趣的对象，如 java.util.EventListener。javax.servlet.http.HttpSessionBindingListenerjavax.servlet.http.HttpSessionAttributeListenerjavax.faces.event.PhaseListener 模板方法模式：让子类可以重写方法的一部分，而不是整个重写，如 java.util.Collections#sort()。 状态模式通过改变对象内部的状态，使得你可以在运行时动态改变一个对象的行为。 java.util.Iterator javax.faces.lifecycle.LifeCycle#execute() 策略模式使用这个模式来将一组算法封装成一系列对象。通过传递这些对象可以灵活的改变程序的功能。 java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter() 模板方法模式让子类可以重写方法的一部分，而不是整个重写，你可以控制子类需要重写那些操作。 java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read()]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F15%2FJava%2Fjava%2F</url>
    <content type="text"><![CDATA[bootstrapclassloader 记载java. extclassloader 记载javax 扩展的 appclassloader 记载程序所在目录 自定义classloader loadclass 和 class.forname区别 还没链接 已经初始化的 为什么递归会导致stackoverflow？ 每次调用都会压入一个栈帧 堆和栈区别 栈自动释放 堆要gc 堆空间大 堆碎片多 123456789String a=new String(&quot;a&quot;);a.intern();String b=&quot;a&quot;;System.out.println(a==b);//falseString c=new String(&quot;a&quot;)+new String(&quot;a&quot;);c.intern();String d=&quot;aa&quot;;System.out.println(c==d);//true 1234Integer a1=64;Integer a2=64;Integer a3=128;System.out.println((a1+a2)==a3);//true +这个操作符不适用于Integer对象，首先两个Integer相加时会先进行自动拆箱操作，再进行数值相加 Integer对象无法与数值进行直接比较，所以会进行自动拆箱变成int 12345String a="aaa";String b="bbb";String c="aaa"+"bbb";// 常量池对象String d=a+b;// 堆内存对象System.out.println(c==d); Java的泛型是如何工作的 ? 什么是类型擦除 ? 泛型是通过类型擦除来实现的，编译器在编译时擦除了所有类型相关的信息，所以在运行时不存在任何类型相关的信息 什么是泛型中的限定通配符和非限定通配符 ? 这是另一个非常流行的Java泛型面试题。限定通配符对类型进行了限制。有两种限定通配符，一种是\&lt;? extends T&gt;它通过确保类型必须是T的子类来设定类型的上界，另一种是\&lt;? super T&gt;它通过确保类型必须是T的父类来设定类型的下界。泛型类型必须用限定内的类型来进行初始化，否则会导致编译错误。另一方面\&lt;?&gt;表示了非限定通配符，因为&lt;?&gt;可以用任意类型来替代。 方法重载是静态分派 方法重写是动态分派 (静态方法是静态分派，不能重写父类的静态方法) 域访问操作由编译器解析，不是多态的 静态代码块对于定义在它之后的静态变量，可以赋值，但是不能访问. 会报illegal forward reference Java是解析运行吗？ 不正确。Java源代码经过Javac编译成.class文件.class文件经JVM解析或编译运行。 解析:.class文件经过JVM内嵌的解析器解析执行。 编译:存在JIT编译器（即时编译器）把经常运行的代码作为”热点代码”编译与本地平台相关的机器码，并进行各种层次的优化。 AOT(Ahead-of-Time Compilation)编译器: Java 9提供的直接将所有代码编译成机器码执行。 final修饰成员变量： final修饰类变量：必须要在静态初始化块中指定初始值或者声明该类变量时指定初始值，而且只能在这两个地方之一进行指定 final修饰实例变量：必要要在非静态初始化块，声明该实例变量或者在构造器中指定初始值，而且只能在这三个地方进行指定 实例方法不能为final成员变量赋值 Arrays.asList使用注意点 在使用 asList 时不要将基本数据类型当做参数，asList接受的是泛型变长参数，8 个基本类型是无法作为 asList 的参数的， 要想作为泛型参数就必须使用其所对应的包装类型，但Java 中数组是一个对象，它是可以泛型化的 12345int[] ints = &#123;1,2,3,4,5&#125;;List list = Arrays.asList(ints);System.out.println(list.size());Class&lt;?&gt; clazz = list.get(0).getClass();System.out.println(clazz==ints.getClass()); asList 产生的列表不可操作，aslist方法返回的是Arrays类中的一个内部类，并没有add等方法 123456Integer[] ints = &#123;1,2,3,4,5&#125;;List&lt;Integer&gt; list = Arrays.asList(ints);//list.add(6);List&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(list);arrayList.add(6);arrayList.forEach(System.out::println); 精度问题：2.0-1.1打印结果0.89999 因为二进制无法准确描述1/10 &lt;9位用int ； &lt;18可以用long ；&gt;18必须使用bigdecimal。BigDecimal：add() 加； subtract() 减； multiply() 乘； divide() 除；构造是传String，否则还有精度问题 finally带return会覆盖返回值，否则finally里的赋值改变不了返回值 接口中的变量都是public static final，方法都是public public 所有类可见； protected 本包和子类可见； 默认 本包可见； private 本类可见 switch可以传 int short byte char string 类初始化顺序：父类静态变量、静态语句块 → 子类静态变量、静态语句块 → 父类实例变量、普通语句块 → 父类构造方法 → 子类实例变量、普通语句块 → 子类构造方法 SpringCloud调试时经常出现服务非正常关闭导致端口被占用的情况，使用cmd使用taskkill /f /t /im java.exe杀进程 如果父类没有实现序列化，而子类实现列序列化。那么父类中的成员没办法做序列化操作 List Array互转 1234// array -&gt; listArrayList&lt;String&gt; strList = new ArrayList&lt;&gt;(Arrays.asList("abc","def"));// list -&gt; arrayString[] strings = strList.toArray(new String[strList.size()]); ==与equals的区别 ==是判断两个对象的地址是不是相等 (基本数据类型比较的是值，引用数据类型比较的是内存地址) equals是判断两个变量或实例所指向的内存空间的值是不是相同，但它一般有两种使用情况： 情况1：类没有覆盖equals()方法。则通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了equals()方法。一般，我们都覆盖equals()方法来两个对象的内容相等；若它们的内容相等，则返回true(即，认为这两个对象相等)。 String中的equals方法是被重写过的，因为object的equals方法是比较的对象的内存地址，而String的equals方法比较的是对象的值。 hashCode equals 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,则equals方法都返回true 两个对象有相同的hashcode值，它们不一定是相等的,因为hashCode() 所使用的杂凑算法也许刚好会让多个对象传回相同的杂凑值 equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 为什么重写equals时必须重写hashCode方法？ 为什么要有hashCode 当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他已经加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。。这样我们就大大减少了equals的次数，相应就大大提高了执行速度。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java IO]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FJava%20IO%2F</url>
    <content type="text"><![CDATA[Java有多种比较典型的文件拷贝实现方式，比如： 1234567891011121314151617private static void copy(String sourcePath,String desPath)&#123; try &#123; FileChannel sourceChannel = new FileInputStream(new File(sourcePath)).getChannel(); FileChannel desChannel = new FileOutputStream(new File(desPath)).getChannel(); for (long count = sourceChannel.size();count&gt;0;)&#123; long transferred=sourceChannel.transferTo(sourceChannel.position(),count,desChannel); try &#123; sourceChannel.position(sourceChannel.position()+transferred); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; count-=transferred; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 知识扩展1.拷贝实现机制分析 而基于NIO transferTo的实现方式，在Linux和Unix上，则会使用到零拷贝技术省去了上下文切换的开销和不必要的内存拷贝，进而可能提高应用拷贝性能。注意，transferTo不仅仅是可以用在文件拷贝中，与其类似的，例如读取磁盘文件，然后进行Socket发送，同样可以享受这种机制带来的性能和扩展性提高。 通道Channel 通道是对原 I/O 包中的流的模拟，可以通过它读取和写入数据，所有数据都通过 Buffer 对象来处理。字节不会直接写入通道中，也不能直接从通道中读取字节。就像不能在高速公路正中间停下来一样，可以在加油站加油一样 通道与流的不同之处在于通道是双向的，而流只是在一个方向上移动，可以用于读、写或者同时用于读写 FileChannel：从文件中读写数据 DatagramChannel：通过 UDP 读写网络中数据 SocketChannel：通过 TCP 读写网络中数据 ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel 缓冲区Buffer 缓冲区是一个容器，包含包含一些要写入或者刚读出的数据。发送给一个通道的所有对象都必须首先放到缓冲区中，从通道中读取的任何数据都要读到缓冲区中 状态变量每一个缓冲区都有复杂的内部统计机制，它会跟踪缓冲区已经读了多少数据以及还有多少空间可以容纳更多的数据，会跟踪缓冲区包含多少数据以及还有多少数据要写入。每一个读/写操作都会改变缓冲区的状态。通过记录和跟踪这些变化，缓冲区就可能够内部地管理自己的资源。状态变量就是内部统计机制的关键。可以用三个值指定缓冲区在任意时刻的状态 position 表示当前已经读写的字节位置 limit 表示还可以读写的字节数 capacity 缓冲区中的最大数据容量 要将数据写到输出通道中。在这之前，必须调用 flip() 方法 将 limit 设置为当前 position 将 position 设置为 0 写position 到limit 间的数据 clear()重设缓冲区以便接收更多的字节 将 limit 设置为与 capacity 相同 设置 position 为 0 回到最初的模样 访问方法 get() 和 put() 方法直接访问缓冲区中的数据 选择器NIO 实现了 IO 多路复用中的 Reactor 模型，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。 通过配置监听的通道 Channel 为非阻塞，那么当 Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮询其它 Channel，找到 IO 事件已经到达的 Channel 执行。 因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件，对于 IO 密集型的应用具有很好地性能。 应该注意的是，只有套接字 Channel 才能配置为非阻塞，而 FileChannel 不能，为 FileChannel 配置非阻塞也没有意义。 读文件Demo12345678910111213File file=new File("C:\\Users\\桔子\\Desktop\\test.txt"); try &#123; FileInputStream fis=new FileInputStream(file); // 获取管道 FileChannel channel=fis.getChannel(); // 创建缓冲区 ByteBuffer buffer=ByteBuffer.allocate(1024); // 数据读到缓冲区 channel.read(buffer); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; 补充NIO入门 Java NIO浅析 NIO与AIO学习总结一 Java NIO 概览二 Java NIO 之 Buffer(缓冲区) Buffer(缓冲区)介绍: Java NIO Buffers用于和NIO Channel交互。 我们从Channel中读取数据到buffers里，从Buffer把数据写入到Channels； Buffer本质上就是一块内存区； 一个Buffer有三个属性是必须掌握的，分别是：capacity容量、position位置、limit限制。 Buffer的常见方法 Buffer clear() Buffer flip() Buffer rewind() Buffer position(int newPosition) 三 Java NIO 之 Channel（通道） Channel（通道）介绍 通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 NIO Channel通道和流的区别： FileChannel的使用 SocketChannel和ServerSocketChannel的使用 ️DatagramChannel的使用 Scatter / Gather Scatter: 从一个Channel读取的信息分散到N个缓冲区中(Buufer). Gather: 将N个Buffer里面内容按照顺序发送到一个Channel. 通道之间的数据传输 在Java NIO中如果一个channel是FileChannel类型的，那么他可以直接把数据传输到另一个channel。 transferFrom() :transferFrom方法把数据从通道源传输到FileChannel transferTo() :transferTo方法把FileChannel数据传输到另一个channel 四 Java NIO之Selector（选择器） Selector（选择器）介绍 Selector 一般称 为选择器 ，当然你也可以翻译为 多路复用器 。它是Java NIO核心组件中的一个，用于检查一个或多个NIO Channel（通道）的状态是否处于可读、可写。如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。 使用Selector的好处在于： 使用更少的线程来就可以来处理通道了， 相比使用多个线程，避免了线程上下文切换带来的开销。 Selector（选择器）的使用方法介绍 Selector的创建 1Selector selector = Selector.open(); 注册Channel到Selector(Channel必须是非阻塞的) 12channel.configureBlocking(false);SelectionKey key = channel.register(selector, Selectionkey.OP_READ); SelectionKey介绍 一个SelectionKey键表示了一个特定的通道对象和一个特定的选择器对象之间的注册关系。 从Selector中选择channel(Selecting Channels via a Selector) 选择器维护注册过的通道的集合，并且这种注册关系都被封装在SelectionKey当中. 停止选择的方法 wakeup()方法 和close()方法。 模板代码 有了模板代码我们在编写程序时，大多数时间都是在模板代码中添加相应的业务代码。 客户端与服务端简单交互实例 五 Java NIO之拥抱Path和Files一 文件I/O基石：Path： 创建一个Path File和Path之间的转换，File和URI之间的转换 获取Path的相关信息 移除Path中的冗余项 二 拥抱Files类： Files.exists() 检测文件路径是否存在 Files.createFile() 创建文件 Files.createDirectories()和Files.createDirectory()创建文件夹 Files.delete()方法 可以删除一个文件或目录 Files.copy()方法可以吧一个文件从一个地址复制到另一个位置 获取文件属性 遍历一个文件夹 Files.walkFileTree()遍历整个目录 六 NIO学习总结以及NIO新特性介绍 内存映射： 这个功能主要是为了提高大文件的读写速度而设计的。内存映射文件(memory-mappedfile)能让你创建和修改那些大到无法读入内存的文件。有了内存映射文件，你就可以认为文件已经全部读进了内存，然后把它当成一个非常大的数组来访问了。将文件的一段区域映射到内存中，比传统的文件处理速度要快很多。内存映射文件它虽然最终也是要从磁盘读取数据，但是它并不需要将数据读取到OS内核缓冲区，而是直接将进程的用户私有地址空间中的一部分区域与文件对象建立起映射关系，就好像直接从内存中读、写文件一样，速度当然快了。 七 Java NIO AsynchronousFileChannel异步文件通Java7中新增了AsynchronousFileChannel作为nio的一部分。AsynchronousFileChannel使得数据可以进行异步读写。 八 高并发Java（8）：NIO和AIO推荐阅读在 Java 7 中体会 NIO.2 异步执行的快乐Java AIO总结与示例]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FutureTask]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FFutureTask%2F</url>
    <content type="text"><![CDATA[FutureTask 实现了 RunnableFuture 接口，RunnableFuture 接口继承 Runnable 和 Future 接口，这使得 FutureTask 既可以当做一个任务执行，也可以有返回值。 12public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt;public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; FutureTask 可用于异步获取执行结果或取消执行任务的场景。FutureTask提供了启动和取消异步任务，查询异步任务是否计算结束以及获取最终的异步任务的结果的一些常用的方法。通过get()方法来获取异步任务的结果，但是会阻塞当前线程直至异步任务执行结束。一旦任务执行结束，任务不能重新启动或取消，除非调用runAndReset()方法。 在FutureTask的源码中为其定义了这些状态： 1234567private static final int NEW = 0;private static final int COMPLETING = 1;private static final int NORMAL = 2;private static final int EXCEPTIONAL = 3;private static final int CANCELLED = 4;private static final int INTERRUPTING = 5;private static final int INTERRUPTED = 6; get方法 当FutureTask处于未启动或已启动状态时，执行FutureTask.get()方法将导致调用线程阻塞 如果FutureTask处于已完成状态，调用FutureTask.get()方法将导致调用线程立即返回结果或者抛出异常 cancel方法 当FutureTask处于未启动状态时，执行cancel()方法将此任务永远不会执行 当FutureTask处于已启动状态时，执行cancel(true)方法将以中断线程的方式来阻止任务继续进行，如果执行cancel(false)将不会对正在执行任务的线程有任何影响 当FutureTask处于已完成状态时，执行cancel(…)方法将返回false 1234567891011public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Callable&lt;Integer&gt; callable=new MyCallable(); ExecutorService service = Executors.newCachedThreadPool(); // Future Future&lt;Integer&gt; future = service.submit(callable); System.out.println(future.get()); // FutureTask FutureTask&lt;Integer&gt; task=new FutureTask&lt;&gt;(callable); service.submit(task); System.out.println(task.get());&#125;]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>FutureTask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CountDownLatch和CyclicBarrier]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FCountDownLatch%E5%92%8CCyclicBarrier%E5%92%8CSemphore%2F</url>
    <content type="text"><![CDATA[https://www.jianshu.com/p/7c7a5df5bda6?ref=myread CountDownLatchCountDownLatch允许一个或多个线程，等待另一组线程完成操作，再继续执行。比如有一个任务A，它要等待其他4个任务执行完毕之后才能执行，此时就可以利用CountDownLatch来实现这种功能了。 构造函数 CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1，计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 构造器中的计数值（count）实际上就是需要等待的线程数量。这个值只能被设置一次，不能重置 countDown() 通知CountDownLatch对象，已经完成了任务。每调用一次这个方法，在构造函数中初始化的count值就减1。当n个线程都调用了这个方法使count的值等于0，主线程就能从await()处恢复，执行自己的任务 await() 主线程必须在启动其他线程后调用await() 这样主线程就会在这个方法上阻塞，直到其他线程完成各自的任务 await(long timeout, TimeUnit unit) 与上面的await方法功能一致，只不过这里有了时间限制，调用该方法的线程等到指定的timeout时间后，不管count是否减至为0，都会继续往下执行 CyclicBarrierCyclicBarrier允许一组线程相互之间等待，直到所有线程都达到一个集合点后再继续执行 构造函数 CyclicBarrier(int parties)：声明需要拦截的线程数 CyclicBarrier(int parties, Runnable barrierAction)：声明需要拦截的线程数并定义一个Runnable对象，在所有线程到达集合点后，执行Runnable任务 await() 等待直到所有的线程都到达指定的临界点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125;// 在CyclicBarrier中，同一批线程属于同一代。当有parties个线程到达barrier，generation就会被更新换代。其中broken标识该当前CyclicBarrier是否已经处于中断状态private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果当前线程被中断，则通过breakBarrier()终止CyclicBarrier，唤醒CyclicBarrier中所有等待线程。 if (Thread.interrupted()) &#123; breakBarrier(); throw new InterruptedException(); &#125; // 计数器-1 int index = --count; if (index == 0) &#123; // 线程都到达集合点 boolean ranAction = false; try &#123; final Runnable command = barrierCommand; if (command != null) // 如果barrierCommand不为null就执行 command.run(); ranAction = true; nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) breakBarrier(); &#125; &#125; // loop until tripped, broken, interrupted, or timed out for (;;) &#123; try &#123; if (!timed) // 阻塞等待 trip.await(); else if (nanos &gt; 0L) // 超时的情况 nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; // // 如果等待过程中，线程被中断 if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // "belong" to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; if (g.broken) throw new BrokenBarrierException(); // 如果“generation已经换代”，则返回index。 if (g != generation) return index; // 如果是“超时等待”，并且时间已到，则通过breakBarrier()终止CyclicBarrier，唤醒CyclicBarrier中所有等待线程，并抛出TimeoutException异常 if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; await(long timeout, TimeUnit unit) 与上面的await方法功能基本一致，只不过这里有超时限制，阻塞等待直至到达超时时间为止 getNumberWaiting() 获取当前有多少个线程阻塞等待在临界点上 isBroken() 用于查询阻塞等待的线程是否被中断 reset() 将屏障重置为初始状态，如果当前有线程正在临界点等待的话，将抛出BrokenBarrierException 实现方式基于ReentrantLock和Condition机制实现。除了getParties()方法，CyclicBarrier的其他方法都需要获取锁。 CyclicBarrier的内部定义了一个Lock对象，每当一个线程调用await()时，将拦截的线程数减1，然后判断剩余拦截数是否为初始值parties，如果不是，进入Lock对象的条件队列等待。如果是，执行barrierAction对象的Runnable方法，然后将锁的条件队列中的所有线程放入锁等待队列中，这些线程会依次的获取锁、释放锁 。 对于失败的同步尝试，CyclicBarrier 使用了一种 all-or-none 的破坏模式：如果因为中断、失败或者超时等原因，导致线程过早地离开了屏障点，那么在该屏障点等待的其他所有线程也将通过 BrokenBarrierException（如果它们几乎同时被中断，则用 InterruptedException）以反常的方式离开。 解除阻塞的情况 最后一个线程调用await() 当前线程被中断 其他正在该CyclicBarrier上等待的线程被中断 其他正在该CyclicBarrier上等待的线程超时 其他某个线程调用该CyclicBarrier的reset()方法 使用CyclicBarrier的线程都会阻塞在await方法上，所以在线程池中使用CyclicBarrier时要特别小心，如果线程池的线程过少，那么就会发生死锁了 区别 CountDownLatch只能使用一次，CyclicBarrier可以使用多次 CountDownLatch某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行不会阻塞，CyclicBarrier某个线程运行到某个点上之后，该线程阻塞，直到所有的线程都到达了这个点，线程才继续执行 CountDownLatch是线程组之间的等待，即一个或多个线程等待一组线程完成某件事情之后再执行；而CyclicBarrier则是线程组内的等待，即每个线程相互等待，即n个线程都被拦截之后，然后依次执行 Semaphore可以理解为信号量，用于控制资源能够被并发访问的线程数量。线程需要通过acquire()获取访问许可才能继续往下执行，否则只能在该方法处阻塞等待。当任务完成后，需要通过release()方法讲访问许可归还，以便其他线程能够继续获得访问许可，可以通过构造函数指定是否具有公平性，默认非公平性，这样也是为了保证吞吐量，不同的是它们获取信号量的机制：对于公平信号量而言，线程在尝试获取信号量许可时如果当前线程不在CLH队列的头部，则排队等候；而对于非公平信号量而言，无论当前线程是不是在CLH队列的头部，它都会直接获取信号量。 Semaphore用来做本地特殊资源的并发访问控制是相当合适的，如果需要进行流量控制，优先使用Semaphore]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CopyOnWrite]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FCopyOnWrite%2F</url>
    <content type="text"><![CDATA[可以对CopyOnWrite容器进行并发的读，而不需要加锁。CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，数据一致性要求不高的场景。 CopyOnWriteArrayList如果想list的读效率更高的话，保证读线程无论什么时候都不被阻塞，使用CopyOnWriteArrayList容器，写时复制的思想来通过延时更新的策略，放弃数据实时性实现数据的最终一致性，并且能够保证读线程间不阻塞，往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，往新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。对CopyOnWrite容器进行并发的读的时候，不需要加锁，因为当前容器不会添加任何元素 CopyOnWriteArrayList适用场景CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 但是 CopyOnWriteArrayList 有其缺陷： 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右；可能会造成频繁的minor GC和major GC 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 CopyOnWriteArrayList实现原理add() 12345678910111213141516171819public boolean add(E e) &#123; final ReentrantLock lock = this.lock; //1. 使用Lock,保证写线程在同一时刻只有一个，防止多次复制 lock.lock(); try &#123; //2. 获取旧数组引用 Object[] elements = getArray(); int len = elements.length; //3. 创建新的数组，并将旧数组的数据复制到新数组中 Object[] newElements = Arrays.copyOf(elements, len + 1); //4. 往新数组中添加新的数据 newElements[len] = e; //5. 将旧数组引用指向新的数组 setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; Arrays.copyof()用于复制指定的数组内容以达到扩容的目的，该方法对不同的基本数据类型都有对应的重载方法 CopyOnWrite与ReadWriteLock对比CopyOnWrite和ReadWriteLock都是通过读写分离的思想实现，读线程之间互不阻塞 ReadWriteLock：对读线程而言，为了实现数据实时性，在写锁被获取后，读线程会等待或者当读锁被获取后，写线程会等待。 CopyOnWrite：牺牲数据实时性而保证数据最终一致性，即读线程对数据的更新是延时感知的，读线程不会存在等待的情况]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap的工作原理及代码实现]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FConcurrentHashMap%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[ConcurrentHashMap 实现原理 1.8 中的 ConcurrentHashMap 数据结构和实现与 1.7 还是有着明显的差异。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125; 其中的 val next 都用了 volatile 修饰，保证了可见性。 put 方法重点来看看 put 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode());// 计算hash int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0)// 判断是否需要初始化 tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123;// f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED)// 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 tab = helpTransfer(tab, f); else &#123;// 如果都不满足，则利用 synchronized 锁写入数据。 V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD)// 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125; get 方法12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 都不满足那就按照链表的方式遍历获取值。 123456789/** * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. */private transient volatile int sizeCtl; 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。还后面可以看到，它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125; 对value和next属性设置了volatile同步锁 允许调用setValue方法直接改变Node的value域 增加了find方法辅助map.get()方法 12345678910111213// 获得在i位置上的Node节点static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;// 利用CAS算法设置i位置上的Node节点static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;// 利用volatile方法设置节点位置的值static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; initTable 12345678910111213141516171819202122private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0)// sizeCtl&lt;0表示有其他线程正在进行初始化操作，把线程挂起 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123;// 利用CAS方法把sizectl的值置为-1 表示本线程正在进行初始化 try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2);// 相当于0.75*n 设置一个扩容的阈值 &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 扩容方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1];// 构造一个两倍容量的nextTable对象 nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true;// 并发扩容的关键属性 如果等于true 说明这个节点已经处理过 boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123;// 如果所有的节点都已经完成复制工作 就把nextTable赋值给table 清空临时对象nextTable nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1);// 扩容阈值设置为原来容量的1.5倍 依然相当于现在容量的0.75倍 return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123;// 利用CAS方法更新这个扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null)// 如果遍历到的节点为空 则放入ForwardingNode指针 advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED)// 如果遍历到ForwardingNode节点，说明这个点已经被处理过了，直接跳过，这里是控制并发扩容的核心 advance = true; // already processed else &#123; synchronized (f) &#123;// 节点上锁 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123;// 如果fh&gt;=0 证明这是一个Node节点 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f;// 以下的部分在完成的工作是构造两个链表 一个是原链表 另一个是原链表的反序排列 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln);// 在nextTable的i位置上插入一个链表 setTabAt(nextTab, i + n, hn);// 在nextTable的i+n的位置上插入另一个链表 setTabAt(tab, i, fwd);// 在table的i位置上插入forwardNode节点 表示已经处理过该节点 advance = true;// 设置advance为true 返回到上面的while循环中 就可以执行i--操作 &#125; else if (f instanceof TreeBin) &#123;// 对TreeBin对象进行处理 与上面的过程类似 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123;// 构造正序和反序两个链表 int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t;// 如果扩容后已经不再需要tree的结构 反向转换为链表结构 hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln);// 在nextTable的i位置上插入一个链表 setTabAt(nextTab, i + n, hn);// 在nextTable的i+n的位置上插入另一个链表 setTabAt(tab, i, fwd);// 在table的i位置上插入forwardNode节点 表示已经处理过该节点 advance = true;// 设置advance为true 返回到上面的while循环中 就可以执行i--操作 &#125; &#125; &#125; &#125; &#125;&#125; 扩容就是遍历、复制的过程。首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。 如果遍历到的节点是forward节点，就向后继续遍历，再加上给节点上锁的机制，就完成了多线程的控制。多线程遍历节点，处理了一个节点，就把对应点的值set为forward，另一个线程看到forward，就向后遍历。这样交叉就完成了复制工作。而且还很好的解决了线程安全的问题。 这个方法的设计实在是让我膜拜 https://blog.csdn.net/jianghuxiaojin/article/details/52006118#commentBox https://blog.csdn.net/fjse51/article/details/55260493 http://www.cnblogs.com/huaizuo/archive/2016/04/20/5413069.html http://www.cnblogs.com/everSeeker/p/5601861.html ConcurrentHashMap不允许key或value为null值 在多线程中可能有以下两个情况 如果一个或多个线程正在对ConcurrentHashMap进行扩容操作，当前线程也要进入扩容的操作中。这个扩容的操作之所以能被检测到，是因为transfer方法中在空结点上插入forward节点，如果检测到需要插入的位置被forward节点占有，就帮助进行扩容； 如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多。 如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多。 更多 HashMap 与 ConcurrentHashMap 相关请查看这里。 JDK 1.8 中使用 CAS + synchronized + Node + 红黑树。锁粒度：Node（首结点）（实现 Map.Entry&lt;K,V&gt;）。锁粒度降低了。 Q：ConcurrentHashMap 在 JDK 1.8 中，为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock？ A：①、粒度降低了； ②、JVM 开发团队没有放弃 synchronized，而且基于 JVM 的 synchronized 优化空间更大，更加自然。 ③、在大量的数据操作下，对于 JVM 的内存压力，基于 API 的 ReentrantLock 会开销更多的内存。 Q：ConcurrentHashMap 简单介绍？ A： ①、重要的常量： private transient volatile int sizeCtl; 当为负数时，-1 表示正在初始化，-N 表示 N - 1 个线程正在进行扩容； 当为 0 时，表示 table 还没有初始化； 当为其他正数时，表示初始化或者下一次进行扩容的大小。 ②、数据结构： Node 是存储结构的基本单元，继承 HashMap 中的 Entry，用于存储数据； TreeNode 继承 Node，但是数据结构换成了二叉树结构，是红黑树的存储结构，用于红黑树中存储数据； TreeBin 是封装 TreeNode 的容器，提供转换红黑树的一些条件和锁的控制。 ③、存储对象时（put() 方法）： 1.如果没有初始化，就调用 initTable() 方法来进行初始化； 2.如果没有 hash 冲突就直接 CAS 无锁插入； 3.如果需要扩容，就先进行扩容； 4.如果存在 hash 冲突，就加锁来保证线程安全，两种情况：一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入； 5.如果该链表的数量大于阀值 8，就要先转换成红黑树的结构，break 再一次进入循环 6.如果添加成功就调用 addCount() 方法统计 size，并且检查是否需要扩容。 ④、扩容方法 transfer()：默认容量为 16，扩容时，容量变为原来的两倍。 helpTransfer()：调用多个工作线程一起帮助进行扩容，这样的效率就会更高。 ⑤、获取对象时（get()方法）： 1.计算 hash 值，定位到该 table 索引位置，如果是首结点符合就返回； 2.如果遇到扩容时，会调用标记正在扩容结点 ForwardingNode.find()方法，查找该结点，匹配就返回； 3.以上都不符合的话，就往下遍历结点，匹配就返回，否则最后就返回 null。 （1）ConcurrentHashMap的锁分段技术 （2）ConcurrentHashMap的读是否要加锁，为什么 （3）ConcurrentHashMap的迭代器是强一致性的迭代器还是弱一致性的迭代器 在使用HashMap时在多线程情况下扩容会出现CPU接近100%的情况，因为HashMap并不是线程安全的，ConcurrentHashMap就是线程安全的map，其中利用了锁分段的思想提高了并发度。JDK1.8前中的ConcurrentHashmap主要使用Segment来实现减小锁粒度，分割成若干个Segment，在put的时候需要锁住Segment，get时候不加锁，使用volatile来保证可见性，当要统计全局时（比如size），首先会尝试多次计算modcount来确定，这几次尝试中，是否有其他线程进行了修改操作，如果没有，则直接返回size。如果有，则需要依次锁住所有的Segment来计算。JDK1.8之前put定位节点时要先定位到具体的segment，然后再在segment中定位到具体的桶。而在1.8的时候摒弃了segment臃肿的设计，直接针对的是Node[] tale数组中的每一个桶，进一步减小了锁粒度。并且防止拉链过长导致性能下降，当链表长度大于8的时候采用红黑树的设计。 主要设计上的变化有以下几点: 不采用segment而采用node，锁住node来实现减小锁粒度。 设计了MOVED状态 当resize的中过程中 线程2还在put数据，线程2会帮助resize。 使用3个CAS操作来确保node的一些操作的原子性，这种方式代替了锁。 sizeCtl的不同值来代表不同含义，起到了控制的作用。 采用synchronized而不是ReentrantLock JDK 1.8的ConcurrentHashMap就有了很大的变化1.8版本舍弃了segment，底层数据结构改变为采用数组+链表+红黑树的数据形式.并且大量使用了synchronized，以及CAS无锁操作以保证ConcurrentHashMap操作的线程安全性。至于为什么不用ReentrantLock而是Synchronzied呢？实际上，synchronzied做了很多的优化，包括偏向锁，轻量级锁，重量级锁，可以依次向上升级锁状态，但不能降级.因此，使用synchronized相较于ReentrantLock的性能会持平甚至在某些情况更优 ConcurrentHashMap是一个哈希桶数组，如果不出现哈希冲突的时候，每个元素均匀的分布在哈希桶数组中。当出现哈希冲突的时候，使用拉链法将hash值相同的节点构成链表，另外在JDK1.8版本中为了防止链表过长，当链表的长度大于8的时候会将链表转换成红黑树。table数组中的每个元素实际上是单链表的头结点或者红黑树的根节点 关键属性及类属性transient volatile Node&lt;K,V&gt;[] table; 装载Node的数组，作为ConcurrentHashMap的数据容器，采用懒加载的方式，直到第一次插入数据的时候才会进行初始化操作，数组的大小总是为2的幂次方。 private transient volatile Node&lt;K,V&gt;[] nextTable; 扩容时使用，平时为null，只有在扩容的时候才为非null private transient volatile int sizeCtl; 该属性用来控制table数组的大小，根据是否初始化和是否正在扩容有几种情况：(1)当值为负数时：如果为-1表示正在初始化，如果为-N则表示当前正有N-1个线程进行扩容操作；(2)当值为正数时：如果当前数组为null的话表示table在初始化过程中，sizeCtl表示为需要新建数组的长度；(3)若已经初始化了，表示当前数据容器（table数组）可用容量也可以理解成临界值（插入节点数超过了该临界值就需要扩容），具体指为数组的长度n 乘以 加载因子loadFactor；(4)当值为0时，即数组长度为默认初始值。 sun.misc.Unsafe U 在ConcurrentHashMapde的实现中可以看到大量的U.compareAndSwapXXXX的方法去修改ConcurrentHashMap的一些属性。这些方法实际上是利用了CAS算法保证了线程安全性，这是一种乐观策略，假设每一次操作都不会产生冲突，当且仅当冲突发生的时候再去尝试。CAS(V,O,N)核心思想为：若当前变量实际值V与期望的旧值O相同，则表明该变量没被其他线程进行修改，因此可以安全的将新值N赋值给变量；若当前变量实际值V与期望的旧值O不相同，则表明该变量已经被其他线程做了处理，此时将新值N赋给变量操作就是不安全的，在进行重试。 类Node 实现了Map.Entry接口，主要存放key-value，并且具有next域。用volatile进行修饰属性，为了保证内存可见性 123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; &#125; TreeNode 树节点，继承于承载数据的Node类。而红黑树的操作是针对TreeBin类的，从该类的注释也可以看出，也就是TreeBin会将TreeNode进行再一次封装 1234567static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; &#125; TreeBin 并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象 12345678910static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock &#125; ForwardingNode 在扩容时才会出现的特殊节点，其key,value,hash全部为null。并拥有nextTable指针引用新的table数组。 1234567static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; &#125; CAS关键操作在ConcurrentHashMap中利用CAS算法来保障线程安全的操作 tabAt 用来获取table数组中索引为i的Node元素 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); &#125; casTabAt 利用CAS操作设置table数组中索引为i的元素 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; setTabAt 用来设置table数组中索引为i的元素 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v); &#125; 重点方法在熟悉上面的这核心信息之后，我们接下来就来依次看看几个常用的方法是怎样实现的。 实例构造器方法在使用ConcurrentHashMap第一件事自然而然就是new 出来一个ConcurrentHashMap对象，一共提供了如下几个构造器方法： 12345678910// 1. 构造一个空的map，即table数组还未初始化，初始化放在第一次插入数据时，默认大小为16ConcurrentHashMap()// 2. 给定map的大小ConcurrentHashMap(int initialCapacity) // 3. 给定一个mapConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m)// 4. 给定map的大小以及加载因子ConcurrentHashMap(int initialCapacity, float loadFactor)// 5. 给定map大小，加载因子以及并发度（预计同时操作数据的线程）ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) ConcurrentHashMap一共给我们提供了5中构造器方法，具体使用请看注释，我们来看看第2种构造器，传入指定大小时的情况，该构造器源码为： 1234567891011public ConcurrentHashMap(int initialCapacity) &#123; //1. 小于0直接抛异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //2. 判断是否超过了允许的最大值，超过了话则取最大值，否则再对该值进一步处理 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); //3. 赋值给sizeCtl this.sizeCtl = cap;&#125; 这段代码的逻辑请看注释，很容易理解，如果小于0就直接抛出异常，如果指定值大于了所允许的最大值的话就取最大值，否则，在对指定值做进一步处理。最后将cap赋值给sizeCtl,关于sizeCtl的说明请看上面的说明，当调用构造器方法之后，sizeCtl的大小应该就代表了ConcurrentHashMap的大小，即table数组长度。tableSizeFor做了哪些事情了？源码为： 12345678910111213/** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 通过注释就很清楚了，该方法会将调用构造器方法时指定的大小转换成一个2的幂次方数，也就是说ConcurrentHashMap的大小一定是2的幂次方，比如，当指定大小为18时，为了满足2的幂次方特性，实际上concurrentHashMapd的大小为2的5次方（32）。另外，需要注意的是，调用构造器方法的时候并未构造出table数组（可以理解为ConcurrentHashMap的数据容器），只是算出table数组的长度，当第一次向ConcurrentHashMap插入数据的时候才真正的完成初始化创建table数组的工作。 initTable1234567891011121314151617181920212223242526private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) // sizeCtl值为-1，说明其他线程正在初始化，所以让出时间片,保证只有一个线程正在进行初始化操作 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // 得出数组的大小 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组 @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; // 计算数组中可用的大小：实际大小n*0.75（加载因子） sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab; &#125; putVal123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 计算key的hash值,spread()重哈希，以减小Hash冲突 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // 如果当前table还没有初始化先调用initTable方法将tab进行初始化 tab = initTable(); // (n - 1) &amp; hash运算等价于对长度n取模，也就是hash%n 获取该位置上的元素判断是否为null else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // tab中索引为i的位置的元素为null，则直接使用CAS将值插入即可 if (casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) break; &#125; // 当前节点不为null，且该节点为特殊节点（forwardingNode）的话，就说明当前concurrentHashMap正在进行扩容操作 else if ((fh = f.hash) == MOVED) // 当前正在扩容 tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; // 当前为链表，将新的键值对插入到链表中 if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 找到hash值相同的key,覆盖旧值 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; // 如果到链表末尾仍未找到，则直接将新值插入到链表末尾即可 pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 判断当前table[i]是否是树节点 当前为红黑树，将新的键值对插入到红黑树中 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // 插入完键值对后再根据实际大小看是否需要转换成红黑树 if (binCount != 0) //当前链表节点个数大于等于TREEIFY_THRESHOLD的时候，调用treeifyBin方法将tabel[i]链表转换成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 对当前容量大小进行检查，如果超过了临界值（实际大小*加载因子）就需要扩容 addCount(1L, binCount); return null; &#125; 整体流程： 首先对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置 如果当前table数组还未初始化，先将table数组进行初始化操作 如果这个位置是null的，那么使用CAS操作直接放入 如果这个位置存在结点，说明发生了hash碰撞，首先判断这个节点的类型。如果该节点fh==MOVED(代表forwardingNode,数组正在进行扩容)的话，说明正在进行扩容 如果是链表节点（fh&gt;0）,则得到的结点就是hash值相同的节点组成的链表的头节点。需要依次向后遍历确定这个新加入的值所在位置。如果遇到key相同的节点，则只需要覆盖该结点的value值即可。否则依次向后遍历，直到链表尾插入这个结点 如果这个节点的类型是TreeBin的话，直接调用红黑树的插入方法进行插入新的节点 插入完节点之后再次检查链表长度，如果长度大于8，就把这个链表转换成红黑树 对当前容量大小进行检查，如果超过了临界值（实际大小*加载因子）就需要扩容 get1234567891011121314151617181920212223public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 计算key的hash值,spread()重哈希，以减小Hash冲突 int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // table[i]桶节点的key与查找的key相同，则直接返回 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 当前节点hash小于0说明为树节点，在红黑树中查找即可 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; // 从链表中查找，查找到则返回该节点的value，否则就返回null即可 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null; &#125; transfer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //1. 新建Node数组，容量为之前的两倍 if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; //2. 新建forwardingNode引用，在之后会用到 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 3. 确定遍历中的索引i while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //4.将原数组中的元素复制到新数组中去 //4.5 for循环退出，扩容结束修改sizeCtl属性 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //4.1 当前数组中第i个元素为null，用CAS设置成特殊节点forwardingNode(可以理解成占位符) else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //4.2 如果遍历到ForwardingNode节点 说明这个点已经被处理过了 直接跳过 这里是控制并发扩容的核心 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; //4.3 处理当前节点为链表的头结点的情况，构造两个链表，一个是原链表 另一个是原链表的反序排列 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //在nextTable的i位置上插入一个链表 setTabAt(nextTab, i, ln); //在nextTable的i+n的位置上插入另一个链表 setTabAt(nextTab, i + n, hn); //在table的i位置上插入forwardNode节点 表示已经处理过该节点 setTabAt(tab, i, fwd); //设置advance为true 返回到上面的while循环中 就可以执行i--操作 advance = true; &#125; //4.4 处理当前节点是TreeBin时的情况，操作和上面的类似 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 代码逻辑请看注释,整个扩容操作分为两个部分： 第一部分是构建一个nextTable,它的容量是原来的两倍，这个操作是单线程完成的。新建table数组的代码为:Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1],在原容量大小的基础上右移一位。 第二个部分就是将原来table中的元素复制到nextTable中，主要是遍历复制的过程。根据运算得到当前遍历的数组的位置i，然后利用tabAt方法获得i位置的元素再进行判断： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。设置为新容量的0.75倍代码为 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1)，仔细体会下是不是很巧妙，n&lt;&lt;1相当于n右移一位表示n的两倍即2n,n&gt;&gt;&gt;1左右一位相当于n除以2即0.5n,然后两者相减为2n-0.5n=1.5n,是不是刚好等于新容量的0.75倍即2n*0.75=1.5n。最后用一个示意图来进行总结（图片摘自网络）： 与size相关的一些方法1234567891011121314151617181920212223242526/** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125;&#125;/******************************************/ /** * 实际上保存的是hashmap中的元素个数 利用CAS锁进行更新 但它并不用返回当前hashmap的元素个数 */private transient volatile long baseCount;/** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */private transient volatile int cellsBusy;/** * Table of counter cells. When non-null, size is a power of 2. */private transient volatile CounterCell[] counterCells; mappingCount与size方法 mappingCount与size方法的类似 从给出的注释来看，应该使用mappingCount代替size方法 两个方法都没有直接返回basecount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 1234567891011121314151617181920212223242526272829303132public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125; /** * Returns the number of mappings. This method should be used * instead of &#123;@link #size&#125; because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n; // ignore transient negative values&#125; final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; return sum;&#125; addCount方法 在put方法结尾处调用了addCount方法，把当前ConcurrentHashMap的元素个数+1这个方法一共做了两件事,更新baseCount的值，检测是否进行扩容。 123456789101112131415161718192021222324252627282930313233343536373839404142private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; //利用CAS方法更新baseCount的值 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; //如果check值大于等于0 则需要检验是否需要进行扩容操作 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); // if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //如果已经有其他线程在执行扩容操作 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //当前线程是唯一的或是第一个发起扩容的线程 此时nextTable=null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 1.7版本与1.8版本的ConcurrentHashMap的实现对比 这篇文章。 ConcurrentHashMap http://www.importnew.com/22007.html http://www.jianshu.com/p/c0642afe03e0 HashMap http://www.importnew.com/20386.html 1.8以后的锁的颗粒度，是加在链表头上的，这个是个思路上的突破。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乐观锁]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FOptimistic%20locking%2F</url>
    <content type="text"><![CDATA[CAS使用锁时，线程获取锁是一种悲观锁策略，即假设每一次执行临界区代码都会产生冲突，所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁。而CAS操作是一种乐观锁策略，它假设所有线程访问共享资源的时候不会出现冲突，既然不会出现冲突自然而然就不会阻塞其他线程的操作。如果出现冲突就重试当前操作直到没有冲突为止。 操作过程CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。反之，V和O不相同，表明该值已经被其他线程改过了则该旧值O不是最新版本的值了，所以不能将新值N赋给V，返回V即可。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程 Synchronized VS CAS 1.5前的Synchronized在存在线程竞争的情况下会出现线程阻塞和唤醒锁带来的性能问题 CAS并不是武断的间线程挂起，当CAS操作失败后会进行一定的尝试，而非进行耗时的挂起唤醒的操作 CAS的应用场景在J.U.C包中利用CAS实现类有很多，可以说是支撑起整个concurrency包的实现，在Lock实现中会有CAS改变state变量，在atomic包中的实现类也几乎都是用CAS实现。 CAS与对象创建那就是在JVM创建对象的过程中。对象创建在虚拟机中是非常频繁的。即使是仅仅修改一个指针所指向的位置，在并发情况下也不是线程安全的，可能正在给对象A分配内存空间，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。解决这个问题的方案有两种，其中一种就是采用CAS配上失败重试的方式保证更新操作的原子性。 CAS的问题1. ABA问题因为CAS会检查旧值有没有变化，这里存在这样一个有意思的问题。比如一个旧值A变为了成B，然后再变成A，刚好在做CAS时检查发现旧值并没有变化依然为A，但是实际上的确发生了变化。解决方案可以沿袭数据库中常用的乐观锁方式，添加一个版本号可以解决。JDK1.5后的atomic包中提供了AtomicStampedReference来解决ABA问题。它通过包装[E,Integer]的元组来对对象标记版本戳stamp，从而避免ABA问题，例如下面的代码分别用AtomicInteger和AtomicStampedReference来对初始值为100的原子整型变量进行更新，AtomicInteger会成功执行CAS操作，而加上版本戳的AtomicStampedReference对于ABA问题会执行CAS失败。 2. 自旋时间过长 使用CAS时非阻塞同步，也就是说不会将线程挂起，会自旋（无非就是一个死循环）进行下一次尝试，如果这里自旋时间过长对性能是很大的消耗。 3. 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时CAS能保证其原子性，如果对多个共享变量进行操作,CAS就不能保证其原子性。有一个解决方案是利用对象整合多个共享变量，即一个类中的成员变量就是这几个共享变量。然后将这个对象做CAS操作就可以保证其原子性。atomic中提供了AtomicReference来保证引用对象之间的原子性。 多写场景冲突多，使用悲观锁 如synchronized 和 ReentrantLock 多读场景，使用乐观锁 如atomic 两种常见的实现方式版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点1 ABA 问题 AtomicStampedReference 类可以解决，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BlockingQueue阻塞队列]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FBlockingQueue%2F</url>
    <content type="text"><![CDATA[基本操作BlockingQueue继承Queue接口 方法 解释 add() 增加一个元素，如果队列已满，则抛出一个IIIegaISlabEepeplian异常 offer() 添加一个元素并返回true，如果队列已满，则返回false put() 添加一个元素，如果队列满，则阻塞 offer(E e, long timeout, TimeUnit unit) 往队列插入元素，队列满时，插入数据的线程会阻塞，直到队列有空余位置，超过给定的时间，插入的线程会退出 poll() 移除并返问队列头部的元素，队列为空返回null remove() 从队列中删除数据，成功返回true，失败返回false element() 不移除元素，返回队列头部的元素，如果队列为空，则抛出一个NoSuchElementException异常 peek() 不移除元素，返回队列头部的元素，队列为空返回null take() 移除并返回队列头部的元素，如果队列为空，则阻塞 poll(long timeout, TimeUnit unit) 取出队头元素，队列为空时，线程会阻塞，超过给定的时间，线程会退出 常见的阻塞队列ArrayBlockingQueue数组实现的有界阻塞队列，一旦创建，容量不能改变。当队列容量满时，插入元素会阻塞，队列为空时，获得元素也会阻塞。默认情况下非公平，不能保证线程访问队列的公平性，访问ArrayBlockingQueue的顺序不是遵守严格的时间顺序，有可能存在，一旦ArrayBlockingQueue可以被访问时，长时间阻塞的线程依然无法访问到队列 如果保证公平性，通常会降低吞吐量，初始化时除了设置队列的大小还要设为true 主要变量12345678910111213141516171819202122232425/** The queued items */ final Object[] items; /** items index for next take, poll, peek or remove */ int takeIndex; /** items index for next put, offer, or add */ int putIndex; /** Number of elements in the queue */ int count; /* * Concurrency control uses the classic two-condition algorithm * found in any textbook. */ /** Main lock guarding all access */ final ReentrantLock lock; /** Condition for waiting takes */ private final Condition notEmpty; /** Condition for waiting puts */ private final Condition notFull; 为了保证线程安全，采用的是ReentrantLock lock，为了保证可阻塞式的插入删除数据利用的是Condition，当获取数据的消费者线程被阻塞时会将该线程放置到notEmpty等待队列中，当插入数据的生产者线程被阻塞时，会将该线程放置到notFull等待队列中 主要方法put() 1234567891011121314public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //如果当前队列已满，将线程移入到notFull等待队列中 while (count == items.length) notFull.await(); //满足插入数据的要求，直接进行入队操作 enqueue(e); &#125; finally &#123; lock.unlock(); &#125;&#125; enqueue() 123456789101112private void enqueue(E x) &#123; // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; //插入数据,即往数组中添加数据 items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; //通知消费者线程，当前队列中有数据可供消费 notEmpty.signal();&#125; take() 12345678910111213public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; //如果队列为空，没有数据，将消费者线程移入等待队列中 while (count == 0) notEmpty.await(); //获取数据 return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; dequeue() 1234567891011121314151617private E dequeue() &#123; // assert lock.getHoldCount() == 1; // assert items[takeIndex] != null; final Object[] items = this.items; @SuppressWarnings(&quot;unchecked&quot;) //获取队列中的数据，即获取数组中的数据元素 E x = (E) items[takeIndex]; items[takeIndex] = null; if (++takeIndex == items.length) takeIndex = 0; count--; if (itrs != null) itrs.elementDequeued(); //通知notFull等待队列中的线程，使其由等待队列移入到同步队列中，使其能够有机会获得lock notFull.signal(); return x;&#125; 可以看出put()和take()主要是通过condition的通知机制来完成可阻塞式的插入数据和获取数据 LinkedBlockingQueue链表实现的有界阻塞队列，与ArrayBlockingQueue相比起来具有更高的吞吐量，初始化时需要指定大小，如果未指定，容量等于Integer.MAX_VALUE 主要变量1234567891011121314151617181920212223242526272829/** The capacity bound, or Integer.MAX_VALUE if none */ private final int capacity; /** Current number of elements */ private final AtomicInteger count = new AtomicInteger(); /** * Head of linked list. * Invariant: head.item == null */ transient Node&lt;E&gt; head; /** * Tail of linked list. * Invariant: last.next == null */ private transient Node&lt;E&gt; last; /** Lock held by take, poll, etc */ private final ReentrantLock takeLock = new ReentrantLock(); /** Wait queue for waiting takes */ private final Condition notEmpty = takeLock.newCondition(); /** Lock held by put, offer, etc */ private final ReentrantLock putLock = new ReentrantLock(); /** Wait queue for waiting puts */ private final Condition notFull = putLock.newCondition(); 插入数据和删除数据时分别是由两个不同的lock（takeLock和putLock）来控制线程安全的，两个对应的condition（notEmpty和notFull）来实现可阻塞的插入和删除数据，可以降低线程由于线程无法获取到lock而进入WAITING状态的可能性，从而提高了线程并发执行的效率 主要方法put() 12345678910111213141516171819202122232425262728293031323334public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); // Note: convention in all put/take/etc is to preset local var // holding count negative to indicate failure unless set. int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; /* * Note that count is used in wait guard even though it is * not protected by lock. This works because count can * only decrease at this point (all other puts are shut * out by lock), and we (or some other waiting put) are * signalled if it ever changes from capacity. Similarly * for all other uses of count in other wait guards. */ //如果队列已满，则阻塞当前线程，将其移入等待队列 while (count.get() == capacity) &#123; notFull.await(); &#125; //入队操作，插入数据 enqueue(node); c = count.getAndIncrement(); //若队列满足插入数据的条件，则通知被阻塞的生产者线程 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) signalNotEmpty();&#125; take() 123456789101112131415161718192021222324public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; //当前队列为空，则阻塞当前线程，将其移入到等待队列中，直至满足条件 while (count.get() == 0) &#123; notEmpty.await(); &#125; //移除队头元素，获取数据 x = dequeue(); c = count.getAndDecrement(); //如果当前满足移除元素的条件，则通知被阻塞的消费者线程 if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125; LinkedBlockingDeque链表实现的的有界阻塞双端队列 基本操作可以分为四种类型 特殊情况，抛出异常 特殊情况，返回特殊值如null或者false 当线程不满足操作条件时，线程会被阻塞直至条件满足 操作具有超时特性 PriorityBlockingQueue支持优先级的无界阻塞队列，默认情况下元素采用自然顺序进行排序，也可以通过实现Comparable接口的compareTo()方法来指定元素排序规则，或者初始化时通过构造器参数Comparator来指定排序规则 SynchronousQueue因为只有线程在删除数据时，其他线程才能插入数据。如果当前有线程在插入数据时，线程才能删除数据 LinkedTransferQueue链表实现的无界阻塞队列，实现了TransferQueue接口 transfer(E e)如果当前有消费线程想要获得队头元素而阻塞时，生产线程可以调用transfer方法将数据传递给消费线程。如果当前没有消费线程的话，生产线程就会将数据插入到队尾，直到有消费线程进行消费才退出 tryTransfer(E e)如果当前有消费线程正在消费数据，该方法可以将数据立即传送给消费线程，如果当前没有消费者线程消费数据的话，就立即返回false tryTransfer(E e,long timeout,imeUnit unit)如果在规定的时间内，数据没有被消费线程进行消费的话，就返回false DelayQueue实现Delayed接口的无界阻塞队列，只有当数据对象的延时时间达到时才能插入到队列进行存储。通过Delayed接口的getDelay(TimeUnit.NANOSECONDS)来进行判定，如果该方法返回的值小于等于0则说明该数据元素的延时期已满 实现生产者消费者生产者线程生产数据，消费者线程消费数据，为了解耦生产者和消费者的关系，通常会采用共享的数据区域，生产者生产数据之后直接放置在共享数据区中，这个共享数据区域中应该具备这样的线程间并发协作的功能 如果共享数据区已满的话，阻塞生产者继续生产数据放置入内 如果共享数据区为空的话，阻塞消费者继续消费数据 前面介绍了各种队列实现，在日常的应用开发中，如何进行选择呢？以LinkedBlockingQueue、ArrayBlockingQueue和SynchronousQueue为例，我们一起来分析一下，根据需求可以从很多方面考量：考虑应用场景中对队列边界的要求。ArrayBlockingQueue是有明确的容量限制的，而LinkedBlockingQueue则取决于我们是否在创建时指定，SynchronousQueue则干脆不能缓存任何元素。从空间利用角度，数组结构的ArrayBlockingQueue要比LinkedBlockingQueue紧凑，因为其不需要创建所谓节点，但是其初始分配阶段就需要一段连续的空间，所以初始内存需求更大。通用场景中，LinkedBlockingQueue的吞吐量一般优于ArrayBlockingQueue，因为它实现了更加细粒度的锁操作。ArrayBlockingQueue实现比较简单，性能更好预测，属于表现稳定的“选手”。如果我们需要实现的是两个线程之间接力性（handof）的场景，按照专栏上一讲的例子，你可能会选择CountDownLatch，但是SynchronousQueue也是完美符合这种场景的，而且线程间协调和数据传输统一起来，代码更加规范。可能令人意外的是，很多时候SynchronousQueue的性能表现，往往大大超过其他实现，尤其是在队列元素较小的场景。 Queue是一个FIFO的数据结构，Queue接口与List、Set一样都继承了Collection接口 Deque双向队列，队列两端的元素既能入队也能出队，LinkedList实现了Deque接口 非阻塞队列 PriorityQueue实质上维护了一个有序列表。加入到 Queue 中的元素根据它们的天然排序（通过其 java.util.Comparable 实现）或者根据传递给构造函数的 java.util.Comparator 实现来定位 ConcurrentLinkedQueue是基于链接节点的、线程安全的无界队列。并发访问不需要同步，需要遍历队列。 因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小，ConcurrentLinkedQueue 对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。采用CAS机制（compareAndSwapObject原子操作）。不支持阻塞去取元素 阻塞队列 LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。LinkedBlockingQueue的容量是没有上限的（说的不准确，在不指定时容量为Integer.MAX_VALUE，不要然的话在put时怎么会受阻呢），但是也可以选择指定其最大容量，它是基于链表的队列，此队列按 FIFO（先进先出）排序元素。支持阻塞的take()方法 。使用 ReentrantLock 锁,添加元素为原子操作的队列 ArrayBlockingQueue ：一个由数组支持的有界队列。ArrayBlockingQueue在构造时需要指定容量， 并可以选择是否需要公平性，如果公平参数被设置true，等待时间最长的线程会优先得到处理（其实就是通过将ReentrantLock设置为true来 达到这种公平性的：即等待时间最长的线程会先操作）。通常，公平性会使你在性能上付出代价，只有在的确非常需要的时候再使用它。它是基于数组的阻塞循环队 列，此队列按 FIFO（先进先出）原则对元素进行排序。 PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列，而不是先进先出队列。元素按优先级顺序被移除，该队列也没有上限（看了一下源码，PriorityBlockingQueue是对 PriorityQueue的再次包装，是基于堆数据结构的，而PriorityQueue是没有容量限制的，与ArrayList一样，所以在优先阻塞 队列上put时是不会受阻的。虽然此队列逻辑上是无界的，但是由于资源被耗尽，所以试图执行添加操作可能会导致 OutOfMemoryError），但是如果队列为空，那么取元素的操作take就会阻塞，所以它的检索操作take是受阻的。另外，往入该队列中的元 素要具有比较能力。 DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。DelayQueue（基于PriorityQueue来实现的）是一个存放Delayed 元素的无界阻塞队列，只有在延迟期满时才能从中提取元素。该队列的头部是延迟期满后保存时间最长的 Delayed 元素。如果延迟都还没有期满，则队列没有头部，并且poll将返回null。当一个元素的 getDelay(TimeUnit.NANOSECONDS) 方法返回一个小于或等于零的值时，则出现期满，poll就以移除这个元素了。此队列不允许使用 null 元素。 SynchronousQueue （并发同步阻塞队列）一个利用 BlockingQueue 接口的简单聚集（rendezvous）机制。 生产者-消费者阻塞队列支持设计模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.yao;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class BlockingQueueTest &#123; /** 定义装苹果的篮子 */ public static class Basket&#123; // 篮子，能够容纳3个苹果 BlockingQueue&lt;String&gt; basket = new ArrayBlockingQueue&lt;String&gt;(3); // 生产苹果，放入篮子 public void produce() throws InterruptedException&#123; // put方法放入一个苹果，若basket满了，等到basket有位置 basket.put("An apple"); &#125; // 消费苹果，从篮子中取走 public String consume() throws InterruptedException&#123; // get方法取出一个苹果，若basket为空，等到basket有苹果为止 String apple = basket.take(); return apple; &#125; public int getAppleNumber()&#123; return basket.size(); &#125; &#125; // 测试方法 public static void testBasket() &#123; // 建立一个装苹果的篮子 final Basket basket = new Basket(); // 定义苹果生产者 class Producer implements Runnable &#123; public void run() &#123; try &#123; while (true) &#123; // 生产苹果 System.out.println("生产者准备生产苹果：" + System.currentTimeMillis()); basket.produce(); System.out.println("生产者生产苹果完毕：" + System.currentTimeMillis()); System.out.println("生产完后有苹果："+basket.getAppleNumber()+"个"); // 休眠300ms Thread.sleep(300); &#125; &#125; catch (InterruptedException ex) &#123; &#125; &#125; &#125; // 定义苹果消费者 class Consumer implements Runnable &#123; public void run() &#123; try &#123; while (true) &#123; // 消费苹果 System.out.println("消费者准备消费苹果：" + System.currentTimeMillis()); basket.consume(); System.out.println("消费者消费苹果完毕：" + System.currentTimeMillis()); System.out.println("消费完后有苹果："+basket.getAppleNumber()+"个"); // 休眠1000ms Thread.sleep(1000); &#125; &#125; catch (InterruptedException ex) &#123; &#125; &#125; &#125; ExecutorService service = Executors.newCachedThreadPool(); Producer producer = new Producer(); Consumer consumer = new Consumer(); service.submit(producer); service.submit(consumer); // 程序运行10s后，所有任务停止 try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; &#125; service.shutdownNow(); &#125; public static void main(String[] args) &#123; BlockingQueueTest.testBasket(); &#125;&#125; 123451.LinkedBlockingQueue是使用锁机制，ConcurrentLinkedQueue是使用CAS算法，虽然LinkedBlockingQueue的底层获取锁也是使用的CAS算法2.关于取元素，ConcurrentLinkedQueue不支持阻塞去取元素，LinkedBlockingQueue支持阻塞的take()方法，如若大家需要ConcurrentLinkedQueue的消费者产生阻塞效果，需要自行实现3.关于插入元素的性能，从字面上和代码简单的分析来看ConcurrentLinkedQueue肯定是最快的，但是这个也要看具体的测试场景，我做了两个简单的demo做测试，测试的结果如下，两个的性能差不多，但在实际的使用过程中，尤其在多cpu的服务器上，有锁和无锁的差距便体现出来了，ConcurrentLinkedQueue会比LinkedBlockingQueue快很多： linkedlist线程不安全]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Atomic]]></title>
    <url>%2F2019%2F05%2F15%2FJava%2FAtomic%2F</url>
    <content type="text"><![CDATA[基本类型原子类 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 AtomicInteger 类常用方法 1234567public final int get() //获取当前的值public final int getAndSet(int newValue)//获取当前的值，并设置新的值public final int getAndIncrement()//获取当前的值，并自增public final int getAndDecrement() //获取当前的值，并自减public final int getAndAdd(int delta) //获取当前的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update）public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 atomic包提供的LongAdder，在高度竞争环境下，可能就是比AtomicLong更佳的选择 数组类型原子类 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray ：引用类型数组原子类 AtomicIntegerArray 类常用方法 1234567public final int get(int i) //获取 index=i 位置元素的值public final int getAndSet(int i, int newValue)//返回 index=i 位置的当前的值，并将其设置为新值：newValuepublic final int getAndIncrement(int i)//获取 index=i 位置元素的值，并让该位置的元素自增public final int getAndDecrement(int i) //获取 index=i 位置元素的值，并让该位置的元素自减public final int getAndAdd(int delta) //获取 index=i 位置元素的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将 index=i 位置的元素值设置为输入值（update）public final void lazySet(int i, int newValue)//最终 将index=i 位置的元素设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 引用类型原子类基本类型原子类只能更新一个变量，如果需要原子更新多个变量，需要使用 引用类型原子类。 AtomicReference：原子更新引用类型； AtomicReferenceFieldUpdater：原子更新引用类型里的字段； Updater只能修改它可见范围内的变量。因为Updater使用反射得到这个变量。如果变量不可见，就会出错。 比如如果score申明为private，就是不可行的。 为了确保变量被正确的读取，它必须是volatile类型的。如果我们原有代码中未申明这个类型，那么简单得 申明一下就行，这不会引起什么问题。 由于CAS操作会通过对象实例中的偏移量直接进行赋值，因此，它不支持static字段（Unsafe. objectFieldOffset()不支持静态变量）。 AtomicMarkableReference：原子更新带有标记位的引用类型； 对象的属性修改类型原子类如果需要原子更新某个类里的某个字段时，需要用到对象的属性修改类型原子类。 AtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 要想使用原子更新字段需要两步操作： 原子更新字段类都是抽象类，只能通过静态方法newUpdater来创建一个更新器，并且需要设置想要更新的类和属性 更新类的属性必须使用public volatile进行修饰 上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerFieldUpdater为例子来介绍。 AtomicIntegerFieldUpdater 类使用示例 1234567891011121314151617181920public class AtomicDemo &#123; private static AtomicIntegerFieldUpdater updater = AtomicIntegerFieldUpdater.newUpdater(User.class,"age"); public static void main(String[] args) &#123; User user = new User("a", 1); int oldValue = updater.getAndAdd(user, 5); System.out.println(oldValue); System.out.println(updater.get(user)); &#125; static class User &#123; private String userName; public volatile int age; public User(String userName, int age) &#123; this.userName = userName; this.age = age; &#125; &#125;&#125;]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>Atomic</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F15%2FJava%2FAbstractQueuedSynchronizer%2F</url>
    <content type="text"><![CDATA[AQS 原理概览AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。 http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html https://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/showing/p/6858410.html 同步器是实现锁（也可以是任意同步组件）的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。可以这样理解二者的关系：锁是面向使用者，它定义了使用者与锁交互的接口，隐藏了实现细节；同步器是面向锁的实现者，它简化了锁的实现方式，屏蔽了同步状态的管理，线程的排队，等待和唤醒等底层操作。锁和同步器很好的隔离了使用者和实现者所需关注的领域。 AQS的模板方法设计模式 它将一些方法开放给子类进行重写，而同步器给同步组件所提供模板方法又会重新调用被子类所重写的方法。举个例子，AQS中需要重写的方法tryAcquire： 2. 同步队列在AQS有一个静态内部类Node，其中有这样一些属性： volatile int waitStatus //节点状态volatile Node prev //当前节点/线程的前驱节点volatile Node next; //当前节点/线程的后继节点volatile Thread thread;//加入同步队列的线程引用Node nextWaiter;//等待队列中的下一个节点 节点的状态有以下这些： int CANCELLED = 1//节点从同步队列中取消int SIGNAL = -1//后继节点的线程处于等待状态，如果当前节点释放同步状态会通知后继节点，使得后继节点的线程能够运行；int CONDITION = -2//当前节点进入等待队列中int PROPAGATE = -3//表示下一次共享式同步状态获取将会无条件传播下去int INITIAL = 0;//初始状态 现在我们知道了节点的数据结构类型，并且每个节点拥有其前驱和后继节点，很显然这是一个双向队列。 12private transient volatile Node head;private transient volatile Node tail; 也就是说AQS实际上通过头尾指针来管理同步队列，同时实现包括获取锁失败的线程进行入队，释放锁时对同步队列中的线程进行通知等核心方法。其示意图如下： 3. 独占锁3.1 独占锁的获取（acquire方法）我们继续通过看源码和debug的方式来看，还是以上面的demo为例，调用lock()方法是获取独占式锁，获取失败就将当前线程加入同步队列，成功则线程执行。而lock()方法实际上会调用AQS的acquire()方法，源码如下 1234567public final void acquire(int arg) &#123; //先看同步状态是否获取成功，如果成功则方法结束返回 //若失败则先调用addWaiter()方法再调用acquireQueued()方法 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 关键信息请看注释，acquire根据当前获得同步状态成功与否做了两件事情：1. 成功，则方法结束返回，2. 失败，则先调用addWaiter()然后在调用acquireQueued()方法。 获取同步状态失败，入队操作 当线程获取独占式锁失败后就会将当前线程加入同步队列，那么加入队列的方式是怎样的了？我们接下来就应该去研究一下addWaiter()和acquireQueued()。addWaiter()源码如下： 123456789101112131415161718private Node addWaiter(Node mode) &#123; // 1. 将当前线程构建成Node类型 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 2. 当前尾节点是否为null？ Node pred = tail; if (pred != null) &#123; // 2.2 将当前节点尾插入的方式插入同步队列中 node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 2.1. 当前同步队列尾节点为null，说明当前线程是第一个加入同步队列进行等待的线程 enq(node); return node;&#125; 分析可以看上面的注释。程序的逻辑主要分为两个部分：1. 当前同步队列的尾节点为null，调用方法enq()插入;2. 当前队列的尾节点不为null，则采用尾插入（compareAndSetTail（）方法）的方式入队。另外还会有另外一个问题：如果 if (compareAndSetTail(pred, node))为false怎么办？会继续执行到enq()方法，同时很明显compareAndSetTail是一个CAS操作，通常来说如果CAS操作失败会继续自旋（死循环）进行重试。因此，经过我们这样的分析，enq()方法可能承担两个任务：1. 处理当前同步队列尾节点为null时进行入队操作；2. 如果CAS尾插入节点失败后负责自旋进行尝试。那么是不是真的就像我们分析的一样了？只有源码会告诉我们答案:),enq()源码如下： 1234567891011121314151617private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize //1. 构造头结点 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // 2. 尾插入，CAS操作失败自旋尝试 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 在上面的分析中我们可以看出在第1步中会先创建头结点，说明同步队列是带头结点的链式存储结构。带头结点与不带头结点相比，会在入队和出队的操作中获得更大的便捷性，因此同步队列选择了带头结点的链式存储结构。那么带头节点的队列初始化时机是什么？自然而然是在tail为null时，即当前线程是第一次插入同步队列。compareAndSetTail(t, node)方法会利用CAS操作设置尾节点，如果CAS操作失败会在for (;;)for死循环中不断尝试，直至成功return返回为止。因此，对enq()方法可以做这样的总结： 在当前线程是第一个加入同步队列时，调用compareAndSetHead(new Node())方法，完成链式队列的头结点的初始化； 自旋不断尝试CAS尾插入节点直至成功为止。 现在我们已经很清楚获取独占式锁失败的线程包装成Node然后插入同步队列的过程了？那么紧接着会有下一个问题？在同步队列中的节点（线程）会做什么事情了来保证自己能够有机会获得独占式锁了？带着这样的问题我们就来看看acquireQueued()方法，从方法名就可以很清楚，这个方法的作用就是排队获取锁的过程，源码如下： 123456789101112131415161718192021222324252627final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 1. 获得当前节点的先驱节点 final Node p = node.predecessor(); // 2. 当前节点能否获取独占式锁 // 2.1 如果当前节点的先驱节点是头结点并且成功获取同步状态，即可以获得独占式锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //队列头指针用指向当前节点 setHead(node); //释放前驱节点 p.next = null; // help GC failed = false; return interrupted; &#125; // 2.2 获取锁失败，线程进入等待状态等待获取独占式锁 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 程序逻辑通过注释已经标出，整体来看这是一个这又是一个自旋的过程（for (;;)），代码首先获取当前节点的先驱节点，如果先驱节点是头结点的并且成功获得同步状态的时候（if (p == head &amp;&amp; tryAcquire(arg))），当前节点所指向的线程能够获取锁。反之，获取锁失败进入等待状态。整体示意图为下图： 获取锁成功，出队操作 获取锁的节点出队的逻辑是：​​ //队列头结点引用指向当前节点​ setHead(node);​ //释放前驱节点​ p.next = null; // help GC​ failed = false;​ return interrupted; setHead()方法为： 12345private void setHead(Node node) &#123; head = node; node.thread = null; node.prev = null;&#125; 将当前节点通过setHead()方法设置为队列的头结点，然后将之前的头结点的next域设置为null并且pre域也为null，即与队列断开，无任何引用方便GC时能够将内存进行回收。示意图如下： 那么当获取锁失败的时候会调用shouldParkAfterFailedAcquire()方法和parkAndCheckInterrupt()方法，看看他们做了什么事情。shouldParkAfterFailedAcquire()方法源码为： 123456789101112131415161718192021222324252627private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) &#123; /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don&apos;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; shouldParkAfterFailedAcquire()方法主要逻辑是使用compareAndSetWaitStatus(pred, ws, Node.SIGNAL)使用CAS将节点状态由INITIAL设置成SIGNAL，表示当前线程阻塞。当compareAndSetWaitStatus设置失败则说明shouldParkAfterFailedAcquire方法返回false，然后会在acquireQueued()方法中for (;;)死循环中会继续重试，直至compareAndSetWaitStatus设置节点状态位为SIGNAL时shouldParkAfterFailedAcquire返回true时才会执行方法parkAndCheckInterrupt()方法，该方法的源码为： 12345private final boolean parkAndCheckInterrupt() &#123; //使得该线程阻塞 LockSupport.park(this); return Thread.interrupted();&#125; 该方法的关键是会调用LookSupport.park()方法（关于LookSupport会在以后的文章进行讨论），该方法是用来阻塞当前线程的。因此到这里就应该清楚了，acquireQueued()在自旋过程中主要完成了两件事情： 如果当前节点的前驱节点是头节点，并且能够获得同步状态的话，当前线程能够获得锁该方法执行结束退出； 获取锁失败的话，先将节点状态设置成SIGNAL，然后调用LookSupport.park方法使得当前线程阻塞。 经过上面的分析，独占式锁的获取过程也就是acquire()方法的执行流程如下图所示： 3.2 独占锁的释放（release()方法）独占锁的释放就相对来说比较容易理解了，废话不多说先来看下源码： 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 这段代码逻辑就比较容易理解了，如果同步状态释放成功（tryRelease返回true）则会执行if块中的代码，当head指向的头结点不为null，并且该节点的状态值不为0的话才会执行unparkSuccessor()方法。unparkSuccessor方法源码： 1234567891011121314151617181920212223242526272829private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ //头节点的后继节点 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) //后继节点不为null时唤醒该线程 LockSupport.unpark(s.thread);&#125; 源码的关键信息请看注释，首先获取头节点的后继节点，当后继节点的时候会调用LookSupport.unpark()方法，该方法会唤醒该节点的后继节点所包装的线程。因此，每一次锁释放后就会唤醒队列中该节点的后继节点所引用的线程，从而进一步可以佐证获得锁的过程是一个FIFO（先进先出）的过程。 到现在我们终于啃下了一块硬骨头了，通过学习源码的方式非常深刻的学习到了独占式锁的获取和释放的过程以及同步队列。可以做一下总结： 线程获取锁失败，线程被封装成Node进行入队操作，核心方法在于addWaiter()和enq()，同时enq()完成对同步队列的头结点初始化工作以及CAS操作失败的重试; 线程获取锁是一个自旋的过程，当且仅当 当前节点的前驱节点是头结点并且成功获得同步状态时，节点出队即该节点引用的线程获得锁，否则，当不满足条件时就会调用LookSupport.park()方法使得线程阻塞； 释放锁的时候会唤醒后继节点； 总体来说：在获取同步状态时，AQS维护一个同步队列，获取同步状态失败的线程会加入到队列中进行自旋；移除队列（或停止自旋）的条件是前驱节点是头结点并且成功获得了同步状态。在释放同步状态时，同步器会调用unparkSuccessor()方法唤醒后继节点。 独占锁特性学习 3.3 可中断式获取锁（acquireInterruptibly方法）我们知道lock相较于synchronized有一些更方便的特性，比如能响应中断以及超时等待等特性，现在我们依旧采用通过学习源码的方式来看看能够响应中断是怎么实现的。可响应中断式锁可调用方法lock.lockInterruptibly();而该方法其底层会调用AQS的acquireInterruptibly方法，源码为： 12345678public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) //线程获取锁失败 doAcquireInterruptibly(arg);&#125; 在获取同步状态失败后就会调用doAcquireInterruptibly方法： 12345678910111213141516171819202122232425private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; //将节点插入到同步队列中 final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); //获取锁出队 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) //线程中断抛异常 throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 关键信息请看注释，现在看这段代码就很轻松了吧:),与acquire方法逻辑几乎一致，唯一的区别是当parkAndCheckInterrupt返回true时即线程阻塞时该线程被中断，代码抛出被中断异常。 3.4 超时等待式获取锁（tryAcquireNanos()方法）通过调用lock.tryLock(timeout,TimeUnit)方式达到超时等待获取锁的效果，该方法会在三种情况下才会返回： 在超时时间内，当前线程成功获取了锁； 当前线程在超时时间内被中断； 超时时间结束，仍未获得锁返回false。 我们仍然通过采取阅读源码的方式来学习底层具体是怎么实现的，该方法会调用AQS的方法tryAcquireNanos(),源码为： 12345678public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || //实现超时等待的效果 doAcquireNanos(arg, nanosTimeout);&#125; 很显然这段源码最终是靠doAcquireNanos方法实现超时等待的效果，该方法源码如下： 123456789101112131415161718192021222324252627282930313233343536private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (nanosTimeout &lt;= 0L) return false; //1. 根据超时时间和当前时间计算出截止时间 final long deadline = System.nanoTime() + nanosTimeout; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); //2. 当前线程获得锁出队列 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return true; &#125; // 3.1 重新计算超时时间 nanosTimeout = deadline - System.nanoTime(); // 3.2 已经超时返回false if (nanosTimeout &lt;= 0L) return false; // 3.3 线程阻塞等待 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); // 3.4 线程被中断抛出被中断异常 if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 程序逻辑如图所示： 程序逻辑同独占锁可响应中断式获取基本一致，唯一的不同在于获取锁失败后，对超时时间的处理上，在第1步会先计算出按照现在时间和超时时间计算出理论上的截止时间，比如当前时间是8h10min,超时时间是10min，那么根据deadline = System.nanoTime() + nanosTimeout计算出刚好达到超时时间时的系统时间就是8h 10min+10min = 8h 20min。然后根据deadline - System.nanoTime()就可以判断是否已经超时了，比如，当前系统时间是8h 30min很明显已经超过了理论上的系统时间8h 20min，deadline - System.nanoTime()计算出来就是一个负数，自然而然会在3.2步中的If判断之间返回false。如果还没有超时即3.2步中的if判断为true时就会继续执行3.3步通过LockSupport.parkNanos使得当前线程阻塞，同时在3.4步增加了对中断的检测，若检测出被中断直接抛出被中断异常。 4. 共享锁4.1 共享锁的获取（acquireShared()方法）在聊完AQS对独占锁的实现后，我们继续一鼓作气的来看看共享锁是怎样实现的？共享锁的获取方法为acquireShared，源码为： 1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 这段源码的逻辑很容易理解，在该方法中会首先调用tryAcquireShared方法，tryAcquireShared返回值是一个int类型，当返回值为大于等于0的时候方法结束说明获得成功获取锁，否则，表明获取同步状态失败即所引用的线程获取锁失败，会执行doAcquireShared方法，该方法的源码为： 12345678910111213141516171819202122232425262728private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 当该节点的前驱节点是头结点且成功获取同步状态 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 现在来看这段代码会不会很容易了？逻辑几乎和独占式锁的获取一模一样，这里的自旋过程中能够退出的条件是当前节点的前驱节点是头结点并且tryAcquireShared(arg)返回值大于等于0即能成功获得同步状态。 4.2 共享锁的释放（releaseShared()方法）共享锁的释放在AQS中会调用方法releaseShared： 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 当成功释放同步状态之后即tryReleaseShared会继续执行doReleaseShared方法： 1234567891011121314151617181920212223242526272829private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; 这段方法跟独占式锁释放过程有点点不同，在共享式锁的释放过程中，对于能够支持多个线程同时访问的并发组件，必须保证多个线程能够安全的释放同步状态，这里采用的CAS保证，当CAS操作失败continue，在下一次循环中进行重试。 4.3 可中断（acquireSharedInterruptibly()方法），超时等待（tryAcquireSharedNanos()方法）关于可中断锁以及超时等待的特性其实现和独占式锁可中断获取锁以及超时等待的实现几乎一致，具体的就不再说了，如果理解了上面的内容对这部分的理解也是水到渠成的。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringBoot%2Ffile%20upload%2F</url>
    <content type="text"><![CDATA[12345678@PostMapping(&quot;image&quot;) public ResponseEntity&lt;String&gt; uploadImage(@RequestParam(&quot;file&quot;) MultipartFile file) &#123; String url = uploadService.upload(file); if (StringUtils.isBlank(url)) &#123; return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null); &#125; return ResponseEntity.ok(url); &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.steambuy.service;import lombok.extern.slf4j.Slf4j;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Service;import org.springframework.web.multipart.MultipartFile;import javax.imageio.ImageIO;import java.awt.image.BufferedImage;import java.io.File;import java.io.IOException;import java.util.Arrays;import java.util.List;@Slf4j@Servicepublic class UploadService &#123; private static final List&lt;String&gt; allowTypes = Arrays.asList(&quot;image/png&quot;, &quot;image/jpeg&quot;); public String upload(MultipartFile file)&#123; String type = file.getContentType(); if(!allowTypes.contains(type))&#123; log.info(&quot;上传类型&#123;&#125;不匹配&quot;,type); return null; &#125; BufferedImage image = null; try &#123; image = ImageIO.read(file.getInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if(image==null)&#123; log.info(&quot;上传内容不符合要求&quot;); return null; &#125; File dir=new File(&quot;E:\\box\\image&quot;); if(!dir.exists())&#123; dir.mkdirs(); &#125; File target = new File(dir, file.getOriginalFilename()); try &#123; file.transferTo(target); &#125; catch (IOException e) &#123; log.info(&quot;文件落地失败&quot;); &#125; return target.getAbsolutePath(); &#125;&#125; 123456spring: application: name: upload-service servlet: multipart: max-file-size: 5MB # 限制文件上传的大小]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式设计模式的六大原则 《设计模式的六大原则》 开闭原则：对扩展开放,对修改关闭，多使用抽象类和接口。 里氏替换原则：基类可以被子类替换，使用抽象类继承,不使用具体类继承。 依赖倒转原则：要依赖于抽象,不要依赖于具体，针对接口编程,不针对实现编程。 接口隔离原则：使用多个隔离的接口,比使用单个接口好，建立最小的接口。 迪米特法则：一个软件实体应当尽可能少地与其他实体发生相互作用，通过中间类建立联系。 合成复用原则：尽量使用合成/聚合,而不是使用继承。 23种常见设计模式 《设计模式》 《23种设计模式全解析》 《设计模式类图与示例》 IOC 《理解 IOC》 《IOC 的理解与解释》 正向控制：传统通过new的方式。反向控制，通过容器注入对象。 作用：用于模块解耦。 DI：Dependency Injection，即依赖注入，只关心资源使用，不关心资源来源。 AOP 《轻松理解AOP(面向切面编程)》 《Spring AOP详解》 《Spring AOP的实现原理》 Spring AOP使用的动态代理，主要有两种方式：JDK动态代理和CGLIB动态代理。 《Spring AOP 实现原理与 CGLIB 应用》 Spring AOP 框架对 AOP 代理类的处理原则是：如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类；如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E6%80%BB%E4%BA%86%E4%B8%AA%E7%BB%93%2F</url>
    <content type="text"><![CDATA[继承：三角箭头 空心 实线 实现：三角箭头 空心 虚线 组合：棱形 实心 实线 聚合：棱形 空心 实线 关联：箭头 实线 依赖：箭头 虚线 适配器模式： 深入理解适配器模式——加个“适配器”以便于复用 适配器模式原理及实例介绍-IBM 桥接模式： 设计模式笔记16：桥接模式(Bridge Pattern) 组合模式： 大话设计模式—组合模式 装饰模式： java模式—装饰者模式 Java设计模式-装饰者模式 代理模式： 代理模式原理及实例讲解 （IBM出品，很不错） 轻松学，Java 中的代理模式及动态代理 Java代理模式及其应用 https://github.com/CyC2018/CS-Notes/blob/master/docs/notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.md]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[静态代理123456789101112131415161718192021222324252627282930313233public interface MyInterface &#123; void print();&#125;public class MyImplement implements MyInterface &#123; @Override public void print() &#123; System.out.println("hello world"); &#125;&#125;public class MyProxy implements MyInterface&#123; private MyImplement myImplement; public MyProxy() &#123; this.myImplement = new MyImplement(); &#125; @Override public void print() &#123; System.out.println("before"); myImplement.print(); System.out.println("after"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyInterface myInterface=new MyProxy(); myInterface.print(); &#125;&#125; 这样的代理方式调用者不知道被代理对象的存在。 动态代理从静态代理中可以看出，静态代理只能代理一个具体的类，如果要代理一个接口的多个实现的话需要定义不同的代理类，所以需要使用动态代理。 JDK实现JDK动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。 Proxy 类是用于创建代理对象，而 InvocationHandler 接口来处理执行逻辑。 Proxy 的newProxyInstance 方法动态创建代理类。第一个参数为类加载器，第二个参数为代理类需要实现的接口列表，最后一个则是处理器。 1234567891011121314151617181920212223242526272829303132333435public class MyProxy implements InvocationHandler &#123; private Object target; public MyProxy(Class clazz) &#123; try &#123; this.target=clazz.newInstance(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("before"); Object result=method.invoke(target,args); System.out.println("after"); return result; &#125; public Object getProxy()&#123; return Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), target.getClass().getInterfaces(),this); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyProxy myProxy = new MyProxy(MyImplement.class); MyInterface myInterface= (MyInterface) myProxy.getProxy(); myInterface.print(); &#125;&#125; CGLIB实现CGLIB是对一个小而快的字节码处理框架 ASM 的封装。把代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。 12345678910111213141516171819202122232425262728293031323334353637public class MyProxy implements MethodInterceptor &#123; private Object target; public MyProxy(Class clazz) &#123; try &#123; this.target = clazz.newInstance(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; public Object getProxy()&#123; Enhancer enhancer=new Enhancer(); enhancer.setSuperclass(target.getClass()); enhancer.setCallback(this); return enhancer.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("before"); Object result=method.invoke(target,objects); System.out.println("after"); return result; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyInterface myInterface= (MyInterface) new MyProxy(MyImplement.class).getProxy(); myInterface.print(); &#125;&#125; 如何强制使用CGLIB实现AOP？ 添加CGLIB库，SPRING_HOME/cglib/*.jar 在spring配置文件中加入&lt;aop:aspectj-autoproxy proxy-target-class=“true”/&gt; JDK动态代理和CGLIB字节码生成的区别？ JDK动态代理只能对实现了接口的类生成代理，而不能针对类 CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，这就要求被该类或方法不要声明成final]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FGOF%2F</url>
    <content type="text"><![CDATA[创建型模式，共五种： 工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 简单工厂 创建对象放在factory里 beanfactory工厂方法 创建对象放在factory的子类里 collection集合类的iterator factorybean product factory都要抽象抽象工厂 适用于产品族 数据库连接 Connection sqlsessionfactory 深拷贝与浅拷贝问题中，会发生深拷贝的有java中的8中基本类型以及他们的封装类型，另外还有String类型。其余的都是浅拷贝 结构型模式，共七种： 适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种： 策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式 观察者模式 subject :addObserver delObserver notify observer: update ApplicationListener 访问者模式 Element有accept(Visitor visitor) Visitory有visit(XXX xxx) 两次动态分派 命令模式 将请求封装成一个对账 统一网关的Ep100Request Runnable 责任链模式 有 nexthandler handleRequest setnexthandler 策略模式 有个封装类 context 抽线策略类 strategy 具体 ConcreteStrategy 与模板模式的区别是 把具体方法单独抽到另一个类里SimpleInstantiationStrategy Spring中在实例化对象的时候用到Strategy模式 适配器模式 适配接口用 实现类构造函数传别的对象 方法实现调对象的方法 AdviceAdapter handlerAdapteraop 由于Advisor链需要的是MethodInterceptor（拦截器）对象，所以每一个Advisor中的Advice都要适配成对应的MethodInterceptor对象。 模板方法 redistemplate jdbctemplate 桥接 抽象和具体实现分离 而不是使用继承 jdbc的driver 代理模式 aop jdkproxy cglibproxy 单例模式 getSingleton spring依赖注入时，使用了 多重判断加锁 的单例模式。享元模式 复用对象 FlyweightFactory通过 hashmap 维护 没有的创建 放hashmap 有的从hashmap取 Integer的创建 建造者模式 cachebuilder beandefinitionbuilder 原型模式 要克隆 实现clonable接口 重写clone方法 有的字段还需要深克隆 适用于复杂对象的创建 外观模式 封装方法 tomcatRequestFacade 装饰器模式 继承扩展功能 动态增加功能 一个抽象类 一个扩展的装饰类 分类实现 与代理模式的区别 一个是动态添加 一个控制访问 与适配器区别 io mybatis cache 组合模式 多个对象组合成一个对象 arraylist putall]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据库索引]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[MySQL的基本存储结构是页(记录都存在页里边)： 各个数据页可以组成一个双向链表 每个数据页中的记录又可以组成一个单向链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 使用索引之后索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)： 很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 “目录” 就可以很快地定位到对应的页上了！（二分查找，时间复杂度近似为O(logn)） 其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。 以下内容整理自：《Java工程师修炼之道》 最左前缀原则MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是(name,city) 123select * from user where name=xx and city=xx ; ／／可以命中索引select * from user where name=xx ; // 可以命中索引select * from user where city=xx; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的. 由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDERBY子句也遵循此规则。​ 如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），遇到范围查询(&gt;、&lt;、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。 因此，列的排列顺序决定了可命中索引的列数。 hash索引 哈希索引也没办法利用索引完成排序 不支持最左匹配原则 在有大量重复键值情况下，哈希索引的效率也是极低的—-&gt;哈希碰撞问题。 不支持范围查询 ，考虑在WHERE和ORDER BY命令上涉及的列建立索引 OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描 避免%xxx式查询 少用JOIN 尽量避免在WHERE子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描 总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表。在数据库做主从分离的情况下，经常选择MyISAM作为主库的存储引擎 为什么要建索引，避免全表扫描 跟字典差不多 什么能做索引 主键 唯一键 结构 二叉查找树（用的二分查找）增删不行 b b+ hash 索引的数据结构B+Tree B+Tree是一种专门针对磁盘存储而优化的N叉排序树，以树的节点为单位存储在磁盘中，从根开始查找所需数据所在的节点编号和磁盘位置，将其加载到内存中然后继续查找，直到找到所需的数据。 B+Tree是大多数MySQL存储引擎默认的索引类型，是基于B-Tree和叶子节点顺序访问指针实现的，它所有叶子节点位于同一层，并且通过顺序访问指针来提高区间查询的性能。 进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找对应的数据 插入删除操作记录会破坏平衡树的平衡性，因此在插入删除操作之后，需要对树进行一个分裂、合并、旋转等操作来维护平衡性 B+Tree插入删除查找演示地址（需翻墙） 而许多NoSQL使用LSM树作为主要的数据结构，在LSM树上进行一次数据更新不需要磁盘访问，在内存即可完成，速度远超B+Tree。 数据库在做查询时IO消耗比较大，B+Tree每到一层就发生一次IO操作，为了减少IO操作，提高查询效率，要让这棵树尽可能的低 利于计算机的预读特性，内存和磁盘以片为单位读取数据，将连续的数据放到同一片里，读取会更快 红黑树也可以用来实现索引，但是树高，所以IO次数多 Hash 基于Hash表，查找速度很快，一般情况下查找的时间复杂度O(1)，仅需要一次查找就能定位数据。缺点： 失去有序性，无法用于排序和分组 只支持精确查找，无法用于部分查找和范围查找 在InnoDB引擎中，数据库自优化生成Hash索引 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，数据库根据查询情况，对使用频繁的索引自优化生成Hash索引 Fulltext 全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等，查找条件使用 MATCH AGAINST，而不是普通的 WHERE。InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引 R-Tree 空间数据索引,可以用于地理数据存储。空间数据索引会从所有维度来查找数据，可以有效地使用任意维度来进行组合查询，必须使用 GIS 相关的函数来维护数据 聚集索引 · 非聚集索引聚集索引 索引的键值逻辑顺序决定数据行的物理存储顺序 InnoDB主索引的叶子节点 data 域记录着完整的数据记录，因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找 所以不要使用过长的字段作为主键，过长的主索引会令辅助索引变得过大 推荐使用自增字段作为主键，否则非自增的主键会造成在插入新记录时，索引会为了维持B+Tree的特性而频繁的分裂调整 非聚集索引 索引的键值逻辑顺序，但数据行的物理存储地址不一定顺序 MyISAM叶子节点存放的是数据的物理地址而不是数据本身。在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用 Innodb Myisam 存储文件 .frm表定义文件 ibd数据文件 .frm表定义文件 .myd数据文件 .myi索引文件 锁 表锁、行锁 表锁 事务 ACID 不支持 CURD 读、写 读多 count 扫表 专门存储的地方 索引结构 B+Tree B+Tree 单一索引 · 复合索引单一索引 新建索引的语句只实施在一列上，索引要建在数据区分度高的列上，像sex这种区分度低的就不需要单独建索引 复合索引 可以指定多个列作为索引列，组成复合索引。索引列数据区分度高的放在前面 按照where条件建索引，索引包含所有需要查询的字段的值 有复合索引unionindex(column1,column2)，就没有必要再建index(column1) 复合索引的索引列不要太多 优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 减少需要扫描的数据行数，加快数据的查询速度 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量 缺点 随着数据量的增加，建立和维护索引的时间和空间也会增加 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。会增加数据库写操作的成本，降低表更新的速度 会增加查询优化器的选择时间 如何创建索引在哪儿建索引 经常需要搜索的列上，可以加快搜索的速度 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 多表join关联列 select update delete where 的列 orderby groupby distinct中的字段 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 选择索引列的顺序 区分度最高的放最左侧 字段长度小的列放左 最频繁使用的列放左 不该在哪儿建索引 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那些只有数据值区分区很小的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 数据类型为text, image和bit数据类型的列不应该增加索引 当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 索引失效索引列是表达式的一部分或函数的参数不能使用索引 12select * from tbl_user where id+1=2;select * from tbl_user having max(id); 复合索引最左前缀不能使用索引 12create index index_id_username on tbl_user(id,username);select * from tbl_user where username='root'; 负向查询不能使用索引 1select name from user where id not in (1,3,4); 前导模糊查询不能使用索引 1select name from user where name like '%zhangsan' 索引原理局部性原理与磁盘预读：由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 explain通过MySQL工具explain+查询语句 select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[数据库事务]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[原子性（Atomicity） 事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency） 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。可以通过数据库备份和恢复来实现，在系统发生崩溃时，使用备份的数据库进行数据恢复。 不事务隔离带来的问题更新丢失：两事务同时更新，一个失败回滚覆盖另一个事务的更新。或事务1执行更细操作，在事务1结束前事务2也更新，则事务1的更细结果被事务2的覆盖了。 脏读：事务T2读取到事务T1修改了但是还未提交的数据，之后事务T1又回滚其更新操作，导致事务T2读到的是脏数据。 不可重复读：事务T1读取某个数据后，事务T2对其做了修改，当事务T1再次读该数据时得到与前一次不同的值。 虚读（幻读）：事务T1读取在读取某范围数据时，事务T2又插入一条数据，当事务T1再次数据这个范围数据时发现不一样了，出现了一些“幻影行”。 脏读和不可重复读的区别：脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 不可重复读和幻读的异同：都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 未提交读（READ UNCOMMITTED） 事务中的修改，即使没有提交，对其它事务也是可见的。使用查询语句不会加锁，可能会读到未提交的行（Dirty Read） 造成：脏读；不可重复读；幻影读 所需的锁：排他写锁 提交读（READ COMMITTED） 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）大多数数据库默认级别是RC，比如SQL Server，Oracle 造成：不可重复读；幻影读 所需的锁：排他写锁、瞬间共享读锁 可重复读（REPEATABLE READ） 保证在同一个事务中多次读取同样数据的结果是一样的。多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）MySQL InnoDB 就是这个级别 事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。解决幻读的方法是增加范围锁（range lock）或者表锁。 造成：幻影读 所需的锁：排他写锁、共享读锁 可串行化（SERIALIZABLE） 强制事务串行执行。InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题； 所须的锁：范围锁或表锁 MySQL 中默认的事务隔离级别就是 REPEATABLE READ，它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。MVCC 会产生幻读问题（更新时异常）在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（Next-Key Locking）防止幻影读。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>ACID</tag>
        <tag>Isolation</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[为什么要分库分表数据量越大，请求量越大，单个数据库扛不住请求，所以数据库做读写分离分担压力。但数据库读写分离总是会受到数据库大小的限制，一旦数据库过于庞大，尤其是写入过于频繁，很难由一台主机支撑，面临到扩展瓶颈。需要对数据进行切分，将我们存放在同一个数据库中的数据分散存放到多个数据库上面 。另外单表数据量太大，sql执行的性能低。分库与分表的目的在于，减小数据库的单库单表负担，提高查询性能，缩短查询时间 分表面对上千万甚至上亿的数据，查询一次所花费的时间很长长，如果有联合查询的情况下，甚至可能会成为很大的瓶颈。此外，MySQL 存在表锁和行锁，因此更新表数据可能会引起表锁或者行锁，这样也会导致其他操作等待，甚至死锁问题。 通过分表，可以减少数据库的单表负担，将压力分散到不同的表上，同时因为不同的表上的数据量少了，起到提高查询性能，缩短查询时间的作用，此外，可以很大的缓解表锁的问题。 分表策略可以归纳为垂直拆分和水平拆分。 垂直拆分，把表的字段进行拆分，即一张字段比较多的表拆分为多张表，这样使得行数据变小。一方面，可以减少客户端程序和数据库之间的网络传输的字节数，因为生产环境共享同一个网络带宽，随着并发查询的增多，有可能造成带宽瓶颈从而造成阻塞。另一方面，一个数据块能存放更多的数据，在查询时就会减少 I/O 次数。举个例子，假设用户表中有一个字段是家庭地址，这个字段是可选字段，在数据库操作的时候除了个人信息外，并不需要经常读取或是更改这个字段的值。在这种情况下，更建议把它拆分到另外一个表，从而提高性能。 垂直拆分建议： 将不常用的字段单独拆分到另外一张扩展表，例如前面讲解到的用户家庭地址，这个字段是可选字段，在数据库操作的时候除了个人信息外，并不需要经常读取或是更改这个字段的值。 将大文本的字段单独拆分到另外一张扩展表，例如 BLOB 和 TEXT 字符串类型的字段，以及 TINYBLOB、 MEDIUMBLOB、 LONGBLOB、 TINYTEXT、 MEDIUMTEXT、 LONGTEXT字符串类型。这样可以减少客户端程序和数据库之间的网络传输的字节数。 将不经常修改的字段放在同一张表中，将经常改变的字段放在另一张表中。举个例子，假设用户表的设计中，还存在“最后登录时间”字段，每次用户登录时会被更新。这张用户表会存在频繁的更新操作，此外，每次更新时会导致该表的查询缓存被清空。所以，可以把这个字段放到另一个表中，这样查询缓存会增加很多性能。 对于需要经常关联查询的字段，建议放在同一张表中。不然在联合查询的情况下，会带来数据库额外压力。 水平拆分，把表的行进行拆分。因为表的行数超过几百万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。水平拆分，有许多策略，例如，取模分表，时间维度分表，以及自定义 Hash 分表，例如用户 ID 维度分表等。在不同策略分表情况下，根据各自的策略写入与读取。 实际上，垂直拆分后的表依然存在单表数据量过大的问题，需要进行水平拆分。因此，实际情况中，水平拆分往往会和垂直拆分结合使用。假设，随着用户数的不断增加，用户表单表存在上千万的数据，这时可以把一张用户表的数据拆成多张用户表来存放。 常见的水平分表策略归纳起来，可以总结为随机分表和连续分表两种情况。例如，取模分表就属于随机分表，而时间维度分表则属于连续分表。 连续分表可以快速定位到表进行高效查询，大多数情况下，可以有效避免跨表查询。如果想扩展，只需要添加额外的分表就可以了，无需对其他分表的数据进行数据迁移。但是，连续分表有可能存在数据热点的问题，有些表可能会被频繁地查询从而造成较大压力，热数据的表就成为了整个库的瓶颈，而有些表可能存的是历史数据，很少需要被查询到。 随机分表是遵循规则策略进行写入与读取，而不是真正意义上的随机。通常，采用取模分表或者自定义 Hash 分表的方式进行水平拆分。随机分表的数据相对比较均匀，不容易出现热点和并发访问的瓶颈。但是，分表扩展需要迁移旧的数据。此外，随机分表比较容易面临跨表查询的复杂问题。 对于日志场景，可以考虑根据时间维度分表，例如年份维度分表或者月份维度分表，在日志记录表的名字中包含年份和月份的信息，例如 log_2017_01，这样可以在已经没有新增操作的历史表上做频繁地查询操作，而不会影响时间维度分表上新增操作。 对于海量用户场景，可以考虑取模分表，数据相对比较均匀，不容易出现热点和并发访问的瓶颈。 对于租户场景，可以考虑租户维度分表，不同的租户数据独立，而不应该在每张表中添加租户 ID，这是一个不错的选择。 分库概述库内分表，仅仅是解决了单表数据过大的问题，但并没有把单表的数据分散到不同的物理机上，因此并不能减轻 MySQL 服务器的压力，仍然存在同一个物理机上的资源竞争和瓶颈，包括 CPU、内存、磁盘 IO、网络带宽等。 分库策略也可以归纳为垂直拆分和水平拆分。 垂直拆分，按照业务和功能划分，把数据分别放到不同的数据库中。举个例子，可以划分资讯库、百科库等。 水平拆分，把一张表的数据划分到不同的数据库，两个数据库的表结构一样。实际上，水平分库与水平分表类似，水平拆分有许多策略，例如，取模分库，自定义 Hash 分库等，在不同策略分库情况下，根据各自的策略写入与读取。举个例子，随着业务的增长，资讯库的单表数据过大，此时采取水平拆分策略，根据取模分库。 分库分表中间件sharding-jdbc：属于client层方案，支持分库分表、读写分离、分布式id生成、柔性事务（最大努力送达型事务、TCC事务）。 优点：不用部署，运维成本低，不需要代理层的二次转发请求，性能很高 缺点：如果需要升级，各个系统都重新升级版本再发布，各个系统都需要耦合sharding-jdbc的依赖 mycat：基于cobar改造的，属于proxy层方案，支持的功能非常完善。 优点：对于各个项目是透明的，如果需要升级之类的都是自己中间件那里搞就行了 缺点：需要部署，运维一套中间件，运维成本高 如何对数据库进行拆分垂直拆分可以根据字段的访问频率 水平拆分range 优点：扩容的时候很容易，只需要按时准备好一个库，比如一个月订单写一个数据库，到了一个新的月份的时候写新库 缺点：大部分的请求，都是访问最新的数据。用户几乎只会查看最近的几个订单 实际生产用range，要看场景，用户不是仅仅访问最新的数据，而是均匀的访问现在的数据以及历史的数据 hash 优点：可以平均分配没给库的数据量和请求压力 缺点：扩容起来比较麻烦，会有一个数据迁移的过程 实践对于一个尚未进行分库分表的系统，要切换到分库分表上，有几个方案 停机修改配置写，服务会不可用一段时间 取老库的备份，写入新库，在这期间的增删改，以日志的形式存着，备份的数据写完了接着写日志里的知道结束 线上系统里面，所有写库的地方，除了对老库增删改，都加上对新库的增删改，同时写老库和新库。然后用导数据的工具，读老库数据写新库，写的时候要根据last_modified这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 动态扩容缩容 利用32 * 32来分库分表，即分为32个库，每个库里一个表分为32张表，一共就是1024张表。比如对订单表进行拆分，orderId mod 32 确定库的索引 orderId / 32 mod 32 = 确定表的索引 扩容的时候，申请增加更多的数据库服务器，装好mysql，倍数扩容，4台服务器，扩到8台服务器，16台服务器 将原先数据库服务器的库，迁移到新的数据库服务器上去 修改一下配置，调整迁移的库所在数据库服务器的地址，重新发布系统，上线，原先的路由规则变都不用变，直接可以基于2倍的数据库服务器的资源，继续进行线上系统的提供服务]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQL语句优化]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FSQL%E8%AF%AD%E5%8F%A5%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[只返回必要的列，减少查询字段数，不使用select * 只返回必要的行 limit限制返回的数据， 确定只要一行数据时使用limit 1 缓存重复查询的数据 使用索引减少扫描次数 切分大查询 如果一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询，所以要切分 分解大连接查询 让缓存更高效 对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用 在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好 对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符 冷热数据分离 历史数据归档 不使用uuid md5 hash 字符串作为主键 使用分区表 逻辑上是一张表 物理上存在不同的表上（list,hash,range） 避免使用子查询 会产生大量的临时表 子查询的没有索引 合理使用join，表关联尽量用主键 where 从句不要使用函数转换或计算，会导致无法使用索引 尽量不在数据库做运算，否则无法使用索引导致全表扫描 避免前缀模糊查询 用不了索引导致全表扫描 控制表单数据量 合理分表 单库不超过300-400个表 表字段少而精 字段上限控制在20-50个 效率优先 可以适当冗余 用好数据字段类型 tinyint int bigint 字符转化为数字 数字型更高效 查询更快 占用空间小 避免使用null 字段 难以进行查询优化 null列加索引，需要额外的空间按 含null复合索引无效 字符字段必须建前缀索引 ALTER TABLE messages_messagehistory ADD KEY (messagecontent(8)) 尽量不用外键 有额外开销 高并发容易死锁 大sql拆解成多条简单sql 缓存命中高 减少锁表时间 能用上更多的cpu 保持事务连接短小 与事务无关的操作放到事务外面 避免负向查询 如 not != &lt;&gt; !&lt; !&gt; not exists not in not like 减少count(*) 资源开销大 无需对结果去重时，用union all , union有去重开销 同数据类型的列值比较 数字对数字 字符对字符 字符列与数值类型比较 字符列转成数值，不会使用索引查询 两个表join的字段 数据类型要相同 MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化。EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了。 准备建立两个测试用的表并添加相应的数据 1234567891011121314151617181920CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB DEFAULT CHARSET = utf8CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)) ENGINE = InnoDB DEFAULT CHARSET = utf8 EXPLAIN 输出含义idSELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中内层的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. table表示查询涉及的表或衍生表 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型type 常用的取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 1mysql&gt; explain select * from user_info where id = 2 eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 1mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 1mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5 range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 1mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8 index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据.index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. 1mysql&gt; EXPLAIN SELECT name FROM user_info 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免.下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 1mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20 type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下:ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; systemALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的.而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快.后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key此字段是 MySQL 在当前查询时所真正使用到的索引. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: 1mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = &apos;p1&apos; AND productor = &apos;WHH&apos; 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT &#39;0&#39;, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 12mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = &apos;p1&apos; 1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 ref哪个字段或常数与 key 一起被使用 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. filtered表示此查询条件所过滤的数据的百分比 ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 1mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name 我们的索引是 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort.如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 1mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Explain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL主从复制]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[MySQL主从复制原理 主库将增删改操作写binlog日志，从库连接到主库之后有一个IO线程，将主库的binlog日志拷贝到本地，写入到一个中继日志中，接着从库中有一个SQL线程会从中继日志读取binlog日志，并执行binlog日志中的内容，也就是在本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。这里有一个非常重要的点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。所以MySQL在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 半同步复制（semi-sync复制），指的就是主库写入binlog日志之后，就会立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。 并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 MySQL主从同步延时 一般在读远远多于写的场景下来用这个MySQL主从同步，而且读的时候一般对数据时效性要求没那么高。主从同步延时问题，会导致一些线上的bug难以发现。可以通过show status，Seconds_Behind_Master，看到从库复制主库的数据落后了多少ms。 虽然可以用MySQL的并行复制，但是那是库级别的并行，时候作用不是很大。所以对于那种写了之后立马就要保证可以查到的场景，采用强制读主库的方式，这样就可以保证可以读到数据。也可以重写业务代码比如第二句SQL不依赖第一句SQL，就直接更新不查询，如果依赖就先读判断结果是否为空，如果为空就报错下次重试。 实现MySQL的读写分离基于主从复制架构，简单来说就是搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 主从复制主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。 I/O 线程 ：负责从主服务器上读取二进制日志，并写入从服务器的重放日志（Replay log）中。 SQL 线程 ：负责读取重放日志并重放其中的 SQL 语句。 1）Master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； （2）Slave将Master的二进制日志事件(binary log events)拷贝到它的中继日志(relay log)； PS：从图中可以看出，Slave服务器中有一个I/O线程(I/O Thread)在不停地监听Master的二进制日志(Binary Log)是否有更新：如果没有它会睡眠等待Master产生新的日志事件；如果有新的日志事件(Log Events)，则会将其拷贝至Slave服务器中的中继日志(Relay Log)。 （3）Slave重做中继日志(Relay Log)中的事件，将Master上的改变反映到它自己的数据库中。 读写分离主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 replication master写 slave读 可能会同步失败 pxc数据读写是双向的 同步复制 事务在所有集群节点要么同时提交要么不提交]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMySQL%2F</url>
    <content type="text"><![CDATA[优化 《mysql数据库死锁的产生原因及解决办法》 索引聚集索引, 非聚集索引 《MyISAM和InnoDB的索引实现》 复合索引 《复合索引的优点和注意事项》 干货：mysql索引的数据结构 MySQL优化系列（三）–索引的使用、原理和设计优化 数据库两大神器【索引和锁】 详细内容可以参考： 可能是最漂亮的Spring事务管理详解 数据存储 说说 SQL 优化之道 MySQL 遇到的死锁问题 https://www.cnblogs.com/zejin2008/p/5262751.html 存储引擎的 InnoDB 与 MyISAM 数据库索引的原理 https://blog.csdn.net/suifeng3051/article/details/52669644 limit 20000 加载很慢怎么解决 http://ourmysql.com/archives/1451 limit 200000,10 相当于要200010行 舍弃 200000行（第一个参数为起始行，从0开始数；第二个参数为返回的总行数） –返回第3-5行 第一个参数为起始行，从0开始数；第二个参数为返回的总行数SELECT * FROM mytable LIMIT 2, 3;]]></content>
  </entry>
  <entry>
    <title><![CDATA[推荐]]></title>
    <url>%2F2019%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[数据收集： etl 提取 转换 加载 数据存储 ： sql nosql 资源管理 批处理 交互式分析 流处理 数据挖掘 ： 数据仓库 olap 商务智能 数据可视化 Grafana mahout flume kafka 推荐角度： 社交推荐 朋友推荐 流行度推荐 排行榜 内容推荐 搜的内容 协同过滤推荐 买这个的其他人还买什么 稀疏矩阵的问题 冷启动的问题 相似度算法： Jaccard coefficient 交集/并集 余弦相似度 cos(a,b) Perarson Correlation 协方差/标准差 0.8-1.0 0.6-0.8 KNN算法 分类算法 找出和目标用户兴趣相似的目标集合 ​ 相似度 得到K个用户 找到集合中用户喜欢的 ​ s(u,k) 使用单一行为隐反馈数据 UserCF算法 冷启动问题 用户冷启动 商品冷启动 系统冷启动 方法： 非个性化推荐 最流行 排名最高 进去强制选几个分类 注册信息 用户画像是高维向量 消费层级 年龄 性别 爱好 购买的商品等 明确问题 数据预处理 特征工程 模型算法 产出]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fstorm%2F</url>
    <content type="text"><![CDATA[架构 并行度：Worker-&gt;Executor-&gt;Task，没错，是Task 流分组：Task与Task之间的数据流向关系 Shuffle Grouping：随机发射，负载均衡Fields Grouping：根据某一个，或者某些个，fields，进行分组，那一个或者多个fields如果值完全相同的话，那么这些tuple，就会发送给下游bolt的其中固定的一个task 你发射的每条数据是一个tuple，每个tuple中有多个field作为字段 比如tuple，3个字段，name，age，salary {“name”: “tom”, “age”: 25, “salary”: 10000} -&gt; tuple -&gt; 3个field，name，age，salary All GroupingGlobal GroupingNone GroupingDirect GroupingLocal or Shuffle Grouping]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fambari%2F</url>
    <content type="text"><![CDATA[https://blog.51cto.com/tryingstuff/2066561 yum install -y wgetwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum makecache 配置免密登陆各节点scp /root/.ssh/id_rsa.pub node2:/root/.ssh/authorized_keysscp /root/.ssh/id_rsa.pub node3:/root/.ssh/authorized_keys 节点1 yum install httpd -y安装包放到 /var/www/htmltar xf 解压所有 repo 文件放到/etc/yum.repo.d/ systemctl start httpdyum clean all yum makecache fast yum update openssl/etc/python/cert-verification.cfg disable/etc/ambari-agent/conf/ambari-agent.ini force_https_protocol=PROTOCOL_TLSv1_2 sudo rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[##令牌桶算法 按照恒定的速率向桶中放入令牌，每当请求经过时则消耗一个或多个令牌。当桶中的令牌为 0 时，请求则会被阻塞 固定容量的令牌桶，按照一定速率添加令牌，处理请求前需要拿到令牌，拿不到令牌则丢弃请求，或进入丢队列，可以通过控制添加令牌的速率，来控制整体速度。Guava 中的 RateLimiter 是令牌桶的实现。 单体应用可以使用Guava的RateLimiter实现令牌桶12RateLimiter limiter=RateLimiter.create(2);// 每秒向桶中放入两个令牌limiter.acquire(1);// 请求一次消耗一个令牌 分布式应用可以借助redis实现限流 12345678910111213141516171819202122232425262728293031323334353637383940package com.masonnpe.util.limiting.util;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;import java.util.Collections;public class RedisRateLimiter &#123; private static JedisPool jedisPool; static &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(100); jedisPool=new JedisPool(config,"192.168.170.252",6379); &#125; public static boolean isNeedLimit()&#123; Jedis jedis=jedisPool.getResource(); String limitScript="local key = KEYS[1]\n" + "local limit = tonumber(ARGV[1])\n" + "local curentLimit = tonumber(redis.call('get', key) or \"0\")\n" + "if curentLimit + 1 &gt; limit then\n" + "return 0;\n" + "else\n" + "redis.call(\"INCRBY\", key, 1)\n" + "redis.call(\"EXPIRE\", key, 2) \n" + "return curentLimit + 1\n" + "end"; String key = "redis-limit"+String.valueOf(System.currentTimeMillis() / 1000); Object result=jedis.eval(limitScript, Collections.singletonList(key),Collections.singletonList(String.valueOf(1))); jedis.close(); if ((long)result!=0)&#123; return false; &#125;else &#123; return true; &#125; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/hlhdidi/p/6535677.html https://blog.csdn.net/ztx114/article/details/78410727 《消息队列-推/拉模式》 Kafka高吞吐量、采用拉模式。适合高IO场景。分布式发布-订阅消息系统，Kafka主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输 zero-copy机制。优点是消费端可以按处理能力进行拉去，缺点是会增加消息延迟。 p2p: producer发给queue的消息只能被一个consumer接受 pub/sub: producer发给topic的消息可以被多个subscriber接受 RabbitMQ支持事务，推拉模式都是支持、消费者默认是推模式，适合需要可靠性消息传输的场景。对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。Push方式：优点是可以尽可能快地将消息发送给消费者，缺点是如果消费者处理能力跟不上，消费者的缓冲区可能会溢出。 《RabbitMQ的应用场景以及基本原理介绍》 《消息队列之 RabbitMQ》 《RabbitMQ之消息确认机制（事务+Confirm）》 RocketMQJava实现，推拉模式都是支持，吞吐量逊于Kafka。可以保证消息顺序。 《RocketMQ 实战之快速入门》 消息的顺序问题从业务层面保证 RocketMQ通过轮询所有队列的方式来确定消息被发送到哪一个队列（负载均衡策略）。比如下面的示例中，订单号相同的消息会被先后发送到同一个队列中： 1234567891011// RocketMQ通过MessageQueueSelector中实现的算法来确定消息发送到哪一个队列上// RocketMQ默认提供了两种MessageQueueSelector实现：随机/Hash// 当然你可以根据业务实现自己的MessageQueueSelector来决定消息按照何种策略发送到消息队列中SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); &#125;&#125;, orderId); 在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的肯定是同一个队列。 12345678910111213private SendResult send() &#123; // 获取topic路由信息 TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) &#123; MessageQueue mq = null; // 根据我们的算法，选择一个发送队列 // 这里的arg = orderId mq = selector.select(topicPublishInfo.getMessageQueueList(), msg, arg); if (mq != null) &#123; return this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); &#125; &#125;&#125; 消息的重复问题 消费端处理消息的业务逻辑保持幂等性 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现 尽量使用批量方式消费方式，可以很大程度上提高消费吞吐量。 消息模型点对点消息生产者向消息队列中发送了一个消息之后，只能被一个消费者消费一次。 发布/订阅消息生产者向频道发送一个消息之后，多个消费者可以从该频道订阅到这条消息并消费。 发布与订阅模式和观察者模式有以下不同： 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，发布者与订阅者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会调用观察者的方法，然后等待方法返回；而发布与订阅模式是异步的，发布者向频道发送一个消息之后，就不需要关心订阅者何时去订阅这个消息，可以立即返回。 使用场景异步处理发送者将消息发送给消息队列之后，不需要同步等待消息接收者处理完毕，而是立即返回进行其它操作。消息接收者从消息队列中订阅消息之后异步处理。 例如在注册流程中通常需要发送验证邮件来确保注册用户身份的合法性，可以使用消息队列使发送验证邮件的操作异步处理，用户在填写完注册信息之后就可以完成注册，而将发送验证邮件这一消息发送到消息队列中。 只有在业务流程允许异步处理的情况下才能这么做，例如上面的注册流程中，如果要求用户对验证邮件进行点击之后才能完成注册的话，就不能再使用消息队列。 流量削锋在高并发的场景下，如果短时间有大量的请求到达会压垮服务器。 可以将请求发送到消息队列中，服务器按照其处理能力从消息队列中订阅消息进行处理。 应用解耦如果模块之间不直接进行调用，模块之间耦合度就会很低，那么修改一个模块或者新增一个模块对其它模块的影响会很小，从而实现可扩展性。 通过使用消息队列，一个模块只需要向消息队列中发送消息，其它模块可以选择性地从消息队列中订阅消息从而完成调用。 可靠性发送端的可靠性发送端完成操作后一定能将消息成功发送到消息队列中。 实现方法： 在本地数据库建一张消息表，将消息数据与业务数据保存在同一数据库实例里，这样就可以利用本地数据库的事务机制。事务提交成功后，将消息表中的消息转移到消息队列中，若转移消息成功则删除消息表中的数据，否则继续重传。 接收端的可靠性接收端能够从消息队列成功消费一次消息。 实现方法： 保证接收端处理消息的业务逻辑具有幂等性：只要具有幂等性，那么消费多少次消息，最后处理的结果都是一样的。 保证消息具有唯一编号，并使用一张日志表来记录已经消费的消息编号。 缺点​ 系统可用性降低，mq挂了 、 ​ 复杂性提高，重复消费，消失丢失，消息顺序性 ​ 一致性 有的写入成功了 有地写入失败了 参考资料 Observer vs Pub-Sub 消息队列中点对点与发布订阅区别 sudo yum-config-manager –add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install epel-release yum install container-selinux 使用消息队列的场景​ 应用解耦 pub/sub解耦 ​ 用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口，如果库存挂了，订单也下不了 ​ 使用队列后 用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功 库存系统 订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作 假如：在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与库存系统的应用解耦 ​ 消息异步 降低请求响应时间 ​ 用户注册后，需要发注册邮件和注册短信 ​ 流量削峰 应对突然的高并发 mq每秒消费的固定大小 ​ 秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列 用户的请求，服务器 接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面 ​ 日志 ​ 面试官：kafka、activemq、rabbitmq、rocketmq都有什么区别？ RocketMQ 的topic到几百几千吞吐量也不会受太大影响 RocketMQ Kafka吞吐量高 RabbitMQ延迟低 如何保证消息的可靠性传输啊？要是消息丢失了怎么办啊？ rabbitmq 生产者-》mq-》消费者 生产者写消息的时候，还么到mq在网络传输时丢了，或者到了mq，但mq出错没保存下来 报错了 回滚重发 事务机制是同步的，会降低吞吐量 channel设置成confirm模式 ，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 是异步的 mq接受到消息后存到内存，还没消费，自己就挂了，导致内存里的数据丢失 消息持久化到磁盘 持久化queue的元数据， deliveryMode为2 消费者接受消息，还没来得机处理就挂了，mq以为消费者处理完了 autoAck消费后自动通知mq已消费 ，但还没处理完就挂了，需要关闭autoAck,自己处理完后再发送 kafka 消费段弄丢数据 offset 等同autoAck kafka自己把数据搞丢了 broker宕机，选举时，其他的follower还有些数据没有同步 follower是副本保证在leader所在broker发生故障，进行leader切换时，数据不会丢失 给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本 在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧 在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了 在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了 生产者 设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 如何保证消息的顺序性？rabbitmq:消息串行消费 拆分多个queue，一个queue对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理 kafka:一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可 ​ 消费者multi thread处理提高吞吐量时又会顺序错乱 所以不能这么搞 换成内存队列 如何解决消息队列的延时以及过期失效问题？​ 手动写程序把那1000个订单给查出来，手动发到mq里去再补一次 消息队列满了以后该怎么处理？​ 快速消费掉所有的消息 然后补偿 有几百万消息持续积压几小时，说说怎么解决？恢复消费者 新建topic patition是原来的10倍去消费 原有的消费者，然后落库 面试官：如何保证消息不被重复消费啊？如何保证消费的时候是幂等的啊？ 设计一个消息队列（1）首先这个mq得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下kafka的设计理念，broker -&gt; topic -&gt; partition，每个partition放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给topic增加partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ （2）其次你得考虑一下这个mq的数据要不要落地磁盘吧？那肯定要了，落磁盘，才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是kafka的思路。 （3）其次你考虑一下你的mq的可用性啊？这个事儿，具体参考我们之前可用性那个环节讲解的kafka的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker挂了重新选举leader即可对外服务。 （4）能不能支持数据0丢失啊？可以的，参考我们之前说的那个kafka数据零丢失方案 消息队列，kafka，复制的底层原理，leader选举的算法，增加partition以后的rebalance算法，扣很多很多的细节，如何优化kafka写入的吞吐量 消息队列 消息的重发补偿解决思路 https://blog.csdn.net/wangtaomtk/article/details/51531278 消息的幂等性解决思路（已解答，待补充） https://www.jianshu.com/p/8b77d4583bab?utm_campaign 消息的堆积解决思路 https://docs.oracle.com/cd/E19148-01/820-0843/6nci9sed1/index.html 异步调用 解耦 ： 一个业务的非核心流程需要依赖其他系统，但结果并不重要，有通知即可。 最终一致性 ： 指的是两个系统的状态保持一致，可以有一定的延迟，只要最终达到一致性即可。经常用在解决分布式事务上。 广播 ： 消息队列最基本的功能。生产者只负责生产消息，订阅者接收消息。 错峰和流控 具体可以参考： 《消息队列深入解析》 RabbitMQ: RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗 AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。 具体可以参考： 《消息队列之 RabbitMQ》 RocketMQ： 具体可以参考： 《RocketMQ 实战之快速入门》 《十分钟入门RocketMQ》 （阿里中间件团队博客） Kafka：Kafka是一个分布式的、可分区的、可复制的、基于发布/订阅的消息系统,Kafka主要用于大数据领域,当然在分布式系统中也有应用。目前市面上流行的消息队列RocketMQ就是阿里借鉴Kafka的原理、用Java开发而得。 具体可以参考： 《Kafka应用场景》 《初谈Kafka》 ,]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E5%B9%82%E7%AD%89%E6%80%A7%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[幂等性是指一次和多次请求某一个资源应该具有同样的副作用。幂等性是系统的接口对外一种承诺(而不是实现), 承诺只要调用接口成功, 外部多次调用对系统的影响是一致的。幂等性是分布式系统设计中的一个重要概念，对超时处理、系统恢复等具有重要意义。 保证幂等的手段1.MVCC方案多版本并发控制，该策略主要使用update with condition（更新带条件来防止）来保证多次外部请求调用对系统的影响是一致的。在系统设计的过程中，合理的使用乐观锁 ，在更新的过程中利用version来防止，其他操作对对象的并发更新，导致更新丢失。为了避免失败，通常需要一定的重试机制。 12SELECT state,version FROM tb_user WHERE id=1update tb_user set state=&apos;2&apos;,version=version+1 where version=1 2.去重表 在插入数据的时候，插入去重表，利用数据库的唯一索引特性，保证唯一的逻辑。 3.悲观锁 select for update，整个执行过程中锁定该订单对应的记录。注意：这种在DB读大于写的情况下尽量少用。 4. select + insert 并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法。 5.状态机幂等 在设计单据相关的业务，或者是任务相关的业务，肯定会涉及到状态机，就是业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机，这时候，如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。 6. token机制，防止页面重复提交 业务要求：页面的数据只能被点击提交一次 发生原因：由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交解决办法： 集群环境：采用token加redis（redis单线程的，处理需要排队） 单JVM环境：采用token加redis或token加jvm内存 处理流程： 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间 提交后后台校验token，同时删除token，生成新的token返回 token特点:要申请，一次有效性，可以限流 7. 对外提供接口的api如何保证幂等 如银联提供的付款接口：需要接入商户提交付款请求时附带：source来源，seq序列号。source+seq在数据库里面做唯一索引，防止多次付款，(并发时，只能处理一个请求) 总结： 幂等性应该是合格程序员的一个基因，在设计系统时，是首要考虑的问题，尤其是在像支付宝，银行，互联网金融公司等涉及的都是钱的系统，既要高效，数据也要准确，所以不能出现多扣款，多打款等问题，这样会很难处理，用户体验也不好 。 8. 业务逻辑幂等 比如接到支付成功的消息订单状态变成支付完成，如果当前状态是支付完成，则再收到一个支付成功的消息则说明消息重复了，直接作为消息成功处理。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[两阶段提交、多阶段提交 《关于分布式事务、两阶段提交协议、三阶提交协议》 分布式一致方 《分布式系统事务一致性解决方案》 《保证分布式系统数据一致性的6种方案》 分布式事务 本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 （1）分布式事务了解吗？你们如何解决分布式事务问题的？TCC如果出现网络连不通怎么办？XA的一致性如何保证？ （1）两阶段提交方案/XA方案 所以这个就是所谓的XA事务，两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复ok，那么就正式提交事务，在各个数据库上执行操作；如果任何一个数据库回答不ok，那么就回滚事务。 这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于spring + JTA就可以搞定，自己随便搜个demo看看就知道了。 这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几百个服务，几十个服务。一般来说，我们的规定和规范，是要求说每个服务只能操作自己对应的一个数据库。 如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，经常数据被别人改错，自己的库被别人写挂。 如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许你交叉访问别人的数据库！ （2）TCC方案 Try、Confirm、Cancel 这个其实是用到了补偿的概念，分为了三个阶段： 给大家举个例子吧，比如说跨银行转账的时候，要涉及到两个银行的分布式事务，如果用TCC方案来实现，思路是这样的： 1）Try阶段：先把两个银行账户中的资金给它冻结住就不让操作了 2）Confirm阶段：执行实际的转账操作，A银行账户的资金扣减，B银行账户的资金增加 3）Cancel阶段：如果任何一个银行的操作执行失败，那么就需要回滚进行补偿，就是比如A银行账户如果已经扣减了，但是B银行账户资金增加失败了，那么就得把A银行账户资金给加回去 这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心,很难维护。 比较适合的场景：一致性要求很高，比如常见的就是资金类的场景，那你可以用TCC方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否ok，不ok就执行补偿/回滚代码，严格保证资金的正确性 （3）本地消息表 1月份做的统一网关支付平台，CMBB2B的前置机模式要求要和前置机放在一起，为了方便处理方便将银行的后台通知放在了客户端接受，再发给网关，网关更新支付状态成功后，再告诉客户端结果，再让客户端更新，更新成功再返回成功接受的报文，告诉银行不需要再发送通知了。整个过程是同步的，响应过慢可能会造成线程阻塞卡死，想了想好像跟分布式事务有点关系 1）A系统在自己本地一个事务里操作同时，插入一条数据到消息表 2）接着A系统将这个消息发送到MQ中去 3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息 4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态 5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理 6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止 一般确实很少用，这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务，高并发场景不适合，不好扩展 （4）可靠消息最终一致性方案 这个的意思，就是干脆不要用本地的消息表了，直接基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务。 大概的意思就是： 1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了 2）如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息 3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务 4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。 5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用RocketMQ支持的，要不你就自己基于类似ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的 （5）最大努力通知方案 允许少数的分布式事务失败，用于对分布式事务不严格的情况，比如记录日志 这个方案的大致意思就是： 1）系统A本地事务执行完之后，发送个消息到MQ 2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统B的接口 3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃 （6）你们公司是如何处理分布式事务的？ 一个严格资金要求绝对不能错的场景，你可以说你是用的TCC方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案 要用分布式事务的时候，一定是有成本，代码会很复杂，开发很长时间，性能和吞吐量下跌，系统更加复杂更加脆弱反而更加容易出bug；好处，如果做好了，TCC、可靠消息最终一致性方案，一定可以100%保证你那快数据不会出错。 资金、交易、订单，会用分布式事务方案来保证 背景数据库事务ACID mysql的事务处理过程 记录redo和undo.log文件，确保日志在磁盘上的持久化 更新数据记录 提交事务，redo写入commit记录 分库分表变成几个库了 #### 2PC2-phase-commit 2PC顾名思义分为两个阶段，其实施思路可概括为： （1）投票阶段（voting phase）： ​ 1. 协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果。 事务参与者收到请求之后，执行事务，但不提交，并记录事务日志。 参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。 （2）提交阶段（commit phase）：收到参与者的通知后，协调者再向参与者发出通知，根据反馈情况决定各参与者是否要提交还是回滚； 所有的参与者回复能够正常执行事务 ​ 协调者向各个参与者发送commit通知，请求提交事务。 参与者收到事务提交通知之后，执行commit操作，然后释放占有的资源。 参与者向协调者返回事务commit结果信息。 一个或多个参与者回复事务执行失败 ​ 协调者向各个参与者发送事务rollback通知，请求回滚事务。 参与者收到事务回滚通知之后，执行rollback操作，然后释放占有的资源。 参与者向协调者返回事务rollback结果信息。 协调者等待超时。 ​ tm向所/有的ap发送事务内容，询问是否可以执行事务的提交操作，并等待各个ap的响应 都为yes 发送commit请求执行事务 commit操作很快，失败概率比较小，增大了事务的成功概率 问题： 单点问题协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，那么就会影响整个数据库集群的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。 同步阻塞两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率及其低下。 数据不一致性两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了 事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这 时候就产 生了数据的不一致性 3pc canCommit 协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复 各个参与者依据自身状况回复一个预估值，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否则返回否定信息 preCommit doCommit jotm atomikos https://blog.csdn.net/sofia1217/article/details/53968177]]></content>
  </entry>
  <entry>
    <title><![CDATA[CAP理论]]></title>
    <url>%2F2019%2F05%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%2FCAP%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[一致性(Consistency) 多个数据副本能保持一致，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。如果系统的一个数据更新成功之后，所有用户都能够读取到最新的值，系统就被认为具有强一致性。 可用性(Availability) 分布式系统在面对各种异常时都可以提供正常服务，对于用户的每一个操作、请求总是能够在有限的时间内返回结果。 分区容忍性(Partition Tolerance) 分布式系统在遇到任何网络分区故障的时候，仍然能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。 分布式系统不可能同时满足CAP，最多只能同时满足其中两项，这就是CAP理论。在分布式系统中分区容忍性必不可少，所以CAP理论实际上是要在可用性和一致性之间做取舍。 BASE理论是对CAP理论中一致性和可用性权衡的结果，如果无法做到强一致性，那就要采取合适的方法使系统达到最终一致性。传统的数据库系统要求强一致性(ACID)，BASE理论强调通过牺牲强一致性来达到可用性。在实际业务场景中，要结合业务对一致性的要求，将ACID和BASE结合起来使用。 基本可用(BasicallyAvailable) base available 分布式系统在出现故障的时候，保证核心功能可用，允许损失部分可用性。 ##软状态(SoftState) software 允许系统中的数据存在中间状态，即系统不同节点的数据副本之间进行同步的过程存在时间延迟 ##最终一致性(EventuallyConsistent) eventually consistent 系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。 eureka强调了cap中的ap zookeeper强调 cp]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>CAP</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2F%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Java程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，也就是说，方法不能修改传递给它的任何参数变量的内容。 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub Student s1 = new Student("小张"); Student s2 = new Student("小李"); Test.swap(s1, s2); System.out.println("s1:" + s1.getName()); System.out.println("s2:" + s2.getName()); &#125; public static void swap(Student x, Student y) &#123; Student temp = x; x = y; y = temp; System.out.println("x:" + x.getName()); System.out.println("y:" + y.getName()); &#125;&#125; 结果： 1234x:小李y:小张s1:小张s2:小李 下面再总结一下Java中方法参数的使用情况： 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型》 一个方法可以改变一个对象参数的状态。 一个方法不能让对象参数引用一个新的对象。 什么是反射机制？反射机制的应用场景有哪些？ 推荐阅读： Reflection：Java反射机制的应用场景 Java基础之—反射（非常重要） 重载和重写的区别重载： 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写： 发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为private则子类就不能重写该方法。 ###7. 为什么要使用索引？索引这么多优点，为什么不对表中的每一个列创建一个索引呢？索引是如何提高查询速度的？说一下使用索引的注意事项？Mysql索引主要使用的两种数据结构？什么是覆盖索引? 为什么要使用索引？ 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。 帮助服务器避免排序和临时表 将随机IO变为顺序IO 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 索引这么多优点，为什么不对表中的每一个列创建一个索引呢？ 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引是如何提高查询速度的？ 将无序的数据变成相对有序的数据（就像查目录一样） 说一下使用索引的注意事项 避免 where 子句中对宇段施加函数，这会造成无法命中索引。 在使用InnoDB时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。 将打算加索引的列设置为 NOT NULL ，否则将导致引擎放弃使用索引而进行全表扫描 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 chema_unused_indexes 视图来查询哪些索引从未被使用 在使用 limit offset 查询缓慢时，可以借助索引来提高性能 Mysql索引主要使用的哪两种数据结构？ 哈希索引：对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 BTree索引：Mysql的BTree索引使用的是B树中的B+Tree。但对于主要的两种存储引擎（MyISAM和InnoDB）的实现方式是不同的。 更多关于索引的内容可以查看我的这篇文章：【思维导图-索引篇】搞定数据库索引就是这么简单 什么是覆盖索引? 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次,这样就会比较慢。覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 线程间的几种通信方式知道不？ 1、锁机制 互斥锁：提供了以排它方式阻止数据结构被并发修改的方法。 读写锁：允许多个线程同时读共享数据，而对写操作互斥。 条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 2、信号量机制：包括无名线程信号量与有名线程信号量 3、信号机制：类似于进程间的信号处理。 线程间通信的主要目的是用于线程同步，所以线程没有象进程通信中用于数据交换的通信机制。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[一 OSI与TCP/IP各层的结构与功能,都有哪些协议五层协议的体系结构 1 应用层应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的 HTTP协议，支持电子邮件的 SMTP协议等等。我们把应用层交互的数据单元称为报文。 域名系统 域名系统(Domain Name System缩写 DNS，Domain Name被译为域名)是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的 Web 网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM 公司的域名是 www.ibm.com、Oracle 公司的域名是 www.oracle.com、Cisco公司的域名是 www.cisco.com 等。 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的 WWW（万维网） 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。（百度百科） 2 运输层运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 运输层主要使用以下两种协议 传输控制协议 TCP（Transmisson Control Protocol）–提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）–提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 UDP 的主要特点 UDP 是无连接的； UDP 使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP 是面向报文的； UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如 直播，实时视频会议等）； UDP 支持一对一、一对多、多对一和多对多的交互通信； UDP 的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 TCP 的主要特点 TCP 是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条 TCP 连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）； TCP 提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP 中的“流”（Stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和 TCP 的交互是一次一个数据块（大小不等），但 TCP 把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 3 网络层在 计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。 这里要注意：不要把运输层的“用户数据报 UDP ”和网络层的“ IP 数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。 这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称. 互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Prococol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。 4 数据链路层数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装程帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。 5 物理层在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。 TCP通信原理对于TCP通信来说，每个Socket的内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式及TCP的滑动窗口就是依赖于这两个独立的缓冲区和缓冲区填充状态。 接收缓冲区把数据缓存到内核，若应用进程一直没有调用Socket的read方法进行读取，那么该数据会一直被缓存在接收缓冲区内，不管进程是否读取Socket，对发送端发来的数据都会经过内核接收并缓存到Socket的内核接受缓冲区。 TCP报文段首部格式 序号seq：用于对字节流进行编号，例如序号为 101，表示第一个字节的编号为 101，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 201。 确认号ack：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认ACK：当 ACK=1 时确认号字段有效，否则无效。TCP 规定在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步SYN：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止FIN：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 三次握手 第一次握手：Client 向 Server 发送连接请求报文SYN=1，随机产生一个值seq= x给Server，Client 进入SYN_SENT状态，等待Server确认。 第二次握手：Server 收到连接请求报文，由标志位SYN=1知道Client请求建立连接,Server 向Client 发送连接确认报文SYN=1，ACK=1，seq为 x+1，同时随机产生一个值seq= y,Server进入SYN_RCVD状态 第三次握手：Client 收到 Server 的连接确认报文后,检查ACK是否为1，ack是否为x+1,如果正确则还要向 Server 发出确认，ack为 y+1,将标志位ACK置为1。Server检查ack是否为y+1,ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，开始传输数据。 SYN攻击时一种典型的DDOS攻击，就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。 三次握手的原因第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。客户端发送的连接请求如果在网络中延迟，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新请求连接。但是这个滞留的连接请求最后还是会到达服务器，如果不进行三次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 四次挥手 第一次挥手：Client 发送连接释放报文，FIN=1，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server 收到FIN后之后发出确认，此时 TCP 属于半关闭状态，Server 能向 Client 发送数据但是 Client 不能向 Server 发送数据。发送一个ACK给Client，ack为u+1后进入CLOSE_WAIT状态。 第三次挥手：当Server 不再需要连接时，发送连接释放报文，FIN=1用来关闭Server到Client的数据传送，Server进入LAST_ACK状态 第四次挥手：Client 收到后FIN后进入 TIME-WAIT 状态，发出确认，等待 2 MSL（最大报文存活时间）后释放连接。Server 收到 Client 的确认后进入CLOSED状态 四次挥手的原因客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 TIME_WAIT客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由 确保最后一个确认报文能够到达。如果Server 没收到Client 发送来的确认报文，那么就会重新发送连接释放请求报文，Client 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 滑动窗口窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。 超时重发当发送者向接收者发包后，如果过了一段时间(超时时间)依然没有收到消息，就当做本次包丢失，需要重新补发。并且如果一次性发了三个包，只要最后一个包确认收到之后就默认前面两个也收到了。 二 TCP 三次握手和四次挥手(面试常客)TCP FLAGS ACK确认序号标志 syn同步序号 fin finish标志 释放链接 为了准确无误地把数据送达目标处，TCP协议采用了三次握手策略。 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端 为什么要三次握手为了初始化sequence num 的初始值 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己接收正常，对方发送正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 为什么要传回 SYN接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 SYN 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement[汉译：确认字符 ,在数据通信传输中，接收站发给发送站的一种传输控制字符。它表示确认发来的数据已经接受无误。 ]）消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。 传了 SYN,为啥还要传 ACK双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要 ACK 信号来进行验证。 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个FIN给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1 为什么要四次挥手任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。 举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 上面讲的比较概括，推荐一篇讲的比较细致的文章：https://blog.csdn.net/qzcsu/article/details/72861891 三 TCP、UDP 协议的区别UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的运输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 四 TCP 协议如何保证可靠传输 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 停止等待协议 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 停止等待协议 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组； 在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认； 1) 无差错情况: 发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。 2) 出现差错情况（超时重传）:停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 3) 确认丢失和确认迟到 确认丢失：确认消息在传输过程丢失 当A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施： 丢弃这个重复的M1消息，不向上层交付。 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。 确认迟到 ：确认消息在传输过程中迟到A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下： A收到重复的确认后，直接丢弃。 B收到重复的M1后，也直接丢弃重复的M1。 拥塞控制在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。 拥塞避免： 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加1. 快重传与快恢复：在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 五 在浏览器中输入url地址 -&gt;&gt; 显示主页的过程（面试常客）百度好像最喜欢问这个问题。 打开一个网页，整个过程会使用哪些协议 图解（图片来源：《图解HTTP》）： 总体来说分为以下几个过程: DNS解析 TCP连接 发送HTTP请求 服务器处理请求并返回HTTP报文 浏览器解析渲染页面 连接结束 具体可以参考下面这篇文章： https://segmentfault.com/a/1190000006879700 八 HTTP长连接、短连接在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 —— 《HTTP长连接、短连接究竟是什么？》 写在最后计算机网络常见问题回顾 ①TCP三次握手和四次挥手、 ②在浏览器中输入url地址-&gt;&gt;显示主页的过程 ③HTTP和HTTPS的区别 ④TCP、UDP协议的区别 ⑤常见的状态码。 参考 https://blog.csdn.net/qq_16209077/article/details/52718250 https://blog.csdn.net/zixiaomuwu/article/details/60965466 https://blog.csdn.net/turn__back/article/details/73743641 5. IP地址与MAC地址的区别参考：https://blog.csdn.net/guoweimelon/article/details/50858597 IP地址是指互联网协议地址（Internet Protocol Address）IP Address的缩写。IP地址是IP协议提供的一种统一的地址格式，它为互联网上的每一个网络和每一台主机分配一个逻辑地址，以此来屏蔽物理地址的差异。 MAC 地址又称为物理地址、硬件地址，用来定义网络设备的位置。网卡的物理地址通常是由网卡生产厂家写入网卡的，具有全球唯一性。MAC地址用于在网络中唯一标示一个网卡，一台电脑会有一或多个网卡，每个网卡都需要有一个唯一的MAC地址。 网络模型 《web优化必须了解的原理之I/o的五种模型和web的三种工作模式》 五种I/O模型：阻塞I/O，非阻塞I/O，I/O复用、事件(信号)驱动I/O、异步I/O，前四种I/O属于同步操作，I/O的第一阶段不同、第二阶段相同，最后的一种则属于异步操作。 三种 Web Server 工作方式：Prefork(多进程)、Worker方式(线程方式)、Event方式。 《select、poll、epoll之间的区别总结》 select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。 select 有打开文件描述符数量限制，默认1024（2048 for x64），100万并发，就要用1000个进程、切换开销大；poll采用链表结构，没有数量限制。 select，poll “醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，通过回调机制节省大量CPU时间；select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，而epoll只要一次拷贝。 poll会随着并发增加，性能逐渐下降，epoll采用红黑树结构，性能稳定，不会随着连接数增加而降低。 《select，poll，epoll比较 》 在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。 《深入理解Java NIO》 NIO 是一种同步非阻塞的 IO 模型。同步是指线程不断轮询 IO 事件是否就绪，非阻塞是指线程在等待 IO 的时候，可以同时做其他任务 《BIO与NIO、AIO的区别》 《两种高效的服务器设计模型：Reactor和Proactor模型》 Epoll 《epoll使用详解（精髓）》 kqueue 《kqueue用法简介》 连接和短连接 《TCP/IP系列——长连接与短连接的区别》]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2F%E6%8E%92%E6%9F%A5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[九大Java性能调试工具 通过 Java Flight Recorde 监控应用是否大量出现了某种类型的异常。如果有，那么异常可能就是个突破点，如果没有，可以先检查系统级别的资源等情况，监控CPU、内存等资源是否被其他进程大量占用，并且这种占用是否不符合系统正常运行状况。监控Java服务自身，例如GC日志里面是否观察到Full GC等恶劣情况出现，或者是否Minor GC在变长等；利用jstat等工具，获取内存使用的统计信息也是个常用手段；利用jstack等工具检查是否出现死锁等。如果还不能确定具体问题，对应用进行Profling也是个办法，但因为它会对系统产生侵入性，如果不是非常必要，大多数情况下并不建议在生产系统进行。定位了程序错误或者JVM配置的问题后，就可以采取相应的补救措施，然后验证是否解决，否则还需要重复上面部分过程。系统性能分析中，CPU、内存和IO是主要关注项。对于CPU，如果是常见的Linux，可以先用top命令查看负载状况，进一步的排查有很多思路，怎么找到最耗费CPU的Java线程，简要介绍步骤：利用top命令获取相应pid，“-H”代表thread模式，你可以配合grep命令更精准定位。top –H然后转换成为16进制。最后利用jstack获取的线程栈，对比相应的ID即可。当然，还有更加通用的诊断方向，利用vmstat之类，查看上下文切换的数量，比如下面就是指定时间间隔为1，收集10次。vmsat -1 -10如果每秒上下文（cs，context switch）切换很高，并且比系统中断高很多（in，system interrupt），就表明很有可能是因为不合理的多线程调度所导致。当然还需要利用pidstat等手段，进行更加具体的定位，我就不再进一步展开了。除了CPU，内存和IO是重要的注意事项，比如：利用free之类查看内存使用。或者，进一步判断swap使用情况，top命令输出中Virt作为虚拟内存使用量，就是物理内存（Res）和swap求和，所以可以反推swap使用。显然，JVM是不希望发生大量的swap使用的。 对于IO问题，既可能发生在磁盘IO，也可能是网络IO。例如，利用iostat等命令有助于判断磁盘的健康状况。对于JVM层面的性能分析，利用JMC、JConsole等工具进行运行时监控。利用各种工具，在运行时进行堆转储分析，或者获取各种角度的统计数据（如jstat -gcutil分析GC、内存分带等）。GC日志等手段，诊断Full GC、Minor GC，或者引用堆积等。对于应用Profling，简单来说就是利用一些侵入性的手段，收集程序运行时的细节，以定位性能问题瓶颈。所谓的细节，就是例如内存的使用情况、最频繁调用的方法是什么，或者上下文切换的情况等。我建议使用JFR配合JMC来做Profling，因为它是从Hotspot JVM内部收集底层信息，并经过了大量优化，性能开销非常低，通常是低于 2% 的；并且如此强大的工具，也已经被Oracle开源出来！所以，JFR/JMC完全具备了生产系统Profling的能力，目前也确实在真正大规模部署的云产品上使用过相关技术，快速地定位了问题。它的使用也非常方便，你不需要重新启动系统或者提前增加配置。例如，你可以在运行时启动JFR记录，并将这段时间的信息写入文件：Jcmd JFR.sart duration=120s flename=myrecording.jfr然后，使用JMC打开“.jfr文件”就可以进行分析了，方法、异常、线程、IO等应有尽有，其功能非常强大。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Spark的数据节点可以在一个物理节点中，Spark是一种基于内存的计算框架，有时也会用磁盘shuffle MapReduce的计算模型非常固定必须基于磁盘，适合对速度不敏感的离线批处理任务 SparkStreaming 实时流计算 Flink beam Storm 《最详细的Storm入门教程》 Kafka Stream 《Kafka Stream调研：一种轻量级流计算模式》 《史上最详细的Hadoop环境搭建》 HDFS 《【Hadoop学习】HDFS基本原理》 MapReduce 《用通俗易懂的大白话讲解Map/Reduce原理》 《 简单的map-reduce的java例子》 Yarn 《初步掌握Yarn的架构及原理》 Spark 《Spark(一): 基本架构及原理》]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2F%E4%B8%93%E4%B8%9A%E5%8D%95%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[单词第1天任务 rape [reip] n.强奸;破坏,蹂躏vt.强奸;破坏,蹂躏 rash [ræʃ] a.轻率的,鲁莽的[反]deliberate n.皮疹 gut [ɡʌt] n.[pl.]胆量;内脏a.本能的vt.取出内脏 refund [ˈri:fʌnd] n.v. 退款;赔偿 relish [ˈrelɪʃ] n.喜好,乐趣;美味;胃口v.爱好,喜欢 remnant [‘remnənt] n.残余,剩余;残余物,残存部分 renaissance [ri’neisns] n.[the R-]文艺复兴(时期);新生,复兴 habitat [ˈhæbɪtæt]n.(动物的)栖息地,(植物的)产地 harassment [‘hærəsmənt] n.骚扰,扰乱;烦恼,烦乱 redundant [ri’dʌndənt] a.被解雇的;多余的,过剩的 refugee [ˌrefjuˈdʒi:] n.(政治上的)避难者,难民 ritual [ˈrɪtʃuəl] a.宗教仪式的,典礼的n.仪式,典礼;惯例robust [rəu’bʌst]a.强健的,茁壮的;有力的[反]feeble salvation [sæl’veiʃən]n.(尤指基督)救世,超度;拯救,解困 sanction [‘sæŋkʃən]v.同意(某事),批准,认可n.批准,国际制裁repression [rɪˈpreʃn] n.压抑,压制,镇压 sane [seɪn] a.心智健全的,神志清醒的;明断的,理智的sarcastic [sɑ:’kæstik]a.讽刺的,嘲笑的,挖苦的 saturate [‘sætʃəreit]vt.使湿透,浸透;使充满,使饱和 repertoire [‘repətwɑ:]n.全部剧目,保留剧目,全部技能 retention [ri’tenʃən]n.保留;保持;记忆(力) retort [ri’tɔ:t]n.v.报复;反击;反驳 bowel [‘bauəl]n.肠;[pl.]内部,深处 retrieve [rɪˈtri:v] vt.重新得到,取回;挽回,补救;检索 retrospect [‘retrəspekt]v.n.回顾,回想,追溯[反]foresee revelation [,revə’leiʃən]n.揭示,透露,显示;被揭示的真相,新发现 第2天任务 rigorous [ˈrɪgərəs] a.严格的,严厉的,严酷的(气候条件) riot [ˈraɪət] n.暴(骚)乱;(色彩等)极度丰富vi.聚众闹事 rip [rɪp] v.猛撕开;扯开;被撕开;裂开 scandal [ˈskændl] n.丑事,丑闻;流言蜚语;反感,愤慨 scramble [‘skræmbl]vi.(快速地)爬,攀登;互相争夺,争先 siege [si:dʒ] n.包围,围攻,围困 sip [sɪp] v.小口地喝,抿,呷n.一小口的量 sceptical/skeptical [ˈskeptɪkl] a.怀疑的 skim [skɪm] vt.撇去(液体表面)之漂浮物vi.轻轻掠过 radiant [‘reidiənt]a.(面容、目光)洋溢着幸福的,明亮照耀的 rap [ræp]vt.急敲;突然厉声说出;训斥n.急敲(声) thermal [‘θə:məl]热的,由热造成的~burns texture [ˈtekstʃə(r)] n.(织物)质地;(材料)构造;结构;肌理transcend [trænˈsend] vt. 超出,超越(理性等)的范围 abolish [əˈbɒlɪʃ] 彻底废除(法律、制度、习俗等)abolish old custom prime [praɪm] 首要的,主要的a matter of ~importance foul [faʊl] a. 令人不快的;恶臭的;邪恶的v.弄脏n.(体育等)犯规thirst [θɜ:st] 渴,口渴The heat creates a ~in me like I’ve never had before trail [treil]痕迹,足迹a ~of destruction left by the violence tramp [træmp]步行,跋涉He loves ~ing over the hills transfer [trænsˈfɜ:(r)] 转移,调动the company has transferred to an eastern location fringe [frindʒ] n.边缘;(窗帘)缘饰;额前垂发vt.饰…的边 fume [fju:m]n.(usu pl.)(浓烈或难闻的)烟,气,汽 blur [blə:] n.模糊不清的事物vt.使模糊,使看不清楚 browse [brauz]vi.随意翻阅,浏览;(牛、羊等)吃草 primitive [‘primitiv]原始的,上古的,早期的a ~forest prior [‘praiə]在先的,在前的Stop making public statements without their ~approval here probe [prəʊb] vt . 探索,探查,调查~the roots of war vertical [ˈvɜ:tɪkl] adj. 垂直的,立式的a ~engine veto [‘vi:təu ] n. 否决,否决权,否决权的行使The rest of the committee could not accept the ~ absolute [ˈæbsəlu:t] adj 纯粹的,完全的have absolute trust in sb. verge [vɜ:dʒ] n 边,边缘,边沿the ~of a stream accustom [əˈkʌstəm] vt. 使习惯于accustom oneself to rising (to rise) early acquaint [ə’kweint] vt. 使认识,使了解 第3天任务 trash [træʃ] n.垃圾;拙劣的作品;渣滓,败类vt.捣毁 toxic [ˈtɒksɪk] adj.有毒的,因中毒引起的 amaze [əˈmeɪz] vt. 使惊愕,使惊异I am amazed that he should get the post. breach [bri:tʃ] n.违反,不履行;破裂vt.冲破,攻破 briefcase [ˈbri:fkeɪs] n.手提箱,公事皮包 ambiguous [æmˈbɪgjuəs] adj.含糊不清的,模棱两可的an ambiguous ozone [ˈəʊzəʊn] n. 臭氧;清新的空气 script [skrɪpt] n.剧本(原稿);手稿,原稿;笔迹,手迹 scrutiny [ˈskru:təni] n.周密的调查;仔细看;监视;选票复查sculpture [ˈskʌlptʃə(r)] n.雕刻(术),雕塑(术);雕刻作品 pact [pækt] n.合同,条约,公约 trace [treɪs] n. 痕迹,踪迹The wound healed, leaving almost no ~ of a scar track [træk] n.足迹,踪迹~s in the snow tragedy [ˈtrædʒədi] n. 悲剧Hamlet is one of shakespeare’s best known tragedies paperback [ˈpeɪpəbæk] n.平装本,简装本 tame [teɪm] adj.驯化的,驯服的,温顺的~animails tan [tæn] n.棕黄色,黄褐色buy some shoes in tan abrupt [əˈbrʌpt]adj. 突然的,意外的an abrupt departure versatile [ˈvɜ:sətaɪl] adj. 多才多艺的a ~man versus [ˈvɜ:səs] prep.(介词)诉讼,竞赛等中以……为对手The match tonight is China ~Japan blunt [blʌnt] adj.钝的The sun was blazing down and the heat was oppressive. Blush [blʌʃ] (因害羞、激动、窘困)脸红blush with(或for) joy abide [əˈbaɪd] vt.遵循(…by);容忍The one thing she cannot abide is lying. delight [dɪˈlaɪt] n. 高兴,快乐:To the teacher’s great delight , all his students passed the examination. deliver [dɪˈlɪvə(r)] vi. 投递,传送,运输:deliver the mail demonstrate [ˈdemənstreɪt] vt.论证,证明:demonstrate a philosophical principle accelerate [əkˈseləreɪt] vt. 加速,增速accelerate one’s steps thread [θred] n.线,线状物cotton ~ thrill [θrɪl] vt.使非常兴奋,使非常激动It’s a sight that never fails to ~me thrive [θraɪv] vi.兴旺发达,繁荣His business is thriving abuse [əˈbju:s]n. vt. 滥用abuse one’s authority(office) stress [stres]n.压力,紧张Not all of us can cope with the ~es of modern life stretch [stretʃ] v. 伸直,伸长He yawned and ~ed himself stride [straɪd] vt.&amp; vi. 阔步前进,大踏步走He strode to the platform temper [ˈtempə(r)] n. 心情,脾气be in a good~ tempt [tempt] vt. 引诱,诱惑Nothing could ~me to do such a thing strap [stræp] n. 带,皮带silks ~s 1 strike [straɪk] vi.打击,撞 A stone struck me on the head strip [strɪp] vi.&amp;vt剥去,除去The wind stripped the tree of all its leaves tip [tɪp] 末端,尖端the tips of the fingers toast [təʊst] n.干杯;烤面包,吐司two slices of ~】 第4天任务 token [ˈtəʊkən] n.&amp;vt.表示,标志,象征He did that as a ~of good faith torture [ˈtɔ:tʃə(r)] n.&amp;vt拷打,拷问~a confession from a prisoner accommodate[əˈkɒmədeɪt] vt.使适应,使符合一致accommodate oneself to changed circumstances unfold [ʌnˈfəʊld] vt.&amp; vi. 展开,打开~a newspaper uniform [ˈju:nɪfɔ:m] n.制服;adj.不变的,相同的,均匀,统一的a ~ temperature unify [ˈju:nɪfaɪ] vt.使成一体,统一become a unified nation adhere [ədˈhɪə(r)] vi&amp;vt 黏附,附着Paste is used to make one surface adhere to adjacent [əˈdʒeɪsnt]adj. 临近的,毗邻的(to) a city and its adjacent suburbs upgrade [ˌʌpˈgreɪd] vt. 提高,使升级,改善~products and services uphold [ʌpˈhəʊld]vt. 举起,高举He upheld his clenched hand upright [ˈʌpraɪt] adj. 挺直的,垂直的an ~seat tick [tɪk] n.(钟声等发出的)滴答声 tilt [tɪlt]vt&amp;vi.使倾斜,使倾倒Tilt your head back so that I can look down your throat toss [tɒs] vt.&amp; vi. 扔,抛,掷The children tossed the ball to each other tough [tʌf]adj. 坚韧的,牢固的Some plastic are as ~as metal tow [təʊ]vt.拖,拉,牵引tow a damaged ship into port abstract [ˈæbstrækt] adj. 抽象的 A flower is beautiful, but beauty itself is abstract. absurd [əbˈsɜ:d]adj. 荒谬的,荒唐的The idea that number 14 brings bad luck is absurd. uproar [ˈʌprɔ:(r)]n. 骚乱,骚动The public ~over unclear-radiation hazards continues to mount utilize [ˈju:təlaɪz]vt.利用~solar energy utter [ˈʌtə(r)]vt. 发出(声音等),说,吐露~the truth vacant [ˈveɪkənt] adj. 未被占用的a ~seat in a bus vague [veɪg]adj. 含糊的,不明确的,模糊的a ~answer slip [slɪp] vi&amp;vt. 滑倒,滑落,滑行She ~ed on the wet stones and fell slit [slɪt]vt. 切开,撕开~the envelope open slope [sləʊp]vi. 倾斜,有坡度The ground ~s down sharply at this point impact [ˈɪmpækt] n.冲击,撞击:the impact of light on the eye vt.挤入,压紧; 撞击; 对…产生影响; impair [ɪmˈpeə(r)] vt.削弱,减少:The output of produce was impaired by the bad weather impartial [ɪmˈpɑ:ʃl]adj. 公正的,无偏见的:A judge should be impartial implement [ˈɪmplɪment]n.工具,器具,用具:new types of farm implements sly [slaɪ] adj.狡猾的,偷偷的a ~answer smash [smæʃ] vt. 打碎,打破,摧毁The ball ~ed the window snatch [snætʃ] vt. 夺得,一把抓住The thief ~ed her handbag and ran off immune [ɪˈmju:n] adj.免除的,豁免的:Nobody is immune from criticism sting [stɪŋ] vt.&amp;vi刺,螫,叮 A bee stung him on the neck stir [stɜ:(r)] vt.&amp; vi.使微动,移动A breeze ~red my hair stitch [stɪtʃ] n.一针,针脚Make your ~es closer together sue [su:] vt.&amp; vi控告,起诉~sb for slander superb [su:ˈpɜ:b] adj. 极好的,一流的,杰出的~science and engineering superficial [ˌsu:pəˈfɪʃl]adj.表面的a ~resemblance superfluous [su:ˈpɜ:fluəs]adj.过多的,剩余的,多余的a ~remark ventilate [ˈventɪleɪt] vt.使通风,使空气流通~a room venture [ˈventʃə(r)]n.冒险,冒险行动,投机行动take a ~in oil uncover [ʌnˈkʌvə(r)] vt. 揭开,揭露~a dish of food supervise [ˈsu:pəvaɪz] vt.&amp; vi 监督,管理,指导~sb’s every move vanish [ˈvænɪʃ] vi.突然消失,逐渐消散Do you want to vanish veil [veɪl] n. 面纱,面罩Jewish women wore ~s in token of reverence and submission underestimate [ˌʌndərˈestɪmeɪt] 低估~the difficulties of the task reassure [ˌri:əˈʃʊə(r)] v.使安心,使放心;使消除疑虑 第5天任务 recede [rɪˈsi:d] v.退去;缩进;收回,撤回[反]approach recipient [rɪˈsɪpiənt] n.a person who receives sth 收到者 reckless [ˈrekləs] a.不注意的,大意的,卤莽的,不顾后果的 rectangle [ˈrektæŋgl] n.[数]矩形, 长方形 recur [rɪˈkɜ:(r)] v.(尤指不好的事)一再发生;重现 portray [pɔ:ˈtreɪ] v.描写,描述;画(人物、景象等) prone [prəʊn] a.(to)易于…的,很可能…的;俯卧的 prophet [ˈprɒfɪt] n.预言家;先知;提倡者 undergo [ˌʌndəˈgəʊ]vt.经历,遭受,忍受~much suffering underlie [ˌʌndəˈlaɪ] vt. 位于……之下Many facts ~my decision undermine [ˌʌndəˈmaɪn] v. 在……下挖~a wall undertake [ˌʌndəˈteɪk] vt. 着手做,从事~an attack undesirable [ˌʌndɪˈzaɪərəbl]adj. 不合意的,不受欢迎的The drug has ~ side effects uneasy [ʌnˈi:zi]adj. 心神不宁的,不安宁的be ~uneasy about the future accord [əˈkɔ:d] vt. 使符合,相一致(with) His violent action do not accord with his peaceful words. account [əˈkaʊnt]n.记述,叙述give a brief account of what has happened ultimate [ˈʌltɪmət] adj.最后的,最终的The war ended for us in ~ unanimous [juˈnænɪməs]adj.全体一致的,一致同意的They are ~in asking for a rise in pay virtue [ˈvɜ:tʃu:] 美德,德行Among her many ~s are loyalty ,courage, and truthfulness visible [ˈvɪzəbl]adj. 看得见的,可见的 a ship barely ~on the horizon vision [ˈvɪʒn]n.视力,视觉~care substitute [ˈsʌbstɪtju:t] n.代替者,代替物He is the doctor’s ~ during holidays times subtle [ˈsʌtl]adj.微妙的,巧妙的His whole attitude has undergone a ~change subtract [səbˈtrækt] vt.减,减去Subtract 4 from 10 you get 6 succession [səkˈseʃn]n.连续,一系列a ~of debates suck [sʌk] vt. 吸,A large mosquito was sucking blood from the back of her hand vital [ˈvaɪtl] adj.维持生命所必需的;有生命力的The heart is a ~ organ vivid [ˈvɪvɪd]adj.鲜艳的the ~green of leaves in spring wedge [wedʒ]n.楔子,三角木Please put a ~in the door so that it will stay open weed [wi:d]n.杂草,野草My garden is running to ~s weep [wi:p] vt.&amp; vi.(通常因悲伤)哭泣,流泪~for one’s failure weigh [weɪ] vt.称……重量,称~oneself on the scales Xerox [ˈzɪərɒks]n.静电复印(法),静电复印件,静电复印机Make me a ~of this report. yield [ji:ld]vt.生产,产生,带来This orchard ~s apples and pears. zeal [zi:l] n.热心,热忱,热情a man of ~ vicious [ˈvɪʃəs]adj.邪恶的,恶毒的,凶残的lead a ~life volunteer [ˌvɒlənˈtɪə(r)]n.志愿者,自愿参加者Are there any ~s for a swim? 2 Zigzag [ˈzɪgzæg]adj.之字形的,弯曲的 a ~road Wander [ˈwɒndə(r)]vt.&amp;vi漫步,徘徊He ~ed home ward [wɔ:d]n. 病房a 4-bed ~ warrant [ˈwɒrənt]n.授权,批准,正当理由,根据He had no ~for his action waterproof [ˈwɔ:təpru:f]adj. 不透水的,防水的~material whisper [ˈwɪspə(r)] vt.低语,耳语,轻声说~to sb audit [ˈɔ:dɪt] v.审计;查帐;核对;旁听 第6天任务 vibrate [vaɪˈbreɪt] vt.&amp; vi.使震动,使摆动His heavy footsteps upstairs made the old house ~ weld [weld] vt.&amp; vi.焊接,煅接It takes speed to ~steel at this heat whirl [wɜ:l]vi.&amp;vt旋转,急转The couples ~ed round the dance floor audio [ˈɔ:diəʊ] n./a.音频(响)(的);声音(的),听觉(的) auditorium [ˌɔ:dɪˈtɔ:riəm] n.观众席,听众席;会堂,礼堂 stab [stæb] vt.&amp;n.刺,戳,猛的一击,突发的一阵a ~of anxiety stack [stæk]n.整齐的一叠a ~of bills stain [steɪn]vt&amp;vi玷污,污染His shirt was ~ed with blood stale [steɪl]adj. 食品等不新鲜的,走味的These old peanuts taste ~ stall [stɔ:l] n.小隔间;牛舍 startle [ˈstɑ:tl]vt.惊吓,使吃惊The horse ~s easily starve [stɑ:v] vi.&amp;vt.使挨饿,饿死Many live in luxury while others are starving addict [ˈædɪkt] v.使沉溺;使上瘾n.沉溺于不良嗜好的人 advent [ˈædvent] n.(重要事件等的)到来,来临 affluent [ˈæfluənt] a. 富裕的,富足的 bait [beɪt] n.饵,引诱物vt.用饵引诱;招惹,捉弄 balcony [ˈbælkəni] n.阳台;(电影院等的)楼厅,楼座 ballet [ˈbæleɪ] n.芭蕾舞,芭蕾舞剧;芭蕾舞团 stock [stɒk] n.原料,备料,库存,现货Rags are used as a ~for making paper stoop [stu:p] vt.&amp; vi.&amp;n. 俯身,弯腰She ~ed to pick up her fan statistics [stə’tɪstɪks] n.统计,统计资料The recent ~on marriage are interesting steer [stɪə(r)] vt.&amp; vi.掌舵,驾驶~a car through the entrance authentic [ɔ:ˈθentɪk] a.真的,真正的;可靠的,可信的,有根据的awe [ɔ:] n.敬畏,惊惧vt.使敬畏,使惊惧 bizarre [bɪˈzɑ:(r)] adj.异乎寻常的,稀奇古怪的 stubborn [ˈstʌbən]adj.顽固的,倔强的The ~boy refused to listen to his parents’advice stuff [stʌf]n.原料,材料,物品,东西He has got all the ~ready for building his new house subject [ˈsʌbdʒɪkt]n.题目,主题the ~for a debate submerge [səbˈmɜ:dʒ] vt.&amp; vi.浸没,淹没,湮没The stream overflowed and ~d the farmland submit [səbˈmɪt]vt.屈服,服从,投降~to foreign pressure anguish [ˈæŋgwɪʃ] n.(尤指心灵上的)极度痛苦,烦恼 antique [ænˈti:k] a.古式的,过时的n.有价值的古物,古董 票 Barbecue [ˈbɑ:bɪkju:] n.烤肉用的台架;烤肉野餐vt.在台架上烤肉appall [ə’pɔ:l] v.使胆寒,使惊骇(a. appalling) wrap [ræp]vt.包,裹~oneself in a blanket wreck [rek]n.失事,遇难a train ~ wrench [rentʃ]n.扳手He is a ~ . ballot [ˈbælət] n.(不记名)投票;投票总数;投票权vi.投 barren [ˈbærən] a.贫瘠的;不育的;(植物)不结果的;无用的 belly [ˈbeli] n.肚子,腹部;(像肚子一样)鼓起的部分,膛 benign [bɪˈnaɪn] a.(病)良性的,(气候)良好的,仁慈的,和蔼的 第7天任务 beverage [ˈbevərɪdʒ] n.(水,酒等之外的)饮料 bin [bɪn] n.(贮藏食物等用的)箱子 wrinkle [ˈrɪŋkl]n.皱,皱纹He will wore the ~s of anxiety on his forehead. write [raɪt]写,写字,写信,写作She ~s to me every other week. album [ˈælbəm] n. 唱片;集邮册,相册 storm [stɔ:m]n.风暴,暴风雨,暴风雪A ~arose straightforward [ˌstreɪtˈfɔ:wəd]adj.一直向前的,直接的 a ~business transaction subordinate [səˈbɔ:dɪnət]adj.下级的In the army ,captains are ~to majors substance [ˈsʌbstəns]n.物质,实质the ~of pre-school education air-conditioning [‘eərkənd’ɪʃnɪŋ] n.空调设备,空调系统 stationary [ˈsteɪʃənri]adj.固定的,静止的,非流动的~troops amiable [ˈeɪmiəbl] a.和蔼可亲的,友善的,亲切的 amid [əˈmɪd] prep.在…中间,在…之中,被…围绕 auction [ˈɔ:kʃn] n./vt.拍卖 endow [ɪnˈdaʊ] vt.资助,捐赠;(with)给予,赋予 escalate [ˈeskəleɪt] v.(使)逐步增长(或发展),(使)逐步升级 escort [ˈeskɔ:t] vt.护送(卫);陪同n.警卫,护送者;仪仗兵 evade [ɪˈveɪd] vt.逃避,回避;避开,躲避 arrogant [ˈærəgənt] a.傲慢的,自大的 artery [ˈɑ:təri] n.动脉;干线,要道 blueprint [ˈblu:prɪnt] n.蓝图,设计图,计划vt.制成蓝图,计划 ego [ˈi:gəʊ] n.自我,自负,利已主义;(心理学)自我意识 eligible [ˈelɪdʒəbl] a.有条件被选中的;(尤指婚姻等)合适(意)的 aspire [əˈspaɪə(r)] vi.(to,after)渴望,追求,有志于 assassinate [əˈsæsɪneɪt] vt.暗杀,行刺;中伤 waken [ˈweɪkən]vi&amp;vt.醒来,睡醒He wakened when the alarm sounded virtual [ˈvɜ:tʃuəl]adj.实际上的,事实上的the ~manager of the business weary [ˈwɪəri]adj.疲倦的,疲劳的a ~look vigor [‘vɪgə]n.体力,精力,活力in the ~of manhood violate [ˈvaɪəleɪt] vt.违反,违背~the traffic regulations zone [zəʊn]n.地带,分布带a danger ~ abdomen [ˈæbdəmən] n.腹,下腹(胸部到腿部的部分) abound [əˈbaʊnd] vi.大量存在;(in,with)充满,富于 acclaim [əˈkleɪm] v.向…欢呼,公认n.欢呼,喝彩,称赞 accountant [əˈkaʊntənt] n.会计人员,会计师 aisle [aɪl] n.(教堂、教室、戏院等里的)过道,通道 supplement [ˈsʌplɪmənt]n.增补,补充a ~to wages 第8天任务 Suppress [səˈpres] vt.压制,镇压~human rights Supreme [su:ˈpri:m]adj.最高的,至上的a ~ruler Surpass [səˈpɑ:s]vt.超过,优于,强于,胜过~advanced world Surplus [ˈsɜ:pləs]n.过剩,剩余a teacher ~ Survey [ˈsɜ:veɪ]vt.调查~population growth in the southern provinces asset [ˈæset] n.(pl.)资产(产);有用的资源;优点,益处blackmail [ˈblækmeɪl] n.讹诈,敲诈,勒索;胁迫,恫吓 blade [bleɪd] n. 刀刃,刀片;桨叶,桨片;草叶,叶片 blouse [blaʊz] n.女衬衣,短上衣,宽阔的罩衫 vain [veɪn]adj.徒劳的,无效的~efforts valid [ˈvælɪd]adj.有根据的a ~excuse foster [ˈfɒstə(r)] vt. 照管;培养;促进 bruise [bru:z] n.青肿,挫伤;伤痕vt.打青;挫伤 bully [ˈbʊli] n.恃强欺弱者,小流氓vt.威胁,欺侮bureaucracy [bjʊəˈrɒkrəsi] n.官僚主义,官僚机构;(非民选的)委任官员burglar [ˈbɜ:glə(r)] n.(入室行窃的)盗贼 cape [keɪp] n.海角,岬;披肩,短披风 carbohydrate [ˌkɑ:bəʊˈhaɪdreɪt] n.碳水化合物; 糖类;(plural)淀粉质或糖类 celebrity [səˈlebrəti] n.名人,名流;著名,名声,名望 Christ [kraɪst] n.基督,救世主,耶稣 clergy [ˈklɜ:dʒi] n.[总称]牧师,神职人员 coalition [ˌkəʊəˈlɪʃn] n.结合体,同盟;结合,联合 costume [ˈkɒstju:m] n. 戏装;泳装;服装 cosy/cozy [‘kəʊzɪ] / [‘kəʊzɪ] a.暖和舒服的;舒适的[反]uncomfortable credentials [krəˈdenʃlz] n.(个人能力及信用的)证明书,证件 glamour [ˈglæmə(r)] n.魔力,魅力vt.迷住 strenuous [ˈstrenjuəs] a.费力的,艰辛的;奋发的,努力的 ebb [eb] vi.衰退,减退,(指潮水)退; n.处于低潮,处于衰退状态eccentric [ɪkˈsentrɪk] a.古怪的,怪癖的,异乎寻常的n.古怪的人 3 futile [ˈfju:taɪl] a.无效的,无用的,无希望的 strife [straɪf] n.争吵;冲突,斗争;竞争 stroll [strəʊl] n.&amp;v.漫步;散步;游荡 gorgeous [ˈgɔ:dʒəs] a.华丽的;灿烂的;美丽的;宜人的;棒的graphic [ˈgræfɪk] a.生动的,形象的;绘画的,文字的,图表的 第9天任务 medieval [ˌmediˈi:vl] a. 中世纪的 excel [ɪkˈsel] vi.(at,in)突出,擅长vt.胜过,优于 excerpt [ˈeksɜ:pt] n.摘录,节录vt.摘,引用 layoff [‘leɪɔ:f] n.临时解雇;关闭;停歇 lottery [ˈlɒtəri] n.抽彩;碰运气的事,难于算计的事 majesty [ˈmædʒəsti] n. stately [ˈsteɪtli], grand appearance [grænd]; splendor [‘splendə]雄伟,壮丽,庄严majestic a. manoeuvre [məˈnu:və(r)] /maneuver [mə’nu:və] vi.熟练地进入 exile [ˈeksaɪl] n.流放;被流放者vt.流放,放逐,把…充军 fairy [ˈfeəri] adj.幻想中的,虚构的 feat [fi:t] n.功绩,伟业,技艺 federation [ˌfedəˈreɪʃn] n.联合会;结盟,联合 fore [fɔ:(r)] ad.在前面a.先前的;在前部的n.前部 solitary [ˈsɒlətri] a.(好)孤独的;偏僻的;单一的,唯一的 spiral [ˈspaɪrəl] a.螺旋形的n.螺旋,螺线v.螺旋上升;盘旋spouse [spaʊs] n. a wife or husband 配偶 stagger [ˈstægə(r)] vi.摇晃着移动;蹒跚n.摇晃不稳的动作;蹒跚stalk [stɔ:k] n.茎,梗vt.悄悄地跟踪vi.高视阔步地走 staple [ˈsteɪpl] n.主要产品;名产;纤维;主要成分,主食statute [ˈstætʃu:t] n.成文法,法令,法规;章程,规则,条例steak [steɪk] n.牛排;大块肉(或鱼)片 suffice [səˈfaɪs] v.足够,(食物)满足 suite [swi:t] n.套间;一套家具;套,组,系列 summit [ˈsʌmɪt] n.顶,最高点;颠峰,高峰;最高级会议superstition [ˌsu:pəˈstɪʃn] n.迷信,迷信的观念习俗 oxide [ˈɒksaɪd] n. 氧化物 stereotype [ˈsteriətaɪp] n.陈规,老套,模式化vt.使定型,使模式化formidable [ˈfɔ:mɪdəbl] a. difficult to defeat 失败[dɪˈfi:t] or deal with 难对付的;可怕的 meditate [ˈmedɪteɪt] v.认真思考,沉思;计划,考虑momentum [məˈmentəm] n. 动量;动力;势头 monarch [ˈmɒnək] n.帝王,君主,最高统治者 motivate [ˈməʊtɪveɪt] vt.促动;激励,鼓励,作为…的动机nightmare [ˈnaɪtmeə(r)] n.恶梦;可怕的事物,无法摆脱的恐惧optimum [ˈɒptɪməm] a.最适宜的n.(生长繁殖的)最佳条件 option [ˈɒpʃn] n.选择(权);可选择的事物(或人),选课 overturn [ˌəʊvəˈtɜ:n] n. 推翻 stun [stʌn] vt.使…失去知觉;使目瞪口呆,使吃惊 subsidy [ˈsʌbsədi] n.补助金;津贴费 grim [grɪm] a.严酷的,令人害怕的;不愉快的,讨厌的 第10天任务 grin [grɪn] vi. 露齿笑n.咧嘴笑 slaughter [ˈslɔ:tə(r)] n.屠杀,屠宰vt.屠杀,宰杀 slot [slɒt ]n.狭缝;空位vt.放入狭缝中;把…纳入 sneak [sni:k] vi.偷偷地走,溜vt.偷偷地做(或拿、吃) sneeze [sni:z] vi.打喷嚏,发喷嚏声n.喷嚏 agitate [ˈædʒɪteɪt] vi&amp;vt.搅动(液体等)The machine agitated the mixture. grope [ˈgru:vi] vi.暗中摸,摸索;探索,搜寻vt.摸索 heave [hi:v] v.(用力)举,提,拉;扔;拖;呕吐n.举起 herb [hɜ:b] n.药草,(调味用的)香草,草本植物 hierarchy [ˈhaɪərɑ:ki] n.等级制度;统治集团,领导层 hike [haɪk] n.徒步旅行;增加vi.徒步旅行vt.提高 passerby [‘pɑ:sə’baɪ] n.过路人,路人 pilgrim [ˈpɪlgrɪm] n.最初的移民(P大写,指1620由欧洲移居美国的清教徒) plea [pli:] n.(法律)抗辩;请求,恳求,托词,口实 skull [skʌl] n.头盖骨,颅骨 alarm [əˈlɑ:m] n.惊恐,忧虑The bandits scattered in alarm. sort [sɔ:t]n. 种类,类别He is not the ~of man to do such a cruel thing sound [saʊnd] n.声音,声响; adj.健康的,健全的,完好的a ~ body in a ~mind sour [ˈsaʊə(r)]adj.酸的,酸味的~pickles alert [əˈlɜ:t]adj.警惕的,留神的In our reading we should always be alert for new usages. alien [ˈeɪliən] n.外国人; 外侨;外国的adjust to an alien culture allege [əˈledʒ] 断言,宣称The newspaper alleges the mayor’s guilt. temperament [ˈtemprəmənt] n.气质,性格,性情;资质 tropic [ˈtrɒpɪk] n.回归线;[the T-s]热带地区 tumour/tumor [ˈtju:mə(r)]/ [‘tju:mə] n.(肿)瘤,肿块 amateur [ˈæmətə(r)] n.(艺术,科学等的)业余爱好者an amateur golfer allocate [ˈæləkeɪt] vt.分配,分派,把…拨给allocate funds for new projects sniff [snɪf] vi.嗅…味道;抽鼻涕;对嗤之以鼻,蔑视 soar [sɔ:(r)] vi.(指鸟等)高飞,翱翔;飞涨;高耸 sociable [ˈsəʊʃəbl] a.好交际的,友好的,合群的 solidarity [ˌsɒlɪˈdærəti] n.团结;休戚相关 surge [sɜ:dʒ] vi.(如浪潮般)汹涌;奔腾n.(感情等的)洋溢susceptible [səˈseptəbl] a.过敏的,敏感的;易动感情的,易受感动的tangle [ˈtæŋgl] n.纠缠;缠结;混乱v.(使)缠绕;变乱 ambition [æmˈbɪʃn]n.(对名利等的)强烈欲望,野心His political ambitions still burn. amend [əˈmend] vt.&amp; vi.修改,修订amend the spelling in sb.’s paper administrate [əd’mɪnɪstreɪt]v.掌管,料理…的事务In many Japanese homes, the funds are administrated by the wife. 第11天任务 adolescent [ˌædəˈlesnt]n.(尤指16岁以下的)青少年 a film aimed at adolescents advance [ədˈvɑ:ns] vt.(使)前进;使向前移动advance a chessman sector [ˈsektə(r)] n.部门,部分;防御地段,防区;扇形seemingly [ˈsi:mɪŋli] ad. in appearance; apparently 表面上的,看起来的 alternate [ɔ:lˈtɜ:nət] vt.交替,轮流Night and day alternate. arouse [əˈraʊz]vt.使……奋发 A man like Tom will be aroused. Don’t worry. array [əˈreɪ]n.排列,队形The troops were formed in battle array. articulate [ɑ:ˈtɪkjuleɪt]adj.发音清晰的,善于表达的He is articulate about everything in the field of shatter [ˈʃætə(r)] n.碎片;粉碎v.粉碎;使疲惫;使震骇 shove [ʃʌv] vt.乱推;乱塞vi.用力推,挤n.猛推 shuttle [ˈʃʌtl] n.往返汽车/列车/飞机;穿梭v.往返穿梭segregate [ˈsegrɪgeɪt] vt. separate or keep apart from others 分离,隔离 serial [ˈsɪəriəl] n.连本影片,连本电视节目a.连续的 setback [ˈsetbæk] n.挫折;失效;复发;倒退 shabby [ˈʃæbi] a.简陋的,破旧的;卑鄙的,不正当的 aggravate [ˈægrəveɪt]vt.加重,加剧,恶化aggravate the declining economy aggressive [əˈgresɪv] adj.侵犯的,侵略的an aggressive weapon allied [ˈælaɪd]adj.结盟的,有关联的allied nations allowance [əˈlaʊəns]n.津贴,补贴,零用钱 a housing allowance alter [ˈɔ:ltə(r)] vt.改变,更改alter an attitude adverse [ˈædvɜ:s]adj.不友好的,敌对的She felt adverse to her husband’s friends. advocate [ˈædvəkeɪt] vt.拥护,提倡,主张advocate self-defence n.提倡者; (辩护)律师; 支持者; aesthetic [i:sˈθetɪk]adj.美学的,艺术的,审美an aesthetic theory affiliate [əˈfɪlieɪt] vt.&amp; vi使隶属(或附属)于,使成为会员 a government affiliated company affirm [əˈfɜ:m] vt.&amp; vi断言,申明affirm the truth of a statement artificial [ˌɑ:tɪˈfɪʃl]adj.人工的,人造的,人为的artificial price controls 4 ascend [əˈsend]vi&amp;vt.登高,(渐渐)上升,升高The airplane ascended into the cloud. ascertain [ˌæsəˈteɪn]vt.查明,弄清,确定I ascertained that he was dead. ashamed [əˈʃeɪmd]adj.惭愧的,羞耻的I am ashamed for being so stupid ashore [əˈʃɔ:(r)]adv.向岸,向陆地He swam ashore. aside [əˈsaɪd] prep在旁边We stood aside to let her pass. assimilate [əˈsɪməleɪt]vi&amp;vt.吸收,消化He is quick to assimilate new ideas. associate [əˈsəʊʃieɪt]vi&amp;vt.(使)联系,(使)结合We naturally associated the name of Darwin with the doctrine of evolution. assault [əˈsɔ:lt] n.攻击,袭击We made an assault on the enemy fort. assemble [əˈsembl] vt.&amp; vi.集合,召集assemble the members of Parliament for a special meeting. assert [əˈsɜ:t] vt.肯定地说(出),坚定地断言He asserted his innocence. assess [əˈses]vt.估价,评价,评论assess the present state of the economy assign [əˈsaɪn]vt.分配,布臵(作业)The teacher assigned (us) a new lesson. beforehand [bɪˈfɔ:hænd]adv.预先,事先If you wanted soup for lunch you should have told me beforehand. bewilder [bɪˈwɪldə(r)]vt.使迷惑,使糊涂I’m bewildered as to which one to buy. 第12天任务 boom [bu:m]n.低沉有回响的声音The great bell tolled with a deep boom. bore [bɔ:(r)]vt&amp;vi.钻孔,挖(通道)bore through a wall bounce [baʊns] vi&amp;vt.弹起,反弹The ball doesn’t bounce well. sovereign [ˈsɒvrɪn] a.独立的,有主权的n.君主,国王,统治者spacious [ˈspeɪʃəs] adj.广阔的,宽敞的 spectrum [ˈspektrəm] n.谱,光谱,频谱;范围,幅度,系列 spicy [ˈspaɪsi] a.加很多香料的;(口味)浓郁的 spine [spaɪn] n.脊柱,脊椎;(动植物的)刺;书脊 bound [baʊnd]vi&amp;vt.跳跃,弹回She bounded to her feet and waved her right hand triumphantly. boycott [ˈbɔɪkɒt]vt.(联合起来)抑制,拒绝参加boycott uncooperative manufactures brace [breɪs] vt.支撑; n.支持物; 铁钳,托架,支架He braced his muscles and lifted the weight. breed [bri:d]vt.(使)繁殖Rabbits breed families rapidly. bribe [braɪb]vt&amp;n.贿赂accept(或take)a bribe from sb. anchor [ˈæŋkə(r)]锚The anchor catches (drags). Annoy [əˈnɔɪ] vt.使恼怒,使生气She was annoyed at (with) his lightheard attitude. annual [ˈænjuəl]adj.每年的,年度的the annual output of steel anonymous [əˈnɒnɪməs] adj.匿名的,无名的an anonymous letter (phone call) anticipate [ænˈtɪsɪpeɪt] vt.&amp;vi预期,预料The directors anticipated a fall in demand (that demand would fall). apart [əˈpɑ:t] adv.&amp;ad.j成零碎Such cheap clothes come apart after a few washings. Apparent [əˈpærənt] adj.显然的,明明白白的It was apparent that they all understood. brisk [brɪsk]adj. 轻快的,生气勃勃的 a brisk pace brittle [ˈbrɪtl] adj.易碎的,一碰就破的brittle glass appoint [əˈpɔɪnt] vt.任命,委派They appointed his father (to be 或as) postmaster. appraise [əˈpreɪz] vt.估量,估计appraise the infant’s weight appreciate [əˈpri:ʃieɪt] vt.感激I appreciated your help much. apprehend [ˌæprɪˈhend]vt&amp;vi对…担心Do you apprehend that there will be any difficulty? appropriate [əˈprəʊpriət]adj.恰当的,相称的Plain, simple clothes are appropriate for school wear. approve [əˈpru:v] vt.&amp; vi赞成,同意I couldn’t approve his conduct. accumulate [əˈkju:mjəleɪt] vt.&amp; vi积累,积攒,积聚accumulate wisdom accurate [ˈækjərət]adj.准确的,精确的an accurate estimate accuse [əˈkju:z] v.指控,指责They accused her publicly of stealing their books. activate [ˈæktɪveɪt] 使活动起来,使开始起作用,启动They have planted secret agents in many countries who could be activated whenever needed. bump [bʌmp] vt.&amp; vi.碰,撞(against, into) The truck bumped against the wall in the dark. burst [bɜ:st]vi&amp;vt 爆炸,爆裂The boiler burst. casual [ˈkæʒuəl]adj.偶然的,无计划的,随便的,非正式的 a casual meeting cease [si:s] vt.&amp;vi.停止,终止,结束The music ceased suddenly. certify [ˈsɜ:tɪfaɪ] vt.证明,证实The accounts were certified (as) correct. challenge [ˈtʃæləndʒ]vt.向…挑战Our school challenged the local champion team to a football match. character [ˈkærəktə(r)]n.(事物的)性质,特质The furniture in Tom’s apartment was pretentious and without character. secure [sɪˈkjʊə(r)]adj.安全的,可靠的,有把握的 a castle ~from attack sensible [ˈsensəbl]adj.明知的,合情理的a ~choice approximate [əˈprɒksɪmət] adj.大概的,大约的,近似的The approximate time is 10 o’clock. 第13天任务 apt [æpt]adj.(习性)易于……的,有……倾向的 A careless person is apt to make mistakes. Arbitrary [ˈɑ:bɪtrəri]adj. 随心所欲的,个人武断的,任意的an arbitrary choice arise [əˈraɪz] vi.&amp; vt 起立,起身arise from one’s seat blaze [bleɪz] n.火焰The fire sprang into a blaze. aggravate [ˈægrəveɪt] vt.加重,加剧,恶化aggravate the declining economy aggressive [əˈgresɪv]adj. 侵犯的,侵略的an aggressive weapon allied [ˈælaɪd]adj.结盟的,有关联的allied nations allowance [əˈlaʊəns]n. 津贴,补贴,零用钱 a housing allowance alter [ˈɔ:ltə(r)]vt&amp;vi. 改变,更改alter an attitude charge [tʃɑ:dʒ] n&amp;vi&amp;vt.要(价),收(费)The airlines charge half price for students. charm [tʃɑ:m] n.魅力,魔力This thriving resort town has retained its village charm. chase [tʃeɪs] vi&amp;vt&amp;n.追逐,追求The police chased the escaping thief and caught him at last. Check [tʃek] vt.使突然停止,制止check one’s steps cherish [ˈtʃerɪʃ]vt. 珍爱,珍视cherish one’s native land chew [tʃu:] vt.&amp; vi嚼,咀嚼He chewed a mouthful of meat. choke [tʃəʊk] vt.&amp; vi.窒息,噎住be choked by smoke bias [ˈbaɪəs] n.偏见,偏心have a bias against sb.(或sth.) blame [bleɪm]vt. 责备,责怪Public opinion blames Mrs Smith for leading the girl astray. blast [blɑ:st]n.一阵(风),一股(气流) a blast of wind alternate [ɔ:lˈtɜ:nət] vi&amp;vt.交替,轮流Night and day alternate. arouse [əˈraʊz]vt. 使……奋发 A man like Tom will be aroused. Don’t worry. array [əˈreɪ]排列,队形The troops were formed in battle array. articulate [ɑ:ˈtɪkjuleɪt] adj.发音清晰的,善于表达的He is articulate about everything in the field of economics. boast [bəʊst]n. 自吹自擂,自夸的话He is full of boasts. bolt [bəʊlt] n.螺栓,(门,窗的)插销 a small bag of nuts and bolts confuse [kənˈfju:z]vt.使混乱,混淆He confused the arrangements by arriving late. Corrupt [kəˈrʌpt]adj.堕落的,腐败的,贪赃舞弊的:lead a corrupt life Count [kaʊnt]v.数,计算: count the money 5 courtesy [ˈkɜ:təsi]n.谦恭有礼。有礼的举止(或)言辞:come in without courtesy credit [ˈkredɪt]n.信任,可信性;学分I have full credit in your ability to do the job。 shade [ʃeɪd]n.荫,荫凉处The group of trees provide some ~from the sun sham [ʃæm]n.假装,虚伪Her illness was a sham to gain sympathy shear [ʃɪə(r)]vt.&amp;vi剪,修剪~a lawn shed [ʃedvt&amp;vi&amp;n.]流~one’s blood for one’s country sheer [ʃɪə(r)]adj.完全的,十足的~nonsense shelter [ˈʃeltə(r)]n.掩蔽处,躲避处 a bus ~ connect [kəˈnekt] vt&amp;vi.连接,连接The bridge connects the island with(或to) the mainland. conscious [ˈkɒnʃəs]adj.意识到的,自觉地They were conscious that he disapproved. consent [kənˈsent]vi&amp;n. 同意,赞同,准许The people will never consent to another war. conserve [kənˈsɜ:v] vt&amp;n.保护,保藏,保存conserve natural resources consist [kənˈsɪst]vi.组成,构成(of) The house consists of six rooms. consolidate [kənˈsɒlɪdeɪt]vt&amp;vi巩固,加强consolidate one’s position conspicuous [kənˈspɪkjuəs]adj显眼的,明显的,引人瞩目的He felt as conspicuous as if he had stood on a stage. 第14天任务 Contrary [ˈkɒntrəri]adj.相反的,对抗的:take the contrary view Contrast [ˈkɒntrɑ:st]vi&amp;vt对照,对比,(对比之下显出的)差异: Contrive [kənˈtraɪv] vt.设计,想出:contrive a new method corrode [kəˈrəʊd] vt.&amp; vi. (渐渐)损害,(一点一点地)损伤:Bribery corrodes the confidence that must exist between buyer and seller dawn [dɔ:n]n. 黎明,拂晓work from dawn to dark dazzle [ˈdæzl]vt.(强光等)使目眩,耀眼:He was dazzled by his sudden success. deceive [dɪˈsi:v]v.欺骗,蒙蔽:deceive oneself decent [ˈdi:snt]adj.体面的,像样的:She didn’t have a decent dress for the ball. decline [dɪˈklaɪn] vt.&amp; vi.拒绝,谢绝:We sent him an invitation but he declined. appeal [əˈpi:l]vt.呼吁,恳求I appealed to the children to make less noise. applaud [əˈplɔ:d]vt.&amp;vi 鼓掌,喝彩,叫好The audience applauds the performers for three minutes. apply [əˈplaɪ] vt.申请;涂,敷He applied two coats of paint to the table. chorus [ˈkɔ:rəs] n.合唱队 chronic [ˈkrɒnɪk]adj.(疾病)慢性的,(人)久病的chronic indigestion clarity [ˈklærəti]n.澄清,阐明clarify an issue cling [klɪŋ] vi&amp;vt.粘着The frightened child clung to her mother. clumsy [ˈklʌmzi]adj.笨拙的 a clumsy boy mortgage [ˈmɔ:gɪdʒ]n.&amp;vt 抵押(借款)He will have to ~his land for a loan. motion [ˈməʊʃn]n.动,运动the ~of the planets motive [ˈməʊtɪv]n.动机,目的be proper in~ mount [maʊnt] vt.&amp; vi.登上,爬上~the steps cluster [ˈklʌstə(r)]n.(果实,花等的)串,束,簇; (人或物)的群,组 a cluster of tourists clutch [klʌtʃ]vt.紧抓,紧握clutch at an opportunity coherent [kəʊˈhɪərənt] adj.一致的,协调的lack coherent political goals cure [kjʊə(r)]vt.治愈,医治:this medicine cured him of his pains。cut [kʌt] vt.&amp; vi.切,割,剪,砍,削; The Mayor cut the ribbon to open a trade exhibition. decompose [ˌdi:kəmˈpəʊz] vt.&amp; vi.分解,腐烂,腐败:decompose a compound into its elements. dedicate [ˈdedɪkeɪt] vt.奉献,把。。。用在(to): The doctor dedicated himself to finding a cure. deduce [dɪˈdju:s]vt.演绎,推论:From her conversation, I deduced that she had a large family. define [dɪˈfaɪn]vt.&amp;vi解释,给。。。下定义: People define him as a genius. Deform [dɪˈfɔ:m]vt.&amp;vi (使)变形:Heat deforms plastic. constant [ˈkɒnstənt] adj.经常的,不断的 a constant noise constitute [ˈkɒnstɪtju:t] vt.组成,构成constitute a threat to sb. consult [kənˈsʌlt]vi&amp;vt.请教,与……商量consult a doctor about one’s illness consume [kənˈsju:m]vt.消费,花费: consume much of one’s time in reading contact [ˈkɒntækt]n.&amp;vt接触,联系,交往: be in contact with sb contain [kənˈteɪn]vt.包括,容纳, a list containing twelve items contaminate [kənˈtæmɪneɪt] vt.弄脏,污染,玷污,毒害: Fumes contaminate the air calculate [ˈkælkjuleɪt] vt.&amp; vi.计算,推算calculate the distance between New York and Chicago cancel [ˈkænsl]vt. 取消,废除,删去cancel a trip capture [ˈkæptʃə(r)]vt. 俘虏,捕获capture butterflies assume [əˈsju:m]假定,假设Farmers will have a bumper harvest, assuming (that) the weather si favarable. assure [əˈʃʊə(r)] vt.深信不疑地对……说,向……保证 assurance [əˈʃʊərəns] n.保证,把握,信心He gave me his assurance that he would help. astonish [əˈstɒnɪʃ] vt.使惊讶I was astonished at his sudden appearance. attach [əˈtætʃ] vt.&amp; vi.系,贴,连接attach a label to a suitcase attain [əˈteɪn] vt.&amp; vi达到,获得attain one’s goal attend [əˈtend]vi&amp;vt.出席,参加(会议等)attend a wedding attendance [əˈtendəns] n.出席,参加,出席人数,出席率Our class has perfect attendance today. attentive [əˈtentɪv]adj.注意的,专心的an attentive audience attribute [əˈtrɪbju:t] vt.把归因于,把……(过错的责任等)归于(to) He attributed his success to hard work. auxiliary [ɔ:gˈzɪliəri]adj.辅助的an auxiliary rocket 第15天任务 avail [əˈveɪl] vt.&amp; vi.有利于,有助于Will force alone avail us? average [ˈævərɪdʒ]n.平均数,平均The paper receives an average of nearly 100 articles a day. avert [əˈvɜ:t] 挡开,防止,避免Many traffic accidents can be averted by courtesy. bang [bæŋ] n. (突发的)猛击; 猛撞; 巨响; 爆炸声; sudden loud bang cast [kɑ:st]vt.投,扔,抛,撒,掷cast doubt(s) on the feasibility of a scheme contempt [kənˈtempt]n.轻视,轻蔑:show a contempt for sb contest [ˈkɒntest]n.&amp;vt&amp;vi.竞赛,比赛,竞争:the contest between the two powers for the control of the region contract [ˈkɒntrækt]n.合同,契约:enter into(make)a contact with contradict [ˌkɒntrəˈdɪkt]vt&amp;vi.反驳,同….相矛盾,同….相抵触:The facts contradict his theory defy [dɪˈfaɪ]vt.&amp;n.公然违抗,反抗:He was going ahead defying all difficulties. degenerate [dɪˈdʒenəreɪt]n.衰退,堕落,变坏(into): Her eyesight degenerated with the years. deliberate [dɪˈlɪbərət]adj.慎重的,深思熟虑的:The government is taking deliberate action to lower prices. denote [dɪˈnəʊt] vt.表示,是。。。的标志,意思是:A frown often denotes displeasure. Correspond [ˌkɒrəˈspɒnd] vi.相符合相称(to,with):Fulfillment seldom corresponds to anticipation Correspondent [ˌkɒrəˈspɒndənt]n.通信者,通讯员,记者:a good(bad)correspondent Corresponding [ˌkɒrəˈspɒndɪŋ]adj.符合的,一致的:corresponding fingerprints compact [kəmˈpækt] vt.&amp; vi. 紧密的,坚实的 a compact piece of luggage compare [kəmˈpeə(r)] vt.&amp; vi.比较,对照(with, to) compare the body to a finely tuned machine compatible [kəmˈpætəbl]adj. 能和睦相处的,合的来的You should choose a roommate more compatible to your tastes. detain [dɪˈteɪn]vt.留住,担搁:be detained by business. detect [dɪˈtekt]vt.(当场)发现(某人在干坏事):detect a student cheating deteriorate [dɪˈtɪəriəreɪt]vt.&amp;vi(使)恶化,(使)变坏,(使)退化: Relations between the two countries began to deteriorate. deviate [ˈdi:vieɪt]v.背离,偏离:deviate from the rule devote [dɪˈvəʊt]vt.将…献给(to):Mary devoted her life to caring for the sick. compile [kəmˈpaɪl]vt.汇编,编制compile an anthology of poems comply [kəmˈplaɪ]vi.遵从,顺从,服从comply with rules(safety regulations) compose [kəmˈpəʊz]vt.组成,构成England, Scotland, and Wales compose the island of Great Britain. compress [kəmˈpres]vt.压缩He compresses a lifetime as a soldier into a few sentences comprise [kəmˈpraɪz]vt.包含,包括The shipyard comprises three docks. compromise [ˈkɒmprəmaɪz] n.&amp;vi.&amp;vt.妥协,折衷compromise over the hard-fought next. confront [kənˈfrʌnt] vt.(使)面临,(使)遭遇The war confronted him with hardships. crucial [ˈkru:ʃl]adj.决定性的,关键性的:a crucial decision cue [kju:]n.提示,暗号,信号:throw cues to sb cultivate [ˈkʌltɪveɪt]vt.耕作,栽培,养殖:cultivate vegetables differentiate [ˌdɪfəˈrenʃieɪt] vt.&amp; vi.使不同,使有差别,区分,区别:differentiate varieties of plants diffuse [dɪˈfju:s]vi.&amp;vt.扩散(气体,液体等):The colors of the sunset were diffused across the sky. disgust [dɪsˈgʌst]vt.厌恶,恶心:feel disgust for bad odors disperse [dɪˈspɜ:s] vt.&amp; vi.分散,驱散,散去:Police dispersed the rioters with tear gas displace [dɪsˈpleɪs] vt.移走; 替换,移动。。。的位臵:Many of the inhabitants were displaced by the rising floodwaters dispose [dɪˈspəʊz] vt.&amp; vi.布臵,整理:The stores disposed the jewellery in an attractive display disregard [ˌdɪsrɪˈgɑ:d]vt.不理会,不顾:Disregard the noise and keep working distinct [dɪˈstɪŋkt]adj.有区别的,不同的:Three distinct types of people distort [dɪˈstɔ:t]vt.&amp;vi扭歪,扭曲:a face distorted by (或with)pain distract [dɪˈstrækt]vt.使转向:be distracted from one’s original purpose distribute [dɪˈstrɪbju:t] vt.分配,分发:distribute the prizes among the winners eliminate [ɪˈlɪmɪneɪt]vt.排除,消除:eliminate the exploitation of man by man elite [eɪˈli:t]n.出类拔萃的人(或物),精华:the elite of the society eloquent [ˈeləkwənt]adj.雄辩的,口才流利的,有说服力的:an eloquent spokesman embark [ɪmˈbɑ:k]vi.上船(或飞机,汽车等):embark for Europe at New York harborvt.vt.使…上船或飞机; embody [ɪmˈbɒdi]vt.使具体化,表现:embody principles in actions embrace [ɪmˈbreɪs] vt.&amp; vi.拥抱:She embraced her son tenderly enlighten [ɪnˈlaɪtn]vt.启发,使摆脱偏见(或迷信)Would you enlighten me on your plans for the future? enroll [ɪn’rəʊl]v.吸收(成员),招生:The school enrolled about 300 pupils last year 第16天任务 ensure [ɪn’ʃʊə(r)]vt.保证,担保:His industriousness and ability will ensure his success diminish [dɪˈmɪnɪʃ]vt.减少,降低:His illness diminished his strength. diplomat [ˈdɪpləmæt]n.外交家,外交官,圆滑的人:a career diplomat discard [dɪsˈkɑ:d]vt.丢弃,抛弃:discard an old pair of shoes discern [dɪˈsɜ:n]vt.看出,觉察出:We could discern from his appearance that he was upset. disclose [dɪsˈkləʊz]vt.揭露,透露,使公开:disclose information to sb. discharge [dɪsˈtʃɑ:dʒ] vt.&amp; vi.释放:The judge found him not guilty and discharged him. discount [ˈdɪskaʊnt]vt.把(价格,费用等)打折扣,削价出售:The store discounts 3 per cent on all bills paid when due. discriminate [dɪˈskrɪmɪneɪt] vt.&amp; vi.区别,辨别:It’s difficult to discriminate between the twin brothers. disgrace [dɪs’greɪs] n.丢脸,耻辱:bring disgrace on oneself entertain [entə’teɪn] vt.给。。。娱乐,使有兴趣:The child is entertaining himself with his building blocks entitle [ɪn’taɪt(ə)l; en-] vt.给。。。权利,给。。。资格(to):Robert was entitled to see the documents equivalent [ɪ’kwɪv(ə)l(ə)nt] adj.相等的,相同的:The misery of such a position is equivalent to its happiness erase [ɪ’reɪz] vt.擦掉,抹掉,消除:erase the chalk marks erect [ɪ’rekt] adj.竖直的,垂直的,竖起的:an erect tree erupt [ɪ’rʌpt] vt.(火山,喷泉等)喷发:The volcano is due to erupt any day essence [‘es(ə)ns] n.本质,实质,要素:Being thoughtful of others is the essence of politeness eternal [ɪˈtɜ:nl] adj.永恒的,永存的,永久的:eternal truths evaluate [ɪ’væljʊeɪt] vt.估价,评价:evaluate property evaporate [ɪ’væpəreɪt] vt.(使)蒸发,(使)挥发:Heats evaporate water exaggerate [ɪg’zædʒəreɪt; eg-] vi.夸张,夸大,对…言过其实:keep to the facts and don’t exaggerate exceed [ɪk’siːd] vt.超出,胜过:Her performance exceeded all the others external [ɪk’stɜːn(ə)l] adj.外面的,外部的:external signs of a disease descend [dɪ’send] vi.下来,下降:Darkness descended too soon. Extract [ˈekstrækt] vt.取出,提取:extract a confession Extravagant [ɪk’strævəg(ə)nt; ek-] adj.奢侈的,浪费的:be extravagant in one’s way of living fabricate [‘fæbrɪkeɪt] vt.建造,制造:The finest craftsmen fabricated this clock facilitate [fə’sɪlɪteɪt] vt.使变得(更)容易,使便利:Such a port would facilitate the passage of oil from the Middle East to Japan fade [feɪd] vi.(颜色)褪去:The color in this material will not fade faint [feɪnt] adj.不清楚的,模糊的,微弱的:a faint idea fancy [‘fænsɪ] n.想象力:Children usually have a lively fancy fare [feə] n.车费,船费,飞机票价:full (half) fare divorce [dɪ’vɔːs] n.离婚,分离,脱离:sue for a divorce dominate [‘dɒmɪneɪt] vt.分配,统治:a white dominated society donate [də(ʊ)’neɪt] vt.捐赠,赠送:They donate to the Red Cross every year. doom [duːm] n.命运,厄运,劫数:His doom was to be poverty. doze [dəʊz] vi.打瞌睡,打盹儿:The old lady dozed in her armchair. drain [dreɪn] vi.慢慢排去:drain water from a pool dread [dred] vt.怕,畏惧:I dread to think of what may happen. drift [drɪft] vi.漂流,漂泊:The sounds of music drifted up to us. dubious [‘djuːbɪəs] adj.引起怀疑的,不确定的:a dubious reply due [djuː] adj.欠款的,应支付的:A great deal of money is due to you. Dump [dʌmp] vt.倾倒,倾卸:He opened his bag and dumped its contents on the desk. Duplicate [ˈdjuːplɪkeɪt] adj.完全一样的,复制的:a duplicate key excess [ɪk’ses; ek-; ‘ekses] n.超越,超过:an excess of supply over demand execute [‘eksɪkjuːt] vt.实行,实施,执行,履行:execute a plan exemplify [ɪg’zemplɪfaɪ; eg-] vt.例示,举例证明,是…的例证(或榜样等):The teacher exemplified the use of the word exert [ɪg’zɜːt; eg-] vt.用力,尽力:exert all one’s strength 第17天任务 Exhaust [ɪg’zɔːst; eg-] vt.汲空,抽完:exhaust a well expand [ɪk’spænd; ek-] vt.扩大,扩展,膨胀:Reading(travel)expands one’s mind expel [ɪk’spel; ek-] vt.驱逐,赶走:expel an invader from a country expend [ɪk’spend; ek-] vt.花费,消费(on, in):They expended all their strength in (on) trying to climb out expire [ɪk’spaɪə; ek-] vi.满期,届期,(期限)终止:His term of office expires this year explicit [ɪk’splɪsɪt; ek-] adj.详尽的,明确的:give explicit directions exploit [ɪk’splɒɪt; ek-] n.英勇的或冒险的行为或事迹:The daring of exploit of the parachutists were much admired exterior [ɪk’stɪərɪə; ek-] adj.外部的,外面的,外表的:exterior decorations durable [‘djʊərəb(ə)l] adj.耐用的:a durable fabric dwell [dwel] vi.(dwelt 或dwelled )(尤指作为常住居民)居住: dwell on an island economical [iːkə’nɒmɪk(ə)l; ek-] adj.节约的,节俭的,经济的: an economical stove generate [‘dʒenəreɪt] vt.产生,发生(电、热、光、力、摩擦力等):generate electricity genuine [‘dʒenjʊɪn] adj.真的,非人造的,名副其实的:genuine leather gesture [‘dʒestʃə] n.手势,示意动作:raise one’s hands in a gesture of despair confine [kən’faɪn] vt.限制,使局限They succeeded in confining the fire to a small area. confirm [kən’fɜːm] vt.证实,肯定confirm a rumour conflict [‘kɒnflɪkt] n.(尤指长期的)战争,战斗The conflict between Greece and Troy lasted ten years. conform [kən’fɔːm] vt.遵照,适应confirm to(或with) the rules(the law) flourish [‘flʌrɪʃ] n.茂盛,繁荣,兴旺:Painting flourished in Italy in the fifteenth century fluctuate [‘flʌktʃʊeɪt; -tjʊ-] vt.波动,起伏:You can’t march in a straight line to the victory, you fluctuate it flush [flʌʃ] vt.脸红:Her cheeks flushed red forbid [fə’bɪd] vt.不许,禁止:Wine is absolutely forbidden to all the children forge [fɔːdʒ] n.锻炼:forge a horse-shoe out of an iron bar glance [glɑːns] n.一瞥,扫视:take a glance at the newspaper headlines glare [gleə] n.怒视,瞪眼:give the jury a glare dense [dens] adj.稠密的,密集的:a dense forest deposit [dɪ’pɒzɪt] vt.存放:She deposited her bag in the cloakroom. Depress [dɪ’pres] vt.使沮丧,使消沉:That movie depressed me. deprive [dɪ’praɪv] vt. 剥夺,使丧失:deprive sb. of his property derive [dɪ’raɪv] vt.取得,得到:He derived his enthusiasm for literature from his father. controversy [‘kɒntrəvɜːsɪ; kən’trɒvəsɪ] n.(尤之以文字形式进行的争论),辨证:a point of controversy convention [kən’venʃ(ə)n] n.(正式)会议,(定期)大会:draft a new constitution at a convention convert [kən’vɜːt] vt.转变,转化:convert coal to (into) pipeline gas convey [kən’veɪ] vt.运送,输送,传送:a wire conveys an electric current convict [kən’vɪkt] vt.(经审讯)证明…有罪,宣判…有罪:He was convicted of smuggling coordinate [kəʊ’ɔ:dɪneɪt] vt.调节,协调:coordinate the functions of government agencies cordial [‘kɔːdɪəl] adj.热情友好的,热诚的,真心的:a cordial hello(smile, invitation) glimpse [glɪm(p)s] n.一瞥,一看:have a glimpse of (into)sth glitter [‘glɪtə] vi.闪闪发亮(或发光),光彩夺目,闪耀:His language glitters with marvelous words glory [‘glɔːrɪ] n.光荣,荣誉:The soldiers hoped to win glory on the battle field glow [gləʊ] vi.发热,发光,发红:the fire glowing in the darkness gossip [‘gɒsɪp] n.流言蜚语:give rise to gossip govern [‘gʌv(ə)n] vt.统治,管理:govern a nation grace [greɪs] vt.优美:She ran well, grace in every movement graduate [‘grædʒʊət; -djʊət] vt. (使)大学毕业:graduate from college grant [grɑːnt] vt.同意,准予:grant a request grease [griːs]n.油脂,动物脂:the grease spot on the envelope heal [hiːl]vt.&amp;vi.治愈,愈合:heal the sick heap [hiːp] n.(一)堆:a big heap of books hedge [hedʒ] n.(矮树)树篱:a quickset hedge 第18天任务 highlight [‘haɪlaɪt]vt.使显著,使突出:Growing economic problems were highlighted by a slowdown in oil output fracture [‘fræktʃə] vt.&amp; vi.&amp;n.裂缝,裂痕: a fracture in the pipe frequent [‘friːkw(ə)nt]adj.时常发生的,频繁的:His trips to France are less frequent now frown [fraʊn] vt.&amp; vi.皱眉:frown at sb frustrate [frʌ’streɪt; ‘frʌs-]vt.挫败,阻挠,使灰心,使无效:frustrate a plan fulfill [ful’fil] (fulfilled, fulfilling)vt.履行,实现,完成: fulfill a contract furnish [‘fɜːnɪʃ]vt.供应,提供:furnish blankets for the refugees fury [‘fjʊərɪ]n.狂怒,暴怒:fly into a fury fuss [fʌs] vi.大惊小怪; 忙乱; 抱怨; vt.使烦恼,使烦忧; 使急躁; n.大惊小怪,乱忙,小题大做:make much fuss over losing a penny hinder [‘hɪndə] vt.&amp; vi妨碍,阻止:Bad weather hindered travel hint [hɪnt]n.暗示:drop(give ,throw out)a hint enforce [ɪn’fɔːs; en-]vt.实施,使生效:The principal enforced the rules of the school engage [ɪn’geɪdʒ; en-] vt.(用契约诺言等)约束,使订婚:He engaged himself as an apprentice to a printer enhance [ɪn’hɑːns; -hæns; en-]vt.提高(价格,质量,吸引力等),增加,增强:enhance one’s reputation figure [‘fɪgə]n.数字:according to official figures finance [faɪ’næns; fɪ-; ‘faɪnæns]n.财政,金融:skill in finance finite [‘faɪnaɪt]adj.有限的,有限制的:a finite number of facts flare [fleə] vt.&amp;vi(火焰)闪耀,(摇曳不定地)燃烧:candle flaring in the wind flash [flæʃ] vt.&amp; vi.闪光,闪烁:a flash of lighting flat [flæt]adj.平坦的,扁平的:a flat land inaccessible [ɪnək’sesɪb(ə)l]adj.达不到的:The two rocks at the top of the steep hill is at first sight inaccessible incline [ɪn’klaɪn] vt.&amp; vi.倾斜:The shaft inclines almost thirty degree incorporate [ɪn’kɔːpəreɪt]vt.&amp;vi.包含,吸收:His book incorporates his earlier essays incur [ɪn’kɜː]vt.招致,引起,遭受:incur sb’s displeasure(envy) modify [‘mɒdɪfaɪ]vi.&amp;vt.修改,更改~a law momentary [‘məʊm(ə)nt(ə)rɪ]adj.片刻的,瞬息的a ~feeling of fear designate [‘dezɪgneɪt]vt.指明,表明:designate boundaries desolate [‘des(ə)lət]adj.荒芜的,不毛的:desolate land despise [dɪ’spaɪz]vt.蔑视,鄙视:He despises flattery. detach [dɪ’tætʃ]vt.拆卸:They detached their trailer and set up camp. Diagnose [‘daɪəgnəʊz; -‘nəʊz]vt&amp;vi.诊断,判断:The doctor diagnosed the illness as influenza. dictate [dɪk’teɪt]vt.&amp;vi口授,使听写:dictate a letter to a secretary coincide [,kəʊɪn’saɪd]vi相符,相一致100 Centigrade coincides with 212 Fahrenheit. collaborate [kə’læbəreɪt]vi(尤指在文艺、科学等方面)合作,协作He and I collaborated in writing plays. collapse [kə’læps]vi&amp;vt.倒坍,崩溃,瓦解The roof collapsed under the weight of the snow. collide [kə’laɪd]vi碰撞,互撞The car collided with the truck. commemorate [kə’meməreɪt]vt.纪念,庆祝commemorate a holiday commend [kə’mend]vt.表扬,称赞commend a soldier for bravery extinct [ɪk’stɪŋkt; ek-] adj. (火等)熄灭了的:an extinct cigarette(fire) extinguish [ɪk’stɪŋgwɪʃ; ek-]vt.熄灭,扑灭:extinguish a cigarette(candle) multiply [‘mʌltɪplaɪ] vt.&amp; vi.乘,相乘~7 and 8 muscle [‘mʌs(ə)l]n.肌肉 facial ~s mutual [‘mjuːtʃʊəl; -tjʊəl]adj.相互的,彼此的~aid narrate [nə’reɪt]vt&amp;vi.叙述,讲述~the story of sb’s life negligent [‘neɡlɪdʒənt] adj.疏忽的,大意的be ~about traffic regulations indignant [ɪn’dɪgnənt]adj.愤怒的,愤慨的:The actress was at the columnist’s personal questions indispensable [ɪndɪ’spensəb(ə)l]adj.必不可少的,必需的(to. for):Oxygen is indispensable to life induce [ɪn’djuːs]vt.引诱,劝使:Nothing will induce me to do that infer [ɪn’fɜː] vt.&amp;vi(根据已知事实等)推断,推定(from):From his grades I inferred that he was a good student ingenious [ɪn’dʒiːnɪəs] adj.(人,头脑等)灵巧的,足智多谋的:an ingenious designer instant [‘ɪnst(ə)nt]adj.紧急的,急迫的,立即的,即刻的:be in instant need of help institute [‘ɪnstɪtjuːt]n.学会,协会,学院,研究院:an institute for the blind insulate [‘ɪnsjʊleɪt]vt.隔音,使隔绝(以免受到影响):We are not insulated from the real world the way we were insult [ɪn’sʌlt]vt.&amp;n.侮辱,辱骂:She insulted him by calling him a coward flatter [‘flætə]vt.奉承,使高兴:I feel greatly flattered by your invitation 第19天任务 Flee [fliː]vi&amp;vt.逃走,逃掉:The enemy troops fled in utter confusion Flexible [‘fleksɪb(ə)l] adj.灵活的; 柔韧的; 易弯曲的:flexible cord fling [flɪŋ] vt.&amp; vi. (用力)扔,掷,抛,丢:fling sb into prison hoist [hɒɪst]vt&amp;vi.举起,升起,吊起:The war hoisted prices hold [həʊld]vt.拿着,抓住:The girl is holding the baby in her arms homogeneous [ˌhɒməˈdʒi:niəs] adj.同种类的,同性质的,有相同特征的: homogeneous parts horizon [hə’raɪz(ə)n]地平(线):The sun has sunk below the horizon hospitable [hɒˈspɪtəbl] adj好客的,殷勤的,热情友好的:be hospitable to the homeless hostile [‘hɒstaɪl]adj 敌人的,敌方的:a hostile army withstand [wɪð’stænd]vi 经受,承受~hardships speculate [‘spekjʊleɪt] vi推测,推断I cannot ~on their motives for doing this spill [spɪl] vt溢出,溅出The ship ~ed oil while in port spin vt纺制~cotton into yarn spit [spɪt] vt吐出He was ~ting out the husks of sunflower seeds on the floor splash [splæʃ] vt 溅泼A passing car ~ed my dress split [splɪt] vt 劈开,切开,裂开~the firewood with an axe spoil [spɒɪl] vt损坏,毁掉It would ~my reputation sponsor [‘spɒnsə] n发起者,主办者,保证人,赞助者the ~of the annual Christmas parade spot[spɒt] n斑点,污点 a blue tie with ~s spray [spreɪ] n浪花,水花The new shower heads reduce ~and waste less water sprout [sprəʊt]vi 抽条,发芽,长出The seeds we planted finally ~ed spur [spɜː] n靴刺,踢马刺a pair of ~s renovate [‘renəveɪt] vt修复,整修~a two storey building repel [rɪ’pel] vt击退,抵制,拒绝,排斥~the enemy repent [rɪ’pent] 后悔,懊悔~of a thoughtless act replace [rɪ’pleɪs] vt 代替,取代,更换The calculator is rapidly replacing the abacus transmit [trænz’mɪt; trɑːnz-; -ns-] vt播送,发射~a match live transparent [træn’spær(ə)nt; trɑːn-; -‘speə-] adj透明的Glass is ~ transplant [træns’plɑːnt; trɑːns-; -nz-] vt移植,移种,移居~a human heart into a patient treasure [‘treʒə] n财宝,财富dig for buried ~ trend [trend] n倾向,趋势an upward ~of prices trick [trɪk] n诡计,骗局He got the money by a ~ trifle [‘traɪf(ə)l] n琐事,小事One should not get angry about such a ~ trim [trɪm] adj整齐的,整洁的~lawns triple [‘trɪp(ə)l] adj三倍的,三重的,三部分的,三方的Black unemployment rate runs ~the figure for whites triumph [‘traɪʌmf] n 胜利a ~over the enemy trivial [‘trɪvɪəl] adj琐碎的,不重要的,无价值的~matters tuck [tʌk] vt 夹入,藏入He ~ed the letter in a book so he wouldn’t lose it tug [tʌg] vt(用力)拖或拉He tugged the door but it wouldn’t open stroke [strəʊk] vt敲,打,鸣drive in a nail with one ~of the hammer tend [tend] vi趋向,倾向Interest rates are ~ing upwards tender [‘tendə] adj嫩的a ~piece of meat represent [reprɪ’zent] vt 代表,象征To many local people these castles ~a hundred years of foreign domination reproach [rɪ’prəʊtʃ] n责备,指责,斥责mute ~ scratch [skrætʃ] vt抓,刮,擦The cat ~ed me with its claws shield [ʃiːld] n 盾,保护物,防御物They piled more sand on top as a ~ snob [snɒb] n势力的人,谄上欺下的人 a ~of the first water soak [səʊk] vi 湿透,浸湿,浸泡You shouldn’t have gone out in the rain -your clothes are ~ing sobe [‘səʊbə] adj未醉的A driver must stay ~ sophisticated [sə’fɪstɪkeɪtɪd]老于世故的a young man quite ~for his age sore [sɔː] adj 痛的,痛苦的,痛心的touch sb on a ~place sow [səʊ] vt播种This plot of land was sown with maize 第20天任务 spark [spɑːk] n火花,火星The firework burst into a shower of ~s sparkle [‘spɑːk(ə)l] n发光,闪耀Her eyes were sparkling with anger tentative [‘tentətɪv] adj 试验性的,试探性的,暂时的,推测的a ~plan terminate [‘tɜːmɪneɪt] vt停止,结束,使终止~a contract terrify [‘terɪfaɪ] vt使害怕,使惊吓She was terrified out of her wits testify [‘testɪfaɪ] vt 作证,证明He agreed to ~on behalf of the accused man humble [‘hʌmbl] adj 地位(或身份)低下的,卑贱的:a humble job hysterical [hɪ’sterɪk(ə)l] adj情绪异常激动的,歇斯底里的an hysterical outburst of fury identical [aɪ’dentɪk(ə)l] adj同一的:the same (very)identical menu identify [aɪ’dentɪfaɪ] vt识别,鉴别:He identified the coat as his brother’s idle [‘aɪd(ə)l] adj闲散的,闲臵的:idle money ignite[ɪg’naɪt] vt点燃,使燃烧:ignite a stove fire with scraps of paper impose [ɪm’pəʊz]vi 征(税),加(负担,惩罚等)于(on, upon):impose additional tax on business expansion impulse [‘ɪmpʌls] vt推动,驱动:give an impulse to the development of the friendly relations between the two countries intact [ɪn’tækt] adj完整无缺的,未经触动的,未受损伤的:an intact family integrate[‘ɪntɪgreɪt] vt使成一体,使合并(with, into):integrate traditional Chinese medicine with western medicine interfere [ɪntə’fɪə] vi 妨碍,冲突(with):Misty weather interfered with the contact intermittent [ɪntə’mɪt(ə)nt] adj 间歇的,断断续续的,周期性的:suffer from intermittent headaches hazard [‘hæzəd] n危险,危害物,危险之源:a life full of hazards label [‘leɪb(ə)l]n 标签,标记,标号:put labels on your luggage lag [læg] n 走得慢,落后,滞留:Some of the runners began to lag latent [‘leɪt(ə)nt] adj潜在的,隐伏的,不易觉察的:a latent infection leak [liːk] n漏洞,裂缝:The pipe has got a leak lean [liːn] vi 斜,倾斜the Leaning tower of Pisa initiate [ɪ’nɪʃɪeɪt]vt 开始,发起:initiate a conversation innovate [‘ɪnəveɪt] vi革新,改革,创新(in,on/upon):innovate on another’s creation inquire [ɪn’kwaɪə] vt 询问,打听:He inquired what the weather was likely to be insert [ɪn’sɜːt] vt插入,嵌入:In the pause he managed to inquire a question inspire [ɪn’spaɪə] vt鼓舞,激起:His speech inspires the crowd tumble [‘tʌmb(ə)l] vi 跌到,滚下The baby is just learning to walk and he’s always tumbling over tune [tjuːn] n曲调,曲子She wrote the words of the song and her brother wrote the ~ turbulent [‘tɜːbjʊl(ə)nt] adj骚乱的,动荡的,混乱的,狂暴的a ~crowd turn [tɜːn] vt 转动,旋转Turn the hands of the clock until they point to 9 O’clock tutor [‘tjuːtə] n家庭教师,指导教师When the students are making their choices, their ~may well guide them twinkle [‘twɪŋk(ə)l] vt 闪烁,闪耀,闪亮The diamond on her finger ~d in the firelight twist [twɪst] vt 捻,搓,绞,拧~a rope out of threads whistle [‘wɪs(ə)l] n 吹口哨,鸣笛The train ~d whitewash [‘waɪtwɒʃ] n粉饰 wholesome [‘həʊls(ə)m] adj有益于健康的~surroundings wicked [‘wɪkɪd] adj坏的,邪恶的,淘气的,恶作剧的~habits tease [tiːz]vt戏弄,取笑The boy is teasing the cat tedious [‘tiːdɪəs] adj单调乏味的,冗长的,罗嗦的a ~debate liable [‘laɪəb(ə)l] adj会……的,有……倾向的We are all liable to make mistakes liberal [‘lɪb(ə)r(ə)l] adj 慷慨的,大方的a liberal donation limp [lɪmp] adj软的,松沓的a young man in a limp gray sweater linger [‘lɪŋgə] vi逗留,徘徊The children lingered at the zoo until closing time lubricate [‘luːbrɪkeɪt] vt使润滑lubricate the skin with cold cream lump[lʌmp] n块,小方块a lump of coal 第21天任务 macroscopic [,mækrə(ʊ)’skɒpɪk] adj肉眼可见的 magnet [‘mægnət] n 磁铁,磁铁石The pin was extracted with a magnet magnify [‘mægnifaiiŋ] v 放大,扩大I want to magnify this picture maintain [meɪn’teɪn; mən’teɪn] vt维持,保持maintain law and order malfunction [mæl’fʌŋ(k)ʃ(ə)n] vi 发生功能障碍,发生障碍If the computer malfunctions it fails to work properly maltreat [mæl’tritmənt] n 粗暴地对待,虐待We’ll not intervene unless the children are being physically maltreated manifest [‘mænɪfest] adj显然的,明了的manifest superiority nominal [‘nɒmɪn(ə)l] adj 名义的,有名无实的a ~leader nominate [‘nɒmɪneɪt]vt 提名a ~leader noteworthy[‘nəʊtwɜːðɪ] adj值得注意的,显著的a ~day intervene [ɪntə’viːn] vi插入,介入:A week intervenes between Christmas and New Year’s Day intricate [‘ɪntrɪkət]adj 错综复杂的,复杂精细的:A detective story usually has an intricate plot invert [ɪn’vɜːt] vt使反向,使倒转,使颠倒:The magician inverted the bag to show it was empty irrespective [ɪrɪ’spektɪv] adj 不考虑的,不顾的(of) :irrespective of consequences irritate [‘ɪrɪteɪt] vt 激怒,使恼怒,使烦躁:be irritated by sb’s insolence justify [‘dʒʌstɪfaɪ] vt证明。。。正当或有理,正确:Such behavior is justified on these grounds knit [nɪt] vt 编织,编结:She’s knitting her husband a sweater greet [griːt] vt 问候,招呼:greet sb with a nod grieve [griːv] vt 使悲伤,使伤心:The little girl grieved over her kitten’s death grind [graɪnd] vt (ground)磨,磨碎,碾碎:grind wheat into flour grip [grɪp] vt抓紧,紧握:Fear gripped his heart gross [grəʊs] adj 总的,毛的:gross income guarantee [gær(ə)n’tiː] n保证,保证书,保单:Wealth is no guarantee of happiness hail [heɪl] vt 欢呼,喝彩:The crowd hailed the actress with joy halt [hɔːlt] vt (行进中的)暂停前进,中止,终止:The car came to a sudden halt hamper [‘hæmpə] vt 妨碍,阻碍:hamper sb from getting elected handicap [‘hændɪkæp]障碍,不利条件:Poor eyesight is a handicap to a student haste [heɪst] n急速,急忙,仓促,草率:Why all this haste? hatch [hætʃ] vt 孵,孵出:hatch (out) chickens manipulate [mə’nɪpjʊleɪt]vt (熟练地)操作Mr. Smith manipulated the ivory chopsticks with great dexterity mature [mə’tʃʊə] adj 熟的,成熟的a mature fruit melt [melt] vt 使融化,使熔化The sun melted the snow mend [mend] vt 修理,修补mend clothes merit [‘merɪt] n 长处,优点,价值,功绩merits and demerits migrate [maɪ’greɪt; ‘maɪgreɪt] vi 移居(尤指移居国外),迁移migrate to another country minimum [‘mɪnɪməm] n最低限度,最少量,最低点 mislead [mɪs’liːd] vt 把……引错方向,把……引入歧途 moan [məʊn] n呻吟声,呜咽声Each time she moved her leg she let out a ~ . mobilize [‘məʊbəlaɪz] vt 动员Our country’s in great danger, we must ~ the army. Mock [mɒk] vt 嘲笑,嘲弄He had ~ed her modest ambitions. Ornament [‘ɔːnəm(ə)nt]n 装饰品,点缀品We bought ~s for the Christmas tree orthodox [‘ɔːθədɒks] adj正统的,正宗的an ~economic theory outline轮廓,外形He saw the ~of a house against the sky overflow [əʊvə’fləʊ] n泛滥Every spring the river ~s overhaul [əʊvə’hɔːl]vt 大修,全面检查,彻底革新~a faulty machine overhear [əʊvə’hɪə]vt 偶然听到,偷听到I wouldn’t like to be ~d occupy [‘ɒkjʊpaɪ] vt占用,占有My books ~a lot of space odd的奇数的,单的,奇怪的One ,three five are ~numbers wicked [‘wɪkɪdli] adj坏的,邪恶的,淘气的,恶作剧的~habits wind [wɪnd]vi 绕,缠The snake wound around a branch 第22天任务 Wire [waɪə] n金属丝,金属线steel ~ withdraw [wɪð’drɔː]抽回,收回,提取She withdrew her eyes from the terrible sight withhold [wɪð’həʊld]vt 使停止,阻挡be obliged to ~one’s pen withstand [wɪð’stænd]vt 经受,承受~hardships witness [‘wɪtnɪs] n目击者,见证人a ~of the accident offend [ə’fend]vt 冒犯,使生气She was afraid of ~ing anyone. offence [ə’fens] n 犯法行为,过错They were arrested for drug ~s. offset [‘ɒfset] vt补偿,抵消They gains ~the losses. Omit [ə(ʊ)’mɪt] vt 省略,删去Minor points may be omitted. overlap [əʊvə’læp] n 重叠,部分相同The Renaissance overlapped the later Middle Ages overtake [əʊvə’teɪk] vt赶上,超过He ran out of the door to ~her overthrow [əʊvə’θrəʊ] vt推翻,打倒a plot to ~the government overwhelm [əʊvə’welm]vt 征服,压倒,毁坏He took 45% of the ballots and ~ ed his two main rivals pace [peɪs] n 步,步速,步伐He took three ~s forward pack [pæk]n 包,包裹The climber carried some food in a ~on his back pad [pæd] n垫,衬垫a seat ~ pant [pænt]vi 气喘,气喘吁吁地说话He ~ed along beside the bicycle parallel [‘pærəlel] adj 平行的The road is ~to the river patent [‘pæt(ə)nt; ‘peɪt(ə)nt]n专利,专利权,专利品take out a ~for an invention pattern [‘pæt(ə)n]n 模范,榜样set the ~for periodical [pɪərɪ’ɒdɪk(ə)l] adj期刊,杂志a ~room perish [‘perɪʃ]vi 卒,丧生~from disease perpetual [pə’petʃʊəl; -tjʊəl]adj 永久的,永恒的,长期的a ~arms race pool 水池,游泳池a swimming ~ notorious [nə(ʊ)’tɔːrɪəs] adj臭名昭著的a ~thief notwithstanding [nɒtwɪð’stændɪŋ; -wɪθ-]adv 虽然,尽管They traveled on,~the storm nourish[‘nʌrɪʃ] vt养育Milk is all we need to ~our small baby numb [nʌm] adj麻木的,失去感觉的fingers ~with cold object [‘ɒbdʒɪkt; -dʒekt] n 物体,事物cultural ~s objective [əb’dʒektɪv] n目的,宗旨political ~s oblige [ə’blaɪdʒ] vt 强迫,迫使In certain countries the law ~s parents to send their children to school obscure [əb’skjʊə] adj 晦涩的,费解的an ~text nasty [‘nɑːstɪ] adj龌龊的,肮脏的a~room necessitate [nɪ’sesɪteɪt] vt 使成为必须,需要The emergency ~d a change in plans. necessity [nɪ’sesɪtɪ] 必要(性),(迫切)需要There is no ~for disappointment. neglect [nɪ’glekt] vt忽视,忽略~on e’s health obsolete [‘ɒbsəliːt] adj 废弃的,淘汰的Wooden warships are ~. obstinate [‘ɒbstɪnət] adj 顽固的,固执的,倔强的The girl would go her own way, in spite of all warnings occasion [ə’keɪʒ(ə)n] n时刻,场合They met on three ~s opaque [ə(ʊ)’peɪk] adj 不透明的A brick wall is ~. optimistic [ɒptɪ’mɪstɪk] adj 乐观的,乐观主义的 option [‘ɒpʃ(ə)n] n选择权It is your ~to take it or leave it orient [‘ɔːrɪənt; ‘ɒr-] n东方,东方国家political theory in the West and in the Orient origin [‘ɒrɪdʒɪn] n 起源,由来a word of Latin ~ portable [‘pɔːtəb(ə)l] adj 便于携带的,手提式的,轻便的This little TV is extremely ~ pose 姿势,样子The model was asked to adopt various ~s for the photographer postpone [pəʊs(t)’pəʊn; pə’spəʊn] vt 延迟,延缓,延期We are postponing our trip until the weather grows warmer potential [pəˈtenʃl] adj 潜在的,可能的That hole in the road is a ~danger 第23天任务 rally [‘rælɪ] vt集合,团结~public opinion in favor of price controls paralyze [‘pærə,laɪz] vt 使瘫痪,使麻痹A stroke ~d half his face partial [‘pɑːʃ(ə)l] adj 偏向一方的,偏袒的,偏爱的A parent should not be ~ to any one of his children participate [pɑː’tɪsɪpeɪt] vt 参与,参加,In a modern democracy people want to ~more fully particular pə’tɪkjʊlə] adj特殊的,特别的a matter of ~importance pat [pæt] vt (用掌或扁平物)轻拍,轻打She patted a place next to her for me to sit down patch [pætʃ] n 补钉,补片patches at elbows of a jacket hip [hɪp] n.臀部,髋;屋脊 pave [peɪv] vt铺路,铺砌,铺筑~a road with concrete hum [hʌm] v.哼曲子;发嗡嗡声;忙碌n.嗡嗡声,嘈杂声 profound [prə’faʊnd] adj很深的a ~sleep prohibit [prə(ʊ)’hɪbɪt] vt禁止Tourist class passengers are ~ed from using the first class lounge prolong [prə’lɒŋ] vt延长,拉长,拖延a means of ~ing life prominent [‘prɒmɪnənt] adj突起的,凸出的a ~forehead prompt [prɒm(p)t]adj 敏捷的,迅速的He is ~on the job pour [pɔː] vt倒,注,灌,倾泻,涌流He began to ~with sweat preach [priːtʃ] vi讲道,布道~to a packed church preside [prɪ’zaɪd] vt 主持,主管(at, over) The vice chairman of the board will ~at today’s meeting press [pres] vt按,挤,压~the bell presume [prɪ’zjuːm] vt推测,假定,假设From the way they talked, I ~d them to be married prevail [prɪ’veɪl] vi流行,盛行This custom still ~s among members of the older generation prey [preɪ] n被捕食的动物,捕获物The lion seized its ~and ate it prick [prɪk] n 一刺,一扎The boy burst the balloon with a ~of the needle proceed [prə’siːd] vi(尤指停顿或打断后)继续进行The train ~ed at the same speed as before proclaim [prə’kleɪm] vt宣告,宣布,声明~war propose [prə’pəʊz] vt提议,建议When she was very ill ,her lawyer has ~d to her that she might like to make a will prosper [‘prɒspə] vi 兴旺,繁荣,昌盛,成功His farm ~ed through good management provoke [prə’vəʊk] 挑动,激起,导致He tried to ~them into fighting radiate [‘reɪdɪeɪt] (光,热等)辐射,发射The sun ~s light and heat radical [‘rædɪk(ə)l] 根本的,基本的a ~error humiliate [hjʊ’mɪlɪeɪt] v.使羞辱,使丢脸[同]disgrace lapse [læps] n.失误;(时间的)流逝vi.逐渐下降;终止 laptop [‘læptɒp] n.小型计算机 perplex [pə’pleks] vt 使困惑,使茫然The question ~ed him persecute [‘pɜːsɪkjuːt] vt 虐待,残害,迫害~animals perspective [pə’spektɪv] n透视画法,透视图a lesson in drawing class on ~ pessimistic [,pesɪ’mɪstɪk] adj 悲观的I am ~about my chances of getting the job petition [pɪ’tɪʃ(ə)n] n 请愿,请愿书get up a ~ pierce pɪəs] vt 刺入,刺穿,穿透The thorn ~d his heel pilot [‘paɪlət] n 飞行员,驾驶员 pin [pɪn] n 大头针,别针 pinch [pɪn(t)ʃ] vt 捏,拧,夹I ~ed myself to make sure I wasn’t dreaming plausible [‘plɔːzɪb(ə)l] adj 论点等貌似有理的a ~excuse plead [pliːd] vt恳求,请求His parents ~ed that he should be given one more chance pledge [pledʒ] n 保证,誓言,誓约fulfill one’s ~ plot [plɒt] n 小块土地a vegetable ~ plug [plʌg] n塞子,栓He wears rubber ~s in his ears when he swims 第24天任务 plunge [plʌn(d)ʒ] vt猛地投入或插入He ran to the edge of the swimming pool and ~d in pocket [‘pɒkɪt] n 衣袋,口袋,袋子a change ~ polish [‘pɒlɪʃ] vt 擦光,擦亮Polish your shoes with a brush poll [pəʊl] n 民意测验A recent ~shows a change in public opinion layman [‘leɪmən] n.俗人,外行,门外汉 rank [ræŋk] n排,行,列the ~s of great pines rate [reɪt] n比率,率Velocity is the ~of change of distance with respect to time reap [riːp] vt 收割,收获~the rice in the field rear [rɪə] n 后部,后边,背部a garden at the ~of the house recall [rɪ’kɔːl]vt回忆,回想I ~seeing a poster on his wall reciprocal [rɪ’sɪprək(ə)l]adj 相互的,交互的~respect reckon [‘rek(ə)n] 计算,数~the days till Christmas hurl [hɜːl] vt.猛投,力掷;大声叫骂 hurricane [‘hʌrɪk(ə)n; -keɪn] n.飓风;十二级风 hypocrisy [hɪ’pɒkrɪsɪ] n.伪善,虚伪 ideology [,aɪdɪ’ɒlədʒɪ; ɪd-] n.意识形态,(政治或社会的)思想意识infrastructure [‘ɪnfrəstrʌktʃə] n.基础结构,基础设施inhale [ɪn’heɪl] v.吸入(气体等),吸(烟) juvenile [‘dʒuːvənaɪl] n.少年a.青少年的,适合青少年的;幼稚的 kin [kɪn] n.家族,亲属a.亲属关系的 reclaim [rɪ’kleɪm] vt 使改过,改造~former criminals recognize [‘rɛkəg’naɪz] vt认出,识别Dogs ~people by their smell recollect [,rekə’lekt] v 回忆,想起I am trying to ~a forgotten address recommend [rekə’mend] vt 推荐,称赞He ~ed me for the post of headmaster reconcile [‘rek(ə)nsaɪl] vt 使和解,使和好After each fight they ate soon ~d recover [rɪ’kʌvə] vt 追回,重新得到The police ~ed stolen watch recruit [rɪ’kruːt] vt 招募(新兵)招收(新成员)try to ~300 men in two days rectify[‘rektɪfaɪ] vt纠正,改正,矫正~the mistakes in a bill refer [rɪ’fɜː] vt提到,谈到Does the remark ~to me refine [rɪ’faɪn] vt提炼~crude oil into various petroleum products pump [pʌmp] n泵fill a bucket from the ~ pursue [pə’sjuː] vt追赶,追随,追求He felt their eyes pursuing him puzzle [‘pʌz(ə)l] n难题,智力玩具,迷,迷惑jigsaw ~ quality [‘kwɒlɪtɪ] n 质量,品级merchandise of ~ quantity [‘kwɒntɪtɪ] n 数量,量an increase in ~ quench [kwen(t)ʃ] vt 消除,平息,压制,扑灭~hatred queue[kjuː] n一队人,一行列车from a ~ peculiar [pɪ’kjuːlɪə] adj 奇怪的,古怪的He has always been a little ~ peel [piːl] v 削皮,剥皮peel potatoes penetrate [‘penɪtreɪt] vt 穿透,渗入The nail ~d the wood at least two inches perceive [pə’siːv] vt 感知,感觉,觉察He was only able to ~light and color ;he could not see properly perfect 完美的,理想的a ~couple perform [pə’fɔːm] vt 做,执行,完成~an experiment 第25天任务 Perfume [‘pɜːfjuːm] n 香水,香料,香气She loves French ~(s) hop [hɒp] v.人单足跳;跳上(车等)n.蹦跳;短程飞行 hound [haʊnd] n.猎狗;卑鄙的人vt.用猎狗追,追逐 housing n.住房,住房供给;(外、阀)壳,防护罩 hover [‘hɒvə] vi. 盘旋;徘徊 howl [haʊl] n.怒吼,嗥叫vi.(风等)怒吼,咆哮 huddle [‘hʌd(ə)l] vi.挤作一团;蜷缩vt.聚集n.挤在一起的人 quiver [‘kwɪvə] vi 颤抖,抖动~with cold quote [kwəʊt] vt引用,引述,援引She ~d several verses to us rack [ræk] n 架子,支架,搁架a clothes ~ reform [rɪ’fɔːm] vt 改革,改良He spend years trying to ~the world refresh [rɪ’freʃ] vt 使恢复活力,使振作精神Sleep ~es the body refute [rɪ’fjuːt] vt驳斥,驳倒The argument cannot be d at the moments regenerate [rɪ’dʒenəreɪt] vt使恢复,使复兴,再生The human body can ~ hair, nails, etc regulate[‘regjʊleɪt] vt管理,控制,为……制订规章The policeman ~d traffic at the intersection reign [reɪn] vt 为王,统治In England ,the sovereign ~s but does not rule reinforce [riːɪn’fɔːs] vt 增援,加强They hope to ~their domination in these countries rejoice [rɪ’dʒɒɪs] vt 使感到高兴,使充满喜悦The whole nation ~over the victory relate [rɪ’leɪt] vt 讲述,叙述She ~d the experience of a Chinese girl in the United States relay [‘riːleɪ] vt接力传送,传递~a message release [rɪ’liːs] vt 排放,释出~a bomb from an aircraft relevant [‘relɪv(ə)nt] adj 有关的,切题的I don’t think his remarks are ~to our discussions relieve [rɪ’liːv] vt缓解,减轻,解除He smoked frequently to ~nervous tension rescue [‘reskjuː] vt 营救,救援,搭救~hostages held by the terrorists resent [rɪ’zent] vt 怨恨~those unfair comments reserve [rɪ’zɜːv] vt 保留,储备We’ll the ticket for you till tomorrow noon resign [rɪ’zaɪn] vt辞职,引退He ~ed from his post resolve [rɪ’zɒlv] vt 决定,打定注意要Have you ~d where to go next? Resort [rɪ’zɔːt] vi 求助,凭借,诉诸~to sb for help reverse [rɪ’vɜːs] adj 相反的,颠倒的,倒退的a ~dictionary revise [rɪ’vaɪz] vi 修订,校订~a dictionary revive ɪ’vaɪv] vi 苏醒,复苏He is dead and never will ~ ridiculous [rɪ’dɪkjʊləs] adj 可笑的,荒唐的,荒谬的In this department of English girls outnumber boys in a ~ratio rigid [‘rɪdʒɪd] adj刚性的,坚硬的a ~piece of plastic ripe [raɪp] adj 成熟的,时机成熟的a ~cluster of grapes rot [rɒt]n 腐烂,腐坏,腐朽,腐败,腐化Meat may ~if it is not refrigerated roundabout [‘raʊndəbaʊt] adj 绕道的,绕圈子的,不直接的The car has to take a ~course to avoid the flood scan [skæn] vt 细看,审视~the doctor’s face for a sign of hope scare [skeə] vt 惊吓,恐惧The thunder ~d the children scatter [‘skætə] vt撒,撒播~corn for the chickens schedule [‘ʃedjuːl; ‘sked-] n时间表,进度表a fight ~ scheme [skiːm] n计划,方案He suggested several ~s to increase sales scold [skəʊld]vi 责骂,训斥Dad ~ed me for coming home late scorn [skɔːn] vt轻蔑,鄙视He ~s lying scrap [skræp] n碎片,零屑~s of bread scrape [skreɪp] vt 刮,擦~one’s chin 总复习测试 remark [rɪ’mɑːk] vt评论,议论(on, upon) It would be rude to ~upon her appearance remedy [‘remɪdɪ] n 治疗,治疗法,药品a folk ~ spectacle [‘spektək(ə)l] n景象,奇观,壮观,场面The burning house was a terrible ~ squeeze [skwiːz] vt榨,挤~juice from oranges stern [stɜːn] adj严厉的,苛刻的a ~punishment stimulate [‘stɪmjʊleɪt] vt刺激,促使~economic growth ignore [ɪg’nɔː] vt 不顾,不理,忽视:He who ignores history is doomed to repeat it illuminate [ɪ’l(j)uːmɪneɪt] vt 照明,照亮,解释,启发,启迪:The room was poorly illuminated by one candle illustrate [‘ɪləstreɪt] vt(用图或例子)说明,阐明:A complicated rule can be illustrated by a simple example immerse [ɪ’mɜːs] vt使浸没:He immersed himself in the pool shift 转移,移动Don’t try to ~the blame onto me shrewd [ʃruːd] adj 机灵的,敏锐的,精明的a ~guess synchronize[‘sɪŋkrənaɪz] vt使同步,使结合,使协调Their steps ~d taboo [tə’buː] n禁忌,忌讳Movies producers attempted to break down all the ~s tackle [‘tæk(ə)l] n 用具,装备shaving ~ transient [‘trænzɪənt] adj短暂的,转瞬即逝的~happiness simulate [‘sɪmjʊleɪt] vt 假装,冒充use tricks to ~illness sketch [sketʃ] n 素描,速写make a ~of a face in pencil slack [slæk] adj 不紧的,不严的a ~soft rope slender [‘slendə] adj苗条的,修长的,细长的She is ~and has long dark hair edible[‘edɪb(ə)l] adj可以吃的,可食用的:edible oil elaborate[ɪ’læb(ə)rət] adj精心计划(或制作)的,详尽的:an elaborate plot elevate[‘elɪveɪt] vt举起,提高,抬起:elevate the living standards of the people fascinate[‘fæsɪneɪt] vt强烈的吸引,迷住,使神魂颠倒:be fascinated with history emigrate [‘emɪgreɪt] vi移居外国(或外地区):emigrate from Canada to Australia emit [ɪ’mɪt] vt发出,散发(光,热,电度,声音,液体,气味等),发射电子:The chimney emitted a cloud of smoke endeavor [ɪn’dɛvɚ]vt (为达到某一目的而)努力,尝试:How much he endeavored, goal stayed unattained compulsory [kəm’pʌls(ə)rɪ] adj 必须做的,义务的 a compulsory subject conceal [kən’siːl] vt 隐藏,隐瞒,隐蔽He concealed his disappointment from his friends. conceive [kən’siːv] vt 构想出,设想conceive an idea restore [rɪ’stɔː] vt恢复,修复,康复The temple has been ~d out of all recognition restrain [rɪ’streɪn] vt抑制,遏止~one’s anger resume [rɪ’zjuːm] vt 重新开始,继续When I quit golf, I ~d writing rouse [raʊz] vt 惊起The boat ~d wild ducks to flight row [rəʊ] n 吵闹,争吵What’s all this ~about? rub [rʌb] vt擦,摩擦He rubbed his eyes rust [rʌst] n锈,铁锈Iron gathers ~easily ruthless [‘ruːθlɪs] adj无情的,残忍的,冷酷的a~enemy sack [sæk] n 麻袋,纸袋,塑料袋a ~of cement sacred [‘seɪkrɪd] adj神圣的the ~name of God sacrifice [‘sækrɪfaɪs] n献祭,供奉a law banning the ~of animals salute [sə’l(j)uːt] vt 敬礼,祝贺~one’s superior officer retain [rɪ’teɪn] vt保留,保持~the right to reply reveal [rɪ’viːl] vt 揭示,揭露I promise never to ~his secret revenge revenge vt报仇,复仇Man may destroy the balance of nature ,but ,from time to time, nature takes a terrible ~ concentrate [‘kɒns(ə)ntreɪt] vt 集中,专心concentrate one’s attention condemn [kən’dem]vt 谴责We all condemn cruelty to children. condense [kən’dens] vt (使)压缩,(使)凝结,使简洁condense a speech to half confer [kən’fɜː]vt 商谈,商议confer with sb. Over (on, about, concerning) sth. Confess [kən’fes] vt 坦白,供认,承认,忏悔confess one’s crime formulate’fɔːmjʊleɪt] vt规划(制度等),构想出(计划方法等)formulate a policy gallop [‘gæləp] vi(马等的)飞跑,疾驰:The horse did a fine gallop during the race gamble[‘gæmb(ə)l] vi赌博:gamble at cards gasp [gɑːsp] vi 喘气,喘息:speak between gasps gear [gɪə] n齿轮,传动装臵,(排)档:interlock cogs and gears generalize [‘dʒɛnrəlaɪz] vt 归纳,概括:I don’t think you can generalize about what voters will do 1. cursor n. 光标 2. directory n. 目录，索引簿 3. current n. 电流 4. specify v. 指定，规定，确定 29. erase v. 擦除，取消，删除 36. prompt n. &amp; v. 提示 41. argument n. 变元，自变量 42. statement n. 语句，陈述，命题 47. macro n. 宏，宏功能，宏指令 48. page n. 页面，页，版面 49. define vt. 定义，规定，分辨 50. reference n. &amp; a. 参考；参考的 51. other a. 别的，另外的 52. while conj. 当…的时候 53. pressing n. &amp; a. 压制；紧急的 54. restore vt. 恢复，复原 55. block n. (字，信息，数据)块 56. decimal n. &amp; a. 十进制；十进制 的 57. optional a. 任选的，可选的 58. arrow n. 箭头，指针 60. within prep. 在…以内 61. issue v. 发行，出版，流出 65. associate v. 相联，联想，关联 78. carriage n. 滑架，托架 87. execution n. 执行 91. pointer n. 指针，指示字 92. subset n. 子集，子设备 98. positioning n. 定位 101. expression n. 表达式 102. through prep. &amp; ad. 通过，直通 103. toggle n. &amp; v. 触发器；系紧 113. call v. 调用，访问，呼叫 115. indicate vt. 指示，表示 118. place vt. 放，位，地点 122. remain vi. 剩下，留下，仍然 124. combination n. 结合，组合 125. profile n. 简要，剖面，概貌 129. turn v. &amp; n. 转，转动；圈，匝 132. section n. 节，段，区域 136. access n. 存取，选取，接近 137. additional a. 附加的，辅助的 141. numeric n. &amp; a. 数字的，分数 142. go vi. 运行，达到 143. load n. &amp; v. 装入，负载，寄存 146. entire a. &amp; n. 完全的；总体 147. leave v. 离开，留下 150. reflow v. &amp; n. 回流，逆流 151. output n. 输出，输出设备 152. out n. &amp; a. 输入，在外 153. both a. &amp; ad. 两，双，都 154. install vt. 安装 155. source n. 源，电源，源点 156. way n. 路线，途径，状态 157. assign vt. 赋值，指定，分派 158. support vt. 支援，支持，配套 159. specific a. 特殊的，具体的 160. join v. &amp; n. 连接，并(运算) 161. expand v. 扩充，扩展，展开 162. like a. 类似的，同样的 163. diskette n. 软磁盘，软盘片 164. skip v. 跳跃(定位)，跳过 165. application n. 应用 166. confirmation n. 认可 167. whether conj. 无论，不管 172. abbreviate vt. 缩写，省略 173. show v. 显示，呈现，出示 174. otherwise ad. &amp; a. 另外 175. working n. 工作，操作，作业 176. delimiter n. 定界符，分界符 177. location n. 定位，(存储器)单元 178. perform v. 执行，完成 179. graphic n. &amp; a. 图形；图形的 180. read v. 读，读阅 181. confirm vt. 证实，确认 182. sort v. 分类，排序 183. clause n. 条款，项目，子句 184. once ad. &amp; n. 只一次，一旦 186. extend v. 扩充 190. original n. &amp; a. 原文；原(初)始 的 191. correspond vi. 通信(联系) 192. property n. 性(质)，特征 193. several a. &amp; n. 若干个，几个 194. learn v. 学习，训练 195. cause n. 原因，理由 196. bracket n. (方)括号，等级 197. omit vt. 省略，删去，遗漏 198. running a. 运行着的，游动的 199. sub-directory n. 子目录 200. edge n. 棱，边，边缘，界限 202. instruction n. 指令，指导 204. below a. &amp; prep. 下列的；低于 205. standard n. 标准 206. occurrence n. 出现，发生 209. destination n. 目的地，接收站 212. variety n. 变化，种类，品种 218. operating a. 操作的，控制的 219. carry v. 进位，带 220. update v. 更新，修改，校正 221. moving n. &amp; a. 活动的，自动的 222. coprocessor n. 协同处理器 223. overlay v. 覆盖，重叠 230. shortcut n. 近路，捷径 233. part n. 部分，零件 234. updated a. 适时的，更新的 235. internal a. 内部的 240. determine v. 确定 241. making n. 制造，构造 245. action n. 操作，运算 248. assigned a. 指定的，赋值的 249. give vt. 给出，赋予，发生 251. chapter n. 章，段 264. recall vt. 撤消，复活，检索 265. deletion n. 删去(部分)，删除 269. quote n. &amp; v. 引号；加引号 270. correct a. &amp; vt. 正确的；改正 271. else ad. &amp; conj. 否则，此外 272. maximum n. &amp; a. 最大(的)，最 高 280. uppercase n. 大写字母 282. found v. 建立，创办 284. force v. &amp; n. 强制；压力，强度 285. lowercase n. 下档，小写体 286. just ad. 恰好 287. undo vt. 取消，废除 288. environ vt. 围绕，包围 290. temporary a. 暂时的，临时的 293. encounter v. &amp; n. 遇到，碰到 294. across prep. 交叉，越过 295. matching n. 匹配，调整 296. wildcard n. 通配符 297. spill v. 漏出，溢出，漏失 299. browse v. 浏览 300. speech n. 说话，言语，语音 301. occur vi. 发生，出现，存在 302. memo n. 备忘录 303. prior a. 先验的，优先的 304. loaded a. 有负载的 305. length n. (字，记录，块)长度 306. round v. 舍入，四舍五入 307. variant n. &amp; a. 变体，易变的 308. floppy n. 软磁盘 309. machine n. 机器，计算机 310. square n. &amp; a. 正方形；方形的 311. supply vt. &amp; n. 电源，供给 314. onto prep. 向…，到…上 315. during prep. 在…期间 316. module n. 模块(程序设计) 317. monochrome n. 单色 318. assistance n. 辅助设备，帮助 319. tell n. 讲，说，教，计算 320. library n. (程序…)库，图书馆 321. demonstration n. (公开)表演，示 范 322. stack n. 栈，堆栈，存储栈 323. even a. &amp; ad. 偶数的；甚至 324. evaluate v. 估计，估算，求值 325. times n. 次数 326. previously ad. 以前，预先 327. directly ad. 直接地，立即 328. logical a. 逻辑的，逻辑“或” 329. template n. 标准框，样板，模板 330. calling n. 呼叫，调用，调入 331. later a. 更后的，后面的 333. therefore ad. &amp; conj. 因此，所以 336. linker n. 连接程序 337. loop n. 圈，环；(程序)循环，回 路 339. scheme n. 方案，计划，图 340. every a. 每个，全体，所有的 341. refer v. 访问，引用，涉及 342. possible a. 可能的，潜在的 343. above a. 在…之上，大于 344. overview n. 综述，概要 346. syntax n. 语法，文法，句法 347. abbreviation n. 缩短，省略，简 称 348. bios n. 基本输入/输出系统 352. private a. 专用的，私人的 353. hard a. 硬的 356. equal vt. &amp; n. 等于，相等；等号 357. pack n. 压缩，包裹 358. minus a. &amp; n. 负的；负数，减 359. alternate a. 交替的，备用的 360. collapse v. 崩溃，破裂 361. corner n. 角，角落，转换 362. present a. &amp; v. 现行的；提供 363. interpreter n. 解释程序，翻译机 364. advance v. &amp; n. 进步，提高；进 展 365. forward a. 正向的 366. fast a. &amp; ad. 快速的 367. special a. 专用的，特殊的 368. slash n. 斜线 369. utility n. &amp; a. 实用程序；实用性 370. regardless a. 不注意的，不考虑 的 371. disable vt. 禁止，停用 372. compatible a. 可兼容的，可共存 的 373. depend vi. 随…而定，取决于 374. empty a. 空，零，未占用 375. alphabetical a. 字母(表)的，abc 的 376. branch n. 分支，支线；v. 转换 377. resume v. 重(新)开(始) 378. multiple a. 多次的，复杂的 379. monitor n. 监视器，监督程序 380. configuration n. 配置 381. replacement n. 替换，置换，更 新 382. required a. 需要的 383. macros n. 宏命令(指令) 384. table n. 表 385. loss n. 损耗，损失 386. batch n. 批，批量，成批 387. exact a. 正确的 388. aboveboard ad. &amp; a. 照直，公开 的 389. activate vt. &amp; n. 使激活，驱动 390. around ad. &amp; prep. 周围，围绕 391. slow a. &amp; ad. 慢速的 392. floating a. 浮动的，浮点的 393. refresh v. 刷新，更新，再生 394. stop v. 停止，停机 395. pass v. 传送，传递，遍(数) 396. public a. 公用的，公共的 397. eject n. 弹出 398. ignore vt. 不管，忽略不计 399. share v. 共享，共用 400. sequence n. 顺序，时序，序列 401. consist vi. 符合，包括 402. step n. 步，步骤，步长，档 403. double a. 两倍的，成双的 404. come vi. 来，到，出现 405. lower a. 下部的，低级的 406. describe vt. 描述，沿…运行 407. count v. 计数，计算 408. pop v. 上托，弹出(栈) 409. valid a. 有效的 410. suspend v. 中止，暂停，挂起 411. enhance vt. 增强，放大，夸张 412. separate v. &amp; a. 分隔，分离，各 自的 413. echo n. 回波，反射波 414. necessary a. 必要的，必然的 415. greater than 大于 416. able a. 能…的，有能力的 417. marking n. 标记，记号，传号 418. ask v. 请求，需要 419. term n. 项，条款，术语 420. bring v. 引起，产生，拿来 421. warning n. &amp; a. 报警，预告 422. less a. &amp; ad. 更小，更少 423. whose pron. 谁的 424. comment n. &amp; vi. 注解，注释 425. effect n. 效率，作用，效能 426. expanding a. 扩展的，扩充的 427. on-line a. 联机的 428. reorder v. (按序)排列，排序 429. direct a. 直接的 430. enclose vt. 封闭，密封，围住， 包装 431. reset vt. 复位，置“0” 432. various a. 不同的，各种各样的 433. paper n. 纸，文件，论文 434. prevent v. 防止，预防 435. side n. (旁)边，面，侧(面) 436. push v. 推，按，压，进(栈) 437. programming n. 程序设计，编程 序 438. upper a. 上的，上部的 439. row n. 行 440. pressed a. 加压的，压缩的 441. temporarily ad. 暂时 442. day n. 日，天，白天，时代 443. repaint vt. 重画 444. redefine vt. 重新规定(定义) 445. relation n. 关系，关系式 446. dimension n. 尺寸，维，因次 447. boundary n. 边界，界限，约束 448. zoom v. 变焦距 449. initialize v. 初始化 450. personal a. 个人的，自身的 451. hello int. &amp; v. 喂！；呼叫 452. true a. &amp; n. 真，实，选中 453. wish v. &amp; n. 祝愿，希望 454. font n. 铅字，字形 455. know v. 知道，了解，认识 456. convert v. 转换，变换 457. global n. 全局，全程，全局符 458. still a. &amp; n. &amp; v. 静止的；静；平 静 459. installation n. 安装，装配 460. invoke vt. 调用，请求 461. interactive a. 交互式，交互的 462. described a. 被看到的，被发现 的 463. century n. 世纪 464. literal a. 文字的 465. rather ad. 宁可，有点 466. exclusive a. 排斥，排它性 467. marker n. 记号，标记，标志 468. wait v. 等待 469. appropriate a. 适当的，合适的 470. fit v. &amp; n. 适合，装配；非特 471. adapter n. 适配器，转换器 472. filter n. 滤波器，滤光材料 473. break v. 断开，撕开，中断 474. backward ad. 向后，逆，倒 475. searching n. 搜索 476. receive v. 接收 477. dual a. 对偶的，双的 478. retry vt. 再试，复算 479. normally ad. 正常地，通常 480. exactly ad. 正好，完全，精确地 481. immediately ad. 直接地 482. separated a. 分开的 483. high a. 高 484. equivalent a. 相等的，等效的 485. light n. &amp; a. 光(波，源)；轻的 486. zero n. 零，零位，零点 487. storage n. 存储，存储器 488. width n. 宽度 489. language n. 语言 490. startup n. 启动 491. much a. &amp; n. 很多，许多，大量 492. per prep. 每，按 493. over prep. 在…上方 494. mirror n. &amp; v. 镜，反射，反映 495. request n. &amp; vt. 请求 496. keypad n. 小键盘 497. keep v. 保持，保存 498. resident a. 驻留的 499. learning n. 学问，知识 500. talk v. 通话，谈话 501. summary n. 摘要，汇总，提要 502. well n. &amp; a. 井；好，良好 503. link n. &amp; v. 链接；连接，联络 504. according to a. 按照，根据 505. identify v. 识别，辨认 506. designated a. 指定的，特指的 507. pertain vi. 附属，属于，关于 508. expansion n. 展开，展开式 509. incompatible a. 不兼容的 510. blinking n. 闪烁 511. month n. 月份 512. precede v. 先于 513. readily ad. 容易地，不勉强 514. transportable a. 可移动的 515. appropriately ad. 适当地 516. routine n. 程序，例行程序 517. ready a. 就绪，准备好的 518. listing n. 列表，编目 519. newly ad. 新近，重新 520. year n. (一)年，年度，年龄 521. contact n. 接触，触点 522. session n. 对话，通话 523. own a. &amp; v. 自己的；拥有 524. redraw vt. 再拉 525. here ad. 在这里 526. manual a. 手工的，手动的 527. particular a. 特定的，特别的 528. rectangle n. 矩形 529. additive a. &amp; n. 相加的；附加物 530. similar a. 相似的 531. assembly n. 汇编，安装，装配 533. description n. 描述 534. retrieve v. 检索 536. produce v. 生产，制造 537. ram 随机存取存储器 538. exception n. 例外，异常，异议 539. digit n. 数字，位数，位 540. reverse v. &amp; a. 反向的，逆 541. minimum n. &amp; a. 最小(的)，最低 542. enough a. &amp; ad. 足够的，充足的 543. although conj. 虽然，即使 544. reindex v. &amp; n. 变换(改变)符号 547. along prep. &amp; ad. 沿着 548. test n. &amp; v. 测试 549. small a. 小的，小型的 550. feed v. 馈给，(打印机)进纸 553. compile vt. 编译 554. frequently ad. 常常，频繁地 555. undefined a. 未定义的 556. state n. &amp; vt. 状态；确定 557. tick v；n. 滴答(响)；勾号(√) 558. accept vt. 接受，认可，同意 559. intense a. 强烈的，高度的 560. documentation n. 文件编制，文 本 561. asterisk n. 星号(*) 562. easily ad. 容易地，轻易地 563. become v. 成为，变成，适宜 564. address vt. &amp; n. 寻址；地址 565. interface n. 接口 566. pause vi. 暂停 567. repeat v. 重复 568. restart v. 重新启动，再启动 569. assumed a. 假定的 570. speed n. 速度 571. entry n. 输入，项(目)，入口 572. combine v. 组合，联合 573. organize v. 组织，创办，成立 574. finished a. 完成的 575. mixed a. 混合的 576. permit v. 许可，容许 577. formatting n. 格式化 579. symbol n. 符号，记号 580. binary n. &amp; a. 二进制；双态的 581. whenever ad. &amp; conj. 随时 582. reach v. &amp; n. 范围，达到范围 583. caution n. &amp; v. 警告，注意 584. subtotal n. &amp; v. 小计，求部分和 585. card n. 卡片，插件(板) 586. general a. 通用的 587. associated a. 联合的，相联的 588. transfer v. 传送，转换，转移 590. partition v. 划分，分区，部分 591. hexadecimal a. 十六进制的 593. specification n. 说明书，规则说 明书 594. customize vt. 定制，定做 596. nest v. 嵌套，后进先出 597. duplicate vt. 复制，转录，加倍 598. compression n. 压缩，浓缩 599. unable a. 不能的 600. means n. 方法，手段 601. alternately ad. 交替地，轮流地 602. intensity n. 强度，亮度 603. reading n. 读，读数 605. explicitly ad. 明显地，显然地 607. sector n. &amp; v. 扇区，段；分段 609. vertically ad. 竖直地，直立地 610. horizontally ad. 水平地 611. backspace v. 退格，回退 612. terminate v. 端接，终止 613. people n. 人们 614. short a. &amp; n. 短的；短路 615. drag vt. 拖，拉，牵，曳 616. formatted a. 有格式的 617. preview n. &amp; vt. 预映 618. underscore vt. 在…下面划线 619. correctly ad. 正确地 620. initially ad. 最初，开头 621. reformat v. 重定格式 622. inside n. &amp; a. 内部，内容；内部 的 623. integrate v. 综合，集成 624. controlled a. 受控制的，受操纵 的 625. period n. 周期 626. huge a. 巨大的，非常的 627. determined a. 坚决的，毅然的 628. trailing n. &amp; a. 结尾；尾随的 629. seek v. 查找，寻找，探求 630. introduction n. 入门，介绍，引 进 631. indent v. 缩排 632. base n. 基，底，基地址 633. integer n. 整数 634. attempt vt. &amp; n. 尝试，试验 635. twice n. &amp; ad. 两次，两倍于 636. formed a. &amp; n. 成形 637. subscript n. 注脚，下标 638. tiny a. 微小的，微量的 639. model n. 模型，样机，型号 640. correction n. 校正，修正 641. rating n. 定额，标称值 642. secondary a. 辅助的，第二的 643. opened a. 开路的，断开的 646. translate v. 翻译，转换，平移 647. reason n. 原因，理由 648. colon n. 冒号“:” 649. avoid vt. 避免，取消，无效 650. range n. 范围，域，区域 651. allocate vt. 分配 652. wordperfect a. 一字不错地熟记 的 653. simply ad. 简单地，单纯地 654. verify vt. 鉴定，检验，核对 655. manner n. 方法，样式，惯例 656. direction n. 方向，定向，指向 657. portion n. &amp; vt. 部分；分配 658. emulator n. 仿真器，仿真程序 659. successful a. 成功的 660. applied a. 适用的，外加的 661. sum n. 和，合计，总额 achieve vt. 完成，实现 ​ 663. together ad. 一同，共同，相互 ​ 664. affect vt. 影响，改变，感动 ​ 665. delay v. 延迟 ​ 666. free a. 自由的，空闲的 ​ 667. properly ad. 真正地，适当地 ​ 668. kind n. 种类，属，级，等 ​ 669. splitting n. 分区(裂) ​ 670. feature n. 特征，特点 ​ 671. console n. 控制台，操作台 ​ 672. operate v. 操作，运算 ​ 673. kernel n. 内核(核心)程序 ​ 674. easy a. &amp; ad. 容易的；容易地 ​ 675. modifier n. 修改量，变址数 ​ 676. invalid a. 无效的 ​ 677. compiler n. 编译程序(器) ​ 678. dot n. 点 ​ 679. beep n. 蜂鸣声，嘀嘀声 ​ 680. face n. 面，表面 ​ 681. random a. 随机的 ​ 682. facility n. 设施，装备，便利 ​ 683. heading n. 标题 ​ 684. asynchronous a. 异步的，非同步 的 ​ 685. series n. 序列，系列，串联 ​ 686. individual a. 个别的，单个的 ​ 687. explain v. 阐明，解释 ​ 688. paste n. 湖，胶，膏 ​ 689. welcome vt. &amp; n. 欢迎 ​ 690. six n. &amp; a. 六(个)(的) ​ 691. early a. &amp; ad. 早期，初期 ​ 692. wrap v. &amp; n. 包装，缠绕 ​ 693. blue a. &amp; n. 蓝(色)，青色 ​ 694. queue v. &amp; n. 排队，队列 ​ 695. interrupt v. &amp; n. 中断 ​ 696. respect n. &amp; vt. 遵守，关系 ​ 697. converted a. 转换的，变换的 ​ 698. common a. 公用的 ​ 699. hyphen n. 连字符，短线 ​ 700. serial a. 串行的，串联的 ​ 701. loading n. 装入，加载，存放 ​ 702. retain vt. 保持，维持 ​ 703. setup n. 安排，准备，配置 ​ 704. freeze v. 冻结，结冰 ​ 705. intend vt. 打算，设计 ​ 706. explanation n. 说明，注解，注释 ​ 707. certain a. 确实的，确定的 ​ 708. zap v. 迅速离去，击溃 ​ 709. archive vt. 归档 ​ 710. negative a. 负的，否定的 ​ 711. image n. 图像，影像，映像 ​ 712. platform n. 平台，台架 ​ 713. often ad. 经常，往往，屡次 ​ 714. signal n. &amp; v. 信号；发信号 ​ 715. cpu 控制处理部件 ​ 716. bit n. 比特；(二进制)位 ​ 717. fully ad. 十分，完全 ​ 718. deactivate vt. 释放，去活化 ​ 719. especially ad. 特别(是)，尤其 ​ 720. usually ad. 通常，平常，一般 ​ 721. recommend vt. 推荐，建议 ​ 722. maintain vt. 维护，保养，保留 ​ 723. important a. 严重的，显著的 ​ 724. central a. 中央的，中心的 ​ 725. addition n. 加法，增加 ​ 726. anytime ad. 在任何时候 ​ 727. analyst n. 分析员 ​ 728. false a. 假(布尔值)，错误 ​ 729. black a. &amp; n. 黑色的，黑色 ​ 730. gather n. 聚集，集合 ​ 731. cycle n. &amp; v. 周，周期；循环 ​ 732. relative a. 相对的 ​ 733. offer v. 提供，给予，呈现 ​ 734. ending n. 结束 ​ 735. rent v. &amp; n. 租用；裂缝 ​ 736. sentence n. 句(子) ​ 737. remember v. 存储，记忆，记住 ​ 738. proper a. 真的，固有的 ​ 739. design v. 设计 ​ 740. examine v. 检验，考试，审查 ​ 741. initial a. 最初的，初始的 ​ 742. corrupt v. &amp; a. 恶化；有毛病的 ​ 743. buy v. 买，购买，赢得 ​ 744. increase v. 增加，增大 ​ 745. host n. 主机 ​ 746. sample n. &amp; v. 样品，样本；抽 样 ​ 747. pending a. 悬而未决的，未定的 ​ 748. divide v. 除 ​ 749. boot n. 引导，靴 ​ 750. hide v. 隐藏，隐蔽 ​ 751. half n. &amp; a. &amp; ad. 一半，半个 ​ 752. magenta n. &amp; a. 深红色(的) ​ 753. leading n. &amp; a. 引导(的) ​ 754. wrong a. &amp; ad. n. 错误(的) ​ 755. today n. &amp; ad. 今天 ​ 756. least a. &amp; ad. 最小(的) ​ 757. opposite a. &amp; n. &amp; ad. 相反的 ​ 758. white a. &amp; n. 白色(的) ​ 759. override v. &amp; n. 超越，克服 ​ 760. brown a. &amp; n. 褐色(的)，棕色 ​ 761. hex a. &amp; n. 六角形的 ​ 762. rest n. &amp; v. 剩余，休息 ​ 763. damage n. &amp; vt. 损伤，故障 ​ 764. instant a. 立刻的，直接的 ​ 765. reserved a. 保留的，预订的 ​ 766. technology n. 工艺，技术，制造 学 ​ 767. handle n. 处理，句柄 ​ 768. apply v. 应用，适用于，作用 ​ 769. stand v. 处于(状态)，保持 ​ 770. payment n. 支付，付款 ​ 771. kilobyte n. 千字节(kb) ​ 772. parenthesis n. 括弧，圆括号 775. developer n. 开发者，显影剂 ​ 776. murder n. 弄坏，毁掉 ​ 777. flush v. 弄平，使齐平 ​ 778. unlock v. 开锁，打开 ​ 779. movement n. 传送，移动 ​ 780. consecutive a. 连续的，连贯的 ​ 781. collection n. 集合，聚集，画卷 ​ 782. front a. 前面的，正面的 ​ 783. addressing n. 寻址 ​ 784. prefix n. 前缀 ​ 785. carousel n. 圆盘传送带 ​ 786. safety n. 安全，保险 ​ 787. static a. 静态的，不变的 ​ 788. background n. 背景，底色，基 础 ​ 789. product n. (乘)积，产品 ​ 790. assignment n. 赋值，分配 ​ 791. bad a. 坏的，不良的 ​ 792. declare v. 说明 ​ 793. adjust vt. 调整，调节，控制 ​ 794. recognize v. 识别 ​ 795. route n. 路线，路由 ​ 796. respectively ad. 分别地 ​ 797. unsuccessful a. 不成功的，失败 的 ​ 798. received a. 被接收的，公认的 ​ 799. navigate v. 导航，驾驶 ​ 800. considered a. 考虑过的，被尊重 的 ​ 801. due a. 到期的，应付(给)的 ​ 802. recently ad. 近来 ​ 803. room n. 房间，空间 ​ 804. descend v. 下降，落下 ​ 805. fact n. 事实 ​ 806. alter v. 改变，修改 ​ 807. track n. 磁道，轨道 ​ 808. precedence n. 优先权 ​ 809. skeleton n. 骨架，框架 ​ 810. log n. &amp; v. 记录，存入 ​ 811. star n. 星形，星号 ​ 812. hot a. 热的 ​ 813. replaceable a. 可替换的 ​ 814. accessible a. 可以使用的 ​ 815. involve vt. 涉及，卷入，占用 ​ 816. configure vt. 使成形 ​ 817. question n. 问题 ​ 818. green n. &amp; a. 绿色绿色的 ​ 819. entirely ad. 完全地，彻底地 ​ 820. helpful a. 有帮助的，有用的 ​ 821. middle a. 中间的 ​ 822. declared a. 承认的，申报的 ​ 823. compress vt. 压缩，精减 ​ 824. graphically ad. 用图表表示 ​ 825. auto a. 自动的 ​ 826. automatic a. 自动的 ​ 827. aligned a. 对准的，均衡的 ​ 828. anywhere ad. 在任何地方 ​ 829. terminal n. 终端，端子 ​ 830. door n. 舱门，入口，孔 ​ 831. expire v. 终止，期满 ​ 832. resolution n. 分辨率 ​ 833. local a. 局部的，本地的 ​ 834. semicolon n. 分号(；) ​ 835. reread vt. 重读 ​ 836. overwrite v. 重写 ​ 837. critical a. &amp; n. 临界的；临界值 ​ 838. manager n. 管理程序 ​ 839. capability n. 能力，效力，权力 ​ 840. affected a. 受了影响的 ​ 841. allowed a. 容许的 ​ 842. border n. 边界，框，界限 ​ 843. cache n. 高速缓存 ​ 844. bell n. 铃，钟 ​ 845. play v. 玩，奏，放音，放象 ​ 846. quickly a. 快，迅速地 ​ 847. fastback n. 快速返回 ​ 848. answer n. &amp; v. 响应，回答；答 复 ​ 849. represent v. 表示，表现，代表 ​ 850. difference n. 差分，差 ​ 851. highest a. 最高的 ​ 852. project n. 项目，计划，设计 ​ 853. physical a. 物理的，实际的 ​ 854. matter n. 物质，内容，事情 ​ 855. hercules n. 大力神，大力士 ​ 856. reduce v. 减少，降低，简化 ​ 857. publisher n. 出版者，发行人 ​ 858. trim n. 区标，微调 ​ 859. substitute v. 代替，替换，代入 ​ 860. disabled a. 禁止的，报废的 ​ 861. recent a. 近来的 ​ 862. positive a. 正的，阳的，正片 ​ 863. upgrade v. 升级，提高质量 ​ 864. instance n. &amp; vt. 例子，情况；举 例 ​ 865. happen vi. (偶然)发生，碰巧 ​ 866. elapsed vi. &amp; n. 经过 ​ 867. future n. &amp; a. 将来，未来的 ​ 868. midnight n. &amp; a. 午夜 ​ 869. though conj. 虽然，尽管 ​ 870. nor conj. 也不 ​ 871. mono a. &amp; n. 单音的 ​ 872. slide v. &amp; n. 滑动，滑动触头 ​ 873. abort v. &amp; n. 中断，故障 ​ 874. jump v. &amp; n. 转移 ​ 875. toward prep. 朝(着…方向) ​ 876. throughout prep. 贯穿，整，遍 ​ 877. via prep. 经过，经由 ​ 878. among prep. 在…之中，中间 ​ 879. neither a. &amp; pron. (两者)都不 ​ 880. layer n. &amp; v. 层，涂层 ​ 881. scatter v. 散射，分散，散布 ​ 882. attention n. 注意(信号) ​ 883. convention n. 常规，约定，协定 ​ 884. conventional a. 常规的，习惯的 ​ 885. tool n. 工具，刀 ​ 886. handler n. 处理程序 ​ 887. processor n. 处理机，处理程序 ​ 888. desktop a. 台式的 ​ 889. build v. 建造，建立，组合 ​ 890. windowing n. 开窗口 ​ 891. development n. 开发，研制， 显影 ​ 892. exceed v. 超过，大于 ​ 893. understand v. 懂，明白(了)， 理解 ​ 894. horizontal a. 水平的，横向的 ​ 895. alphabetically ad. 按字母表顺 序 ​ 896. meet v. “与”，符合，满足 ​ 897. protect vt. 保护 ​ 898. reserve vt. 保留，预定，预约 ​ 899. clock n. 时钟，计时器，同步 ​ 900. manifest vt. 表明，显示，显现 ​ 901. safe a. 安全的，可靠的 ​ 902. disconnect vt. 拆接，断开，拆 线 ​ 903. clockwise a. 顺时针的 ​ 904. eliminate vt. 除去，消除，切断 ​ 905. actual a. 实际的，现实的 ​ 906. declaration n. 说明，申报 ​ 907. probably ad. 多半，很可能 ​ 908. ring n. &amp; v. 环，圈；按铃 ​ 909. cover vt. 盖，罩，套 ​ 910. indicator n. 指示器，指示灯 ​ 911. apple n. 苹果 ​ 912. icon n. 图符，象征 ​ 913. consideration n. 考虑，研究， 讨论 ​ 914. skill n. 技巧 ​ 915. picture n. 图象，画面 ​ 916. layout n. 布置，布局，安排 ​ 917. suggest vt. 建议，提议，暗示 ​ 918. convenient a. 方便的，便利的 ​ 919. instruct vt. 讲授，命令 ​ 920. appendix n. 附录 ​ 921. medium n. &amp; a. 媒体；中等的 ​ 922. truncate vt. 截尾，截断 ​ 923. inhibit vt. 禁止 ​ 924. nearly ad. 近乎，差不多，几乎 ​ 925. warn vt. 警告，警戒，预告 ​ 926. underline n. 下划线 ​ 927. register n. 寄存器 ​ 928. stuff n. &amp; vt. 材料；装入 ​ 929. exclude vt. 排除，除去 ​ 930. destroy vt. 破坏，毁坏，打破 ​ 931. calculation n. 计算，统计，估 计 ​ 932. angle n. 角，角度 ​ 933. lexical a. 辞典的，词法的 ​ 934. decide v. (使)判定，判断 ​ 935. trouble n. 故障 ​ 936. processing n. (数据)处理，加工 ​ 937. customer n. 顾客，客户 ​ 938. port n. 端口，进出口 ​ 939. discuss vt. 讨论，论述 ​ 940. segment n. 段，片段，图块 ​ 941. filing n. (文件的)整理汇集 ​ 942. identically ad. 相等，恒等 ​ 943. market n. 市场，行情，销路 ​ 944. valuable a. 有价值的，贵重的 ​ 945. limited a. 有限的，(受)限制的 ​ 946. trying a. 费劲的，困难的 ​ 947. heap n. 堆阵 ​ 948. grey n. &amp; a. 灰色；灰色的 ​ 949. permanently ad. 永久地，持久 地 ​ 950. accelerator n. 加速装置，加速 剂 ​ 951. originally ad. 原来，最初 ​ 952. ability n. 性能，能力，效率 ​ 953. internally ad. 在内(部) ​ 954. derelict vt. 中途淘汰 ​ 955. redirect vt. 重定向 ​ 956. reside vi. 驻留 ​ 957. header n. 首部，标题，报头 ​ 958. extra a. 特别的，额外的 ​ 959. repeated a. 重复的 ​ 960. death n. 毁灭，消灭 ​ 961. observe v. 观察，探测 ​ 962. density n. 密度 ​ 966. master a. 总要的，总的 ​ 967. recursive a. 递归的，循环的 ​ 968. trap n. &amp; vt. 陷阱；俘获 ​ 969. dimensional n. 尺寸的，…维的 ​ 971. conjunction n. 逻辑乘，“与” ​ 972. identical a. 相等的，相同的 ​ 975. fall n. 落下，降落 ​ 976. interval n. 间歇，区间 ​ 977. compatibility n. 兼容性，适应性 ​ 980. criterion n. 标准，判据，准则 ​ 982. express a. 快速的 ​ 983. volume n. 卷，册，体积，容量 ​ 985. rated a. 额定的 ​ 986. activity n. 活力，功率 ​ 987. odometer n. 里程表，计程仪 ​ 988. phoenix n. 凤凰，绝世珍品 ​ 990. easel n. 框，(画)架 ​ 991. latter a. 后面的，最近的 ​ 993. mainframe n. 主机，大型机 ​ 995. diacritical a. 区分的，辩别的 confidential a. 机密的 trace v. 跟踪，追踪 division n. 除，除法，(程序)部分 regular a. 正则的，正规的 implicit a. 隐式的 mention vt. &amp; n. 叙述，说到 near ad. &amp; prep. 领近，接近 fifth n. &amp; a. 第五，五分之一 whereas conj. 面，其实，既然 review v. &amp; n. (再)检查 transform v. &amp; n. 变换，变换式 align v. &amp; n. 定位，对准 yellow a. &amp; n. 黄色(的) assist v. &amp; n. 加速，帮助 finish v. &amp; n. 完成，结束 micro a. &amp; n. 微的，百万分之 upon prep. 依据，遵照 exhaust v. 取尽，用完 sounding a. 发声的 alpha n. 希腊字母 α，未知数 warranty n. 保证(书)，授权 industry n. 工业 handling n. 处理，操纵 treat v. 处理，加工 usage n. 应用，使用，用法 building n. 建造，建筑，房屋 ally v. 联合，与…关联 exclamation n. 惊叹(号) turning a. 转弯的，旋转的 whole a. 全部的，整个的 parent n. 双亲，父代 connection n. 连接(法) connectivity n. 连通性，联络性 translation n. 翻译，变换，平移 foreground n. 前台 preserve vt. 保存，维持 vice n. 缺点，毛病，错误 necessarily ad. 必定，当然 circle n. 圆，圈，循环，周期 differ vi. 不同，不一致 stationary a. 静止的，平稳的 extract vt. 抽取，摘录，开方 unrecognized a. 未被认出的 thereafter ad. 此后，据此 inverse a. 反向的，逆的 spell v. 拼写 limiting n. (电路参数)限制处理 restructure vt. 调整，重新组织 delimit vt. 定界，定义 separately ad. 分别地 classify vt. 分类，分级 interfere vi. 干涉，干扰，冲突 individually ad. 个别地，单独地 vertical a. 垂直的，立(式)的 undesirable a. 不合乎需要的 lot n. 一块(批，组，套) piece n. 一块，部分，段 unlike a. 不象的，不同的 sit v. 位于，安装 insufficient a. 不足的，不适当的 map n. &amp; vt. 图；映射，变址 figure n. 数字；图，图形，形状 detect vt. 检测 convenience n. 方便，便利 mean n. &amp; vt. 平均；意味着 pacific a. 平稳的，太平(洋)的 emphasize v. 强调，着重，增强 department n. 部门，门类，系 forced a. 强制的，压力的 permanent a. 永久的 remark n. 评注，备注 away ad. 离开，(去)掉 concatenate vt. 连接，串联，并置 additionally ad. 另外，又 emulate v. 仿真，模仿；赶上或超过 tape n. 磁带，纸带 accidentally ad. 偶然地 concept n. 概念 optimize v. 优选，优化 counter n. 计数器，计算器 expect vt. 期望，期待，盼望 subsequently ad. 其后，其次， 按着 registration n. 登记，挂号，读 数 1108. designate vt. 任命，标志 1110. consult v. 咨询，顾问 1111. completely ad. 十分，完全，彻 底 1112. virtually ad. 实际上 1113. substantially ad. 实质上，本质 上 1114. specialize v. (使)专门化 1116. primarily ad. 首先，起初，原来 1117. sequentially ad. 顺序地 1123. commercial a. 商业的，经济的 1126. sheet n. (图)表，纸，片 1127. employee n. 雇员 1129. qualified a. 合格的，受限制的 1131. involved a. 有关的 1132. conditional a. 有条件的 1133. halfway a. 中途的，不彻底的 1134. oriented a. 有向的，定向的 1137. suppressed vt. 抑制，取消 1138. subroutine n. 子程序 1139. bracketed a. 加括号的 1140. manually ad. 用手，手动地 1141. preset vt. 预置 1142. autoindex n. 自动变址(数) 1143. restrict vt. 约束，限制 1144. performance n. 性能，实绩 1145. showing n. 显示，表现 1146. ever ad. 在任何时候，曾经 1147. distribution n. 分布，分配 1148. denote vt. 指示，意味着，代表 1150. repeatedly ad. 重复地 1151. replicate vt. 重复，复制 1152. mega n. 兆，百万 1153. conform vi. 遵从，符合 1154. rebuild v. 重建，修复，改造 1155. certainty n. 必然，确实 1156. controller n. 控制器 1157. pseudo a. 假的，伪的，冒充的 1158. manage v. 管理，经营，使用 1160. ensemble n. 总体，集合体 1162. allowable a. 容许的，承认的 1163. limitations n. 限制，边界 1164. restriction n. 限制，约束，节流 1165. remainder n. 余数，余项，剩余 1166. traverse v. 横渡，横过，横断 1168. resulting a. 结果的，合成的 1170. external a. 外部的 1171. adequate a. 足够的，充分的 1172. interpretability n. 配合动作性 1173. vary v. 变化，变换 1174. gap n. 间隙，间隔，缝隙 1175. indexing n. 变址，标引，加下标 1178. insertion n. 插入，嵌入，插页 1179. intervene vi 插入，干涉 1181. overflow v. 溢出，上溢 1182. charge n. 电荷，充电，负荷 1184. virtual a. 虚(拟)的，虚拟 1185. compose v. 组成，构成，构图 1186. snapshot n. 抽点打印 1187. sensitivity n. 灵敏度 1188. familiar a. 熟悉的，惯用的 1189. mach n. 马赫(速度单位) 1190. incorrect a. 错误的，不正确的 1191. subsequent a. 后来的，其次的 1192. capitalized a. 大写的 1193. compact a. 紧致的，压缩的 1194. plain n. 明码 1195. noted a. 著名的 1196. desirable a. 所希望的，称心的 1197. substitution n. 代替，替换，置换 1198. consume v. 消耗，使用 1199. keyed a. 键控的 1200. overstrike n. 过打印 1201. tornado n. 旋风，龙卷风 1202. quotation n. 引证，引用(句) 1203. ones n. 二进制反码 1204. parse vt. (语法)分析 1205. experience vt. &amp; n. 试验 1206. manufacture vt. &amp; n. 制造(业)，工业 1209. twentieth n. &amp; a. 第二十(的) 1210. understanding n. &amp; a. 了解的，聪明的 1212. restricting n. &amp; a. 限制(的) 1213. fancy n. &amp; a. 想象(的)，精制的 1214. wide a. &amp; ad. 宽的，广阔的 1215. fine a. &amp; ad. 微小的，细的 1216. worry v. &amp; n. (使)烦恼 1217. somewhat pron. &amp; ad. 稍微，有点 1218. quiet a. &amp; n. 静态，静止的 1219. purge v. &amp; n. 清除 1220. mod a. &amp; n. 时髦的 1221. numeral n. &amp; n. 数字的，数码 1222. whichever a. &amp; pron. 无论哪个 1223. purchase n. &amp; v. 购买 1224. endeavor n. &amp; v. 尽力，力图 1229. ellipsis n. 省略符号，省略(法) 1231. british a. &amp; n. 英国的；英国人 1233. custom a. &amp; n. 常规的，惯例；用户 1234. congratulation n. 祝贺 1235. protection n. 保护 1238. insure v. 保证，保障 1242. effort n. 工作，研究计划 1243. ampersand n. &amp;号(and) 1244. deal v. 处理，分配，交易 1245. power n. 功率，电源，幂 1246. difficulty n. 困难，难点 1247. proprietary a. 专有的 1248. aware a. 知道的，察觉到的 1249. numerous a. 为数众多的，无数 的 1253. vowel n. 元音，母音 1254. closely a. 精密地，仔细地 1255. accuracy n. 精确度，准确度 1256. traditional a. 传统的，惯例的 1257. synchronization n. 同步 1258. fragment n. 片段，段，分段 1259. primary a. 原始的，主要的 1260. habit n. 习惯 1261. comprise vt. 包括，由…组成 1262. landler n. 兰德勒舞曲 1263. absence n. 缺少，没有 1264. revolutionize vt. 变革，彻底改革 1266. constantly ad. 不变地，经常地 1267. seldom ad. 不常，很少，难得 1268. unfortunately ad. 不幸，遗憾地 1269. expunge vt. 擦除，删掉 1270. contrast n. 反差，对比，对比度 1271. invent vt. 创造，想象 1272. undone a. 未完成的 1273. unshift v. 未移动，不移档 1274. complex a. &amp; n. 复杂的；复数 1275. complexity n. 复杂性，复杂度 1276. creation n. 创造，创作 1277. unknown a. 未知的，无名的 1278. greatly ad. 大大地，非常 1279. cost n. 值，价值，成本 1280. degrade v. 降低，减少，递降 1281. suggestion n. 暗示，提醒 1282. real n. 实数，实的，实型 1283. experimentation n. 实验(工作， 法) 1285. experiment n. 实验，试验(研究) 1286. substantial a. 实质的，真正的 1287. solely ad. 独自，单独，只 1288. announce vt. 发表，宣布 1289. squeeze v. 挤压 1290. distribute vt. 分布，配线，配给 1291. negate vt. 否定，求反，“非” 1292. capture vt. 俘获，捕捉 1293. reinstate vt. 复原，恢复 1294. tutorial a. 指导的 1295. nicety n. 细节，精细 1296. exponent n. 指数，阶，幂 1297. exponential a. 指数的，幂的， 阶的 1300. prefer vt. 更喜欢，宁愿 1301. complicated v. 使复杂化，使混 乱 1302. reactivate v. 使恢复活动 1303. spread v. 展开，传播 1304. synchronize v. 使同步 1305. formation n. 构造，结构，形成 1306. widely ad. 广泛，很远 1307. comma n. 逗号“,”，逗点 1308. unnecessary a. 不必要的，多余 的 1309. unchanged a. 不变的 1310. cross n. 交叉，十字准线 1311. yet ad. 还，仍然，至今 1312. inexperienced a. 不熟练的，外 行的 1313. noninteractive a. 不相关的，非 交互的 1314. unwanted a. 不需要的，多余的 1315. unused a. 不用的，空着的 1316. unmarked a. 没有标记的 1317. dearly ad. 极，非常，昂贵地 1318. extremely ad. 极端地，非常 1319. hardly ad. 几乎不，未必 1320. placement n. 布局 1321. think v. 考虑，以为，判断 1322. technical a. 技术的，专业的 1323. idea n. 思想，观念 1324. stamp n. 图章 1325. indirectly ad. 间接地 1326. equation n. 方程，方程式 1327. smooth v. &amp; a. 平滑；平滑的 1328. attached a. 附加的 1329. discard v. 删除，废除，放弃 1330. never ad. 决不，从来不 1331. initiate vt. 开创，起始 1332. powerful a. 强大的，大功率的 1333. purpose n. &amp; vt. 目的，用途；打算 1336. regard vt. 考虑，注意，关系 1337. possibly ad. 可能地，合理地 1338. potentially ad. 可能地，大概地 1339. moreover ad. 况且，并且，此外 1340. guard v. &amp; n. 防护；防护装置 1341. world n. 世界，全球 1342. independent a. 独立的 1343. independently a. 独立地 1344. continuously ad. 连续不断地 1345. shield v. 屏蔽，罩，防护 1346. glance n. 闪烁 1347. happening n. 事件，偶然发生的事 1348. transaction n. 事项，事务，学 1349. emulation n. 仿真，仿效 1350. strike v. 敲，击 1351. dump v. (内存信息)转储 1352. occasionally ad. 偶尔(地)，不时 1353. tension n. 张力 1354. probable a. 概率的，可能的 1355. talent n. 才能，技能，人才 1356. financial a. 财务的，金融的 1357. meter n. 仪表，米 1358. logged a. 记录的，浸透的 1359. ware n. 仪器，商品 1360. disregard vt. 轻视，把…忽略不 计 1361. waiting a. 等待的 1362. preceding a. 先的，以前的 1363. comparison n. 比较，对照 1364. advanced a. 先进的，预先的 1365. rate n. 比率，速率，费率 1366. fly v. 飞，跳过 1367. programmable a. 可编程的 1368. definable a. 可定义的，可确定的 1369. readable a. 可读的 1370. recoverable a. 可恢复的，可回收的 1371. possibility n. 可能性 1372. finisher n. 成品机 1373. applicable a. 可适用的，合适的 1374. printable a. 可印刷的 1375. executable a. 可执行的 1376. essentially ad. 实质上，本来 1377. confuse vt. 使混乱，干扰 1378. familiarize vt. 使熟悉，使通俗化 1379. employe vt. 使用，花费 1380. suitable a. 适合的，相适宜的 1381. generation n. (世)代，(发展)阶段 1382. quality n. 质量，性质，属性 1383. defective a. 故障的，有毛病的 1384. interpretable a. 彼此协作的 1385. interest n. 兴趣，注意，影响 1386. fourscore n. 八十 1387. procedural a. 程序上的 1388. phrase n. 短语，成语 1389. specifically ad. 特别地，逐一地 1390. penalty n. 惩罚，罚款，负担 1391. violate vt. 违犯，妨碍，破坏 1392. indefinitely ad. 无限地，无穷地 1395. wise a. 聪明的 1396. becoming a. 合适的，相称的 1397. equally ad. 相等地，相同地 1398. enjoy vt. 享受，欣赏，喜爱 1399. forth ad. 向前 1400. disappear vi. 消失 1401. crop v. 切，剪切 1402. diagonally ad. 斜(对) 1403. labeled a. 有标号的 1404. decision n. 判定，决定，决策 1405. effective a. 有效的 1406. significant a. 有效的，有意义的 1407. avail v. &amp; n. 有益于；利益 1408. hang v. 中止，暂停，挂起 1409. craze n. &amp; v. 裂纹开裂 1410. consequently ad. 因此，从而 1411. introduce vt. 引进，引导 1412. team n. 队，小组 1413. visual a. 视觉的，直观的 1414. acknowledgment n. 接收(收妥)，承认 1415. efficiently ad. 有效地 1416. predict vt. 预测，预言 1417. anticipate vt. 预先考虑，抢…先 1418. bypass n. 旁路 1420. natural a. 自然的 1421. grant vt. 允许，授权 1422. logarithm n. 对数 1423. reappears vi. 再现，重现 1424. reload vt. 再装入 1425. occupy vt. 占有，充满 1426. photograph n. 照片；v.照相 1427. terminating n. 终止，终结，收信 1428. resolve v. 分辨，解像 1429. unsafe v. 恢复 1430. separator n. 分隔符 1431. hierarchical a. 分级的，分层的 1432. assortment n. 种类，花色品种 1433. growing n. 分类，分组，成群 1434. discussion n. 讨论，商议，论述 1435. alphabet n. 字母，字母表 1436. scattered a. 分散的 1437. eventually ad. 终于，最后 1438. finally ad. 终于，最后 1439. subgroup n. 分组，子群 1440. superimpose vt. 重叠，叠加 1441. reorganization vt. 重排，改组 1442. rewrite vt. 重写，再生 1443. university n. (综合性)大学 1444. deter vt. 阻止，拦住，妨碍 1445. pool n. &amp; v. 池，坑；共享 1446. moment n. 矩，力矩，磁矩 1447. shut v. 关闭 1448. closed a. 关闭的，闭迹 1449. respond v. 回答，响应 1450. repeating n. 重复，循环 1451. repetitive a. 重复的 1452. reenter v. 重新进入 1453. rearrange v. 重新整理，重新排序 1454. rectangular a. 矩形的，成直角的 1455. tag n. 特征，标记，标识符 1456. suppose v. 假定，推测 1457. supposed a. 假定的，推测的 1458. manipulating v. 操纵，操作 1459. operator n. 操作员，运算符 1460. masking n. 掩蔽，屏蔽 1461. demonstrate v. 论证，证明，证实 1463. importance n. 价值，重要 1465. overall a. 总共的，全部的 1466. turnkey n. 总控钥匙 1467. restricted a. 受限制的，受约束的 1468. suspension n. 暂停，中止，挂起 1469. seamless a. 无缝的 1470. clipper n. 限幅器，钳位器 1471. unsigned a. 无符号的 1472. unformatted a. 无格式的 1473. infinite a. 无限的，无穷的 1474. limiter n. 限制(幅)器 1475. mountain n. 高山，山脉 1476. redundant a. 冗余的 1477. dependent a. 相关的 1478. contiguous a. 相连的，邻接的 1479. multiprocessing n. 多重处理，多道处理 1480. architecture n. 结构，构造 1481. structural a. 结构的，结构上的 1482. outcome n. 结果，成果，输出 1483. association n. 结合，协会，联想 1484. opinion n. 意见，见解，判断 1485. interpret v. 解释 1486. explanatory a. 解释(性)的 1487. assemble v. 汇编，装配 1488. assembler n. 汇编程序 1489. cad 计算机辅助设计 1490. arithmetic n. 算术，运算 1491. varying a. 变化的，可变的 1492. representative a. 典型的，表示的 1493. typical a. 典型的，标准的 1494. sufficient a. 充足的，足够的 1495. blast v. &amp; n. 清除；爆炸 1496. clean a. 清洁的，干净的 1497. caret n. 插入符号 1498. socket n. 插座，插孔，插口 1499. stated a. 规定的 1500. protocol n. 规约，协议，规程 1501. presence n. 存在，有 1502. equipment n. 设备，装备，仪器 1504. lending n. &amp; a. 借给，出租；借出的 1505. book n. 书，手册，源程序块 1506. circumstances n. 情况，环境，细节 1507. situation n. 情况，状况，势态 1508. please v. 请 1509. mixture n. 混合物 1510. representation n. 表示 1511. esoteric a. 深奥的，奥秘的 1512. depth n. 深度，浓度(颜色的) 1513. physically a. 物理上，实际上 1514. aid n. 帮助，辅助程序 1515. successive a. 逐次的，相继的 1516. succession n. 逐次性，连续性 1517. unpack v. 拆开，卸，分开 1518. chunk n. 厚块，大部分 1519. alignment n. 序列，成直线 1520. typewriter n. 打字机 1521. tone n. 音调，音色，色调 1522. sensitive a. 敏感的，灵敏的 1523. reduction n. 减化，还原，减少 1524. indentation n. 缩进，缩排 1525. terminology n. 术语 1526. ascending a. 增长的，向上的 1527. augment v. 增加，添加，扩充 1528. increment n. 增量，加１，递增 1529. gain n. 增益(系数) 1530. obsolete a. 作废的，过时的 1531. accommodate v. 调节，适应 1532. motif n. 主题，要点，特色 1533. differentiate v. 区别，分辨 1534. distinction n. 区别，相异，特性 1535. distinguish v. 区别，辨识 1536. locking n. 锁定，加锁 1537. progress n. 进度，进展 1538. fundamental a. 基本的，根本的 1539. basis n. 基础，座 1540. underlying a. 基础的，根本的 1541. sound n. 声音，音响 1542. vital a. 生动的，不可缺少的 1543. national a. 国家的 1544. agree v. 符合，相同 1545. iterative a. 迭代的 1546. inclusive a. 包括的，内含的 1547. charm n. 吸引力 1548. course n. 过程，航向，课程 1549. exceeded a. 过度的，非常的 1550. numerical a. 数量的，数字的 1551. digital a. 数字的 1552. combo n. 二进位组合码 1553. cord n. 绳子，电线]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FNetty%2Fnetty%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class NioServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup=new NioEventLoopGroup(1); EventLoopGroup workGroup=new NioEventLoopGroup(); ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childOption(ChannelOption.TCP_NODELAY,true) .childAttr(AttributeKey.newInstance("childattr"),"childvalue") .handler(new MyHandler()) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; //socketChannel.pipeline().addLast() &#125; &#125;); ChannelFuture future=serverBootstrap.bind(9000).sync(); future.channel().closeFuture(); bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); /** * 反射创建服务端new channel * .channel newSocket()通过jdk来创建地城jdk channek * nioserversocketchannelconfig tcp参数配置 * abstractniochannel() * configureblocking(false) * abstracchannel id unsafe pipeline */ /** * 初始化服务端init channel * bind * initandregister * newchannel * init */ /** * chanel 注册到register selector * abstractchannel @ register() * this.eventloop=eventloop * register0 实际注册 * doregister jdk底层注册 * invokehandleraddedifneeded * firechannelregistered() */ /** * 端口绑定 do bind * abstracunsafe.bind * dobind * javachannel.bind jdk底层绑定 * pipeline。firechannelactive 传播 *headcontext.readifisautoread */ &#125; static class MyHandler extends ChannelInboundHandlerAdapter&#123; public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("register"); &#125; public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; &#125; public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("active"); &#125; &#125;&#125; 默认 netty服务端起多少 线程何时启动？ 线程组默认 cpu*2 MultithreadEventExecutorGroup @ new threadpertaskexecutor 每次执行任务都会创建一个线程实体 for new Child 构造nioeventloop 保存线程执行器threadpertaskexecutor 创建mpscqueue接受外部线程去执行 创建一个selector choosefactory.choose nioeventloop[] 绑定 nioeventloop run() run-&gt;while(true) select 检查是否有io事件 ​ deadline以及任务穿插逻辑处理 ​ 阻塞式select ​ 避免jdk空轮询bug 次数多了就替换 processselectedkeys 处理io事件 ​ openselector 事件轮询器 ​ selected keyset优化 实际使用数组实现 替换selectedkeyset ​ processelectedkeyoptimized runalltasks 处理异步任务队列 netty如何保证异步串行无锁化？]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2FUnix%20IO%2F</url>
    <content type="text"><![CDATA[IO模型Unix 有五种 IO 模型： 阻塞式 IO（BIO） 非阻塞式 IO（NIO） 异步 IO（AIO） IO 复用（select 和 poll） 信号驱动式 IO（SIGIO） 阻塞式 I/O应用进程被阻塞，直到数据复制到应用进程缓冲区中才返回。 应该注意到，在阻塞的过程中，其它程序还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其他程序还可以执行，因此不消耗 CPU 时间，这种模型的 CPU 利用率效率会比较高。 非阻塞式 I/O应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）。 由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率是比较低的。 IO 复用使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。 它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。 如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。并且相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。 信号驱动 IO应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。 相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。 异步 IO应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。 异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。 同步 I/O 与异步 I/O 同步 I/O：应用进程在调用 recvfrom 操作时会阻塞。 异步 I/O：不会阻塞。 阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O 都是同步 I/O，虽然非阻塞式 I/O 和信号驱动 I/O 在等待数据阶段不会阻塞，但是在之后的将数据从内核复制到应用进程这个操作会阻塞。 IO 复用select/poll/epoll 都是 IO 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 select1int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义。 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 poll1int poll(struct pollfd *fds, unsigned int nfds, int timeout); 使用链表实现。 比较功能select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。 select 会修改描述符，而 poll 不会； select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 的描述符类型使用链表实现，没有描述符数量的限制； poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。 速度select 和 poll 速度都比较慢。 select 和 poll 每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。 select 和 poll 的返回结果中没有声明哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程都需要使用轮询的方式来找到 I/O 完成的描述符。 可移植性几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 epoll123int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。 工作模式epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。 LT 模式当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 ET 模式和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。 很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。 select 应用场景select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时要求更高的场景，比如核反应堆的控制。 select 可移植性更好，几乎被所有主流平台所支持。 poll 应用场景poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且epoll 的描述符存储在内核，不容易调试。 epoll 应用场景只需要运行在 Linux 平台上，并且有非常大量的描述符需要同时轮询，而且这些连接最好是长连接。 补充阅读Linux IO 模式及 select、poll、epoll 详解]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2FIDEA%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[Ctrl+N，查找类打开类Ctrl+Shift+N，查找文件打开文件Ctrl+Alt+T，可以把代码包在一个块内，例如：try/catchCtrl+Alt+L，格式化代码Shift+Shift搜索文件Ctrl+Alt+H，查看方法在哪儿被调用Ctrl+X，删除行Ctrl+D，复制行Ctrl+Alt+←/→，返回至上次浏览的位置Alt+←/→，切换tab页视图Alt+↑/↓，切换方法Ctrl+Y，删除当前行Ctrl+U，转到父类]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHadoop%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Hbase 《简明 HBase 入门教程（开篇）》 《深入学习HBase架构原理》 《Hbase与传统数据库的区别》 空数据不存储，节省空间，且适用于并发。 《HBase Rowkey设计》 rowkey 按照字典顺序排列，便于批量扫描。 通过散列可以避免热点。 字典表压缩数据求交集]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2FGit%2F</url>
    <content type="text"><![CDATA[工作流程 Workspace：工作空间 Index：暂存区 Repository：本地仓库 Remote：远程仓库 命令 HEAD始终指向当前所处分支的最新的提交点 配置查看配置：git config –global –list git config –global user.name “mason” git config –global user.email “mason@gmail.com“ commit git commit -m \ 提交暂存区到本地仓库,message代表说明信息 git commit \ -m \ 提交暂存区的指定文件到本地仓库 git commit –amend -m \ 使用一次新的commit，替代上一次提交 branch git branch 列出所有本地分支 git branch -r 列出所有远程分支 git branch -a 列出所有本地分支和远程分支 git branch \/git branch newbranch branch 新建一个分支，但依然停留在当前分支 git checkout -b \ 新建一个分支，并切换到该分支 git branch –track ？？？？？？？ 新建一个分支，与指定的远程分支建立追踪关系 git checkout \ 切换到指定分支，并更新工作区 git branch -d \ 删除分支 git push origin –delete ？？？？？？？ 删除远程分支 git branch -vv 查看当前分支 git branch -m br1 newbranch 分支重命名 pushssh-keygen -t rsa -C “mason@gmail.com“ github -&gt; setting -&gt; SSH and GPG keys 新增 id_rsa.pub 的内容 git remote add origin git@github.com:maruami/test.git 把本地库与远程库关联 git push -u origin master 第一次推送时 git push –set-upstream origin newbranch 提交新分支 loggit log 显示当前分支的版本历史 git log –pretty=oneline 信息横向显示 git log -p （显示版本之间的代码差异） git log 7b1558c （指定提交名称前7位） 其他git mv oldfile newfile 文件重命名与移动]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FBasic%2FCSS%E9%80%89%E6%8B%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[表达式 说明 li a 所有li 下的所有a节点 ul + p ul 后面的的第一个p元素 div#pic &gt; ul id为pic的div的第一个ul元素 ul~p 与ul相邻的所有p元素 a[title] 所有有title属性的a元素 a[href=”/upload”] 所有href属性为/upload的a元素 a[href=”^www”] 所有href属性以www开头的a元素 a[href=”$com”] 所有href属性以com结尾的a元素 input[type=radio]:checked 选中的radio元素 div:not(#pic) id不是pic的div属性 li:nth-child(3) 选取第三个li元素 li:nth-child(2n) 选取偶数个li]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringCloud%2F%E9%A1%B5%E9%9D%A2%E9%9D%99%E6%80%81%E5%8C%96%2F</url>
    <content type="text"><![CDATA[@RequestMapping(&quot;/{id}&quot;) public String toHtml(@PathVariable Long id, Model model){ System.out.println(&quot;来过&quot;); Map&lt;String,Object&gt; map=new HashMap&lt;&gt;(); map.put(&quot;message&quot;,&quot;hello&quot;); updateToNginxPath(map,&quot;good&quot;,&quot;good&quot;+id); model.addAllAttributes(map); return &quot;good&quot;; } public void updateToNginxPath(Map map,String templateName,String localName){ PrintWriter writer = null; try { Context context = new Context(); context.setVariables(map); File file = new File(&quot;E:\\nginx-1.15.12\\nginx-1.15.12\\html\\&quot; + localName + &quot;.html&quot;); writer = new PrintWriter(file); templateEngine.process(templateName, context, writer); } catch (Exception e) { e.printStackTrace(); } finally { if (writer != null) { writer.close(); } } }]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringCloud%2Fspringboot%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[java配置java配置主要靠java类和一些注解，比较常用的注解有： @Configuration：声明一个类作为配置类，代替xml文件 @Bean：声明在方法上，将方法的返回值加入Bean容器，代替&lt;bean&gt;标签 @value：属性注入 @PropertySource：指定外部属性文件， 我们接下来用java配置来尝试实现连接池配置： 首先引入Druid连接池依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 创建一个jdbc.properties文件，编写jdbc属性： 1234jdbc.driverClassName=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/leyoujdbc.username=rootjdbc.password=123 然后编写代码： 1234567891011121314151617181920212223@Configuration@PropertySource("classpath:jdbc.properties")public class JdbcConfig &#123; @Value("$&#123;jdbc.url&#125;") String url; @Value("$&#123;jdbc.driverClassName&#125;") String driverClassName; @Value("$&#123;jdbc.username&#125;") String username; @Value("$&#123;jdbc.password&#125;") String password; @Bean public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); dataSource.setUsername(username); dataSource.setPassword(password); return dataSource; &#125;&#125; 解读： @Configuration：声明我们JdbcConfig是一个配置类 @PropertySource：指定属性文件的路径是:classpath:jdbc.properties 通过@Value为属性注入值 通过@Bean将 dataSource()方法声明为一个注册Bean的方法，Spring会自动调用该方法，将方法的返回值加入Spring容器中。 然后我们就可以在任意位置通过@Autowired注入DataSource了！ 我们在HelloController中测试： 1234567891011@RestControllerpublic class HelloController &#123; @Autowired private DataSource dataSource; @GetMapping("hello") public String hello() &#123; return "hello, spring boot!" + dataSource; &#125;&#125; 然后Debug运行并查看： 属性注入成功了！ SpringBoot的属性注入在上面的案例中，我们实验了java配置方式。不过属性注入使用的是@Value注解。这种方式虽然可行，但是不够强大，因为它只能注入基本类型值。 在SpringBoot中，提供了一种新的属性注入方式，支持各种java基本数据类型及复杂类型的注入。 1）我们新建一个类，用来进行属性注入： 123456789@ConfigurationProperties(prefix = "jdbc")public class JdbcProperties &#123; private String url; private String driverClassName; private String username; private String password; // ... 略 // getters 和 setters&#125; 在类上通过@ConfigurationProperties注解声明当前类为属性读取类 prefix=&quot;jdbc&quot;读取属性文件中，前缀为jdbc的值。 在类上定义各个属性，名称必须与属性文件中jdbc.后面部分一致 需要注意的是，这里我们并没有指定属性文件的地址，所以我们需要把jdbc.properties名称改为application.properties，这是SpringBoot默认读取的属性文件名： 2）在JdbcConfig中使用这个属性： 1234567891011121314@Configuration@EnableConfigurationProperties(JdbcProperties.class)public class JdbcConfig &#123; @Bean public DataSource dataSource(JdbcProperties jdbc) &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(jdbc.getUrl()); dataSource.setDriverClassName(jdbc.getDriverClassName()); dataSource.setUsername(jdbc.getUsername()); dataSource.setPassword(jdbc.getPassword()); return dataSource; &#125;&#125; 通过@EnableConfigurationProperties(JdbcProperties.class)来声明要使用JdbcProperties这个类的对象 然后你可以通过以下方式注入JdbcProperties： @Autowired注入 12@Autowiredprivate JdbcProperties prop; 构造函数注入 1234private JdbcProperties prop;public JdbcConfig(Jdbcproperties prop)&#123; this.prop = prop;&#125; 声明有@Bean的方法参数注入 1234@Beanpublic Datasource dataSource(JdbcProperties prop)&#123; // ...&#125; 本例中，我们采用第三种方式。 大家会觉得这种方式似乎更麻烦了，事实上这种方式有更强大的功能，也是SpringBoot推荐的注入方式。两者对比关系： 优势： Relaxed binding：松散绑定 不严格要求属性文件中的属性名与成员变量名一致。支持驼峰，中划线，下划线等等转换，甚至支持对象引导。比如：user.friend.name：代表的是user对象中的friend属性中的name属性，显然friend也是对象。@value注解就难以完成这样的注入方式。 meta-data support：元数据支持，帮助IDE生成属性提示（写开源框架会用到）。 更优雅的注入事实上，如果一段属性只有一个Bean需要使用，我们无需将其注入到一个类（JdbcProperties）中。而是直接在需要的地方声明即可： 1234567891011@Configurationpublic class JdbcConfig &#123; @Bean // 声明要注入的属性前缀，SpringBoot会自动把相关属性通过set方法注入到DataSource中 @ConfigurationProperties(prefix = "jdbc") public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); return dataSource; &#125;&#125; 我们直接把@ConfigurationProperties(prefix = &quot;jdbc&quot;)声明在需要使用的@Bean的方法上，然后SpringBoot就会自动调用这个Bean（此处是DataSource）的set方法，然后完成注入。使用的前提是：该类必须有对应属性的set方法！ 我们将jdbc的url改成：/heima，再次测试：]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringCloud%2Fspringboot%2F</url>
    <content type="text"><![CDATA[springboot添加拦截器 1234567891011121314151617181920212223242526272829public class Interceter implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; System.out.println(&quot;hello&quot;); return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; &#125;&#125;@Configurationpublic class MvcConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry interceptorRegistry) &#123; interceptorRegistry.addInterceptor(new Interceter()).addPathPatterns(&quot;/**&quot;); &#125; &#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Eureka]]></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringCloud%2FEureka%2F</url>
    <content type="text"><![CDATA[当ribbon与eureka联合使用时ribbon的服务配置清单ribbonserverlist会被discoveryEnabledNIWSServerlist重写，扩展成从eureka注册中心获取服务端列表 服务治理机制 注册中心互相注册组成高可用集群 失效剔除 eureka会创建一个定时任务，默认每隔一段时间当清单中超时的没有预约的服务剔除 自我保护 服务再注册是会维护一个心跳连接，eureka会将实例注册信息保存起来,让这些实例不过期，客户端必须要有容错机制，如请求重试，断路器等等 服务提供者 启动的时候会发送rest请求将自己注册到eureka上，同时带上自身服务的一些元数据，eureka接受到rest请求后，将元数据信息存储再一个双层map，第一层key是服务名，第二层key是具体服务的实例名 服务同步 服务提供者注册到不同的服务中心，提供者注册时会将请求装发到集群中其他的注册中心，实现同步 服务续约 注册完服务后，服务提供者会维护一个心跳，防止eureka将服务实例从服务列表中剔除 服务获取 启动消费者时，发送一个rest请求给注册中心，来获取上面注册的服务清单，eureka每隔一段时间返回给客户端 服务调用 消费端获取清单后，通过服务名可以获取具体提供服务的实例名和元数据信息，ribbon会默认采用轮询的方式进行调用]]></content>
      <categories>
        <category>SpringCloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot配置]]></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringBoot%2FSpringBoot%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Spring方式@Configuration 声明一个配置类相当于xml @Bean 将方法的返回值加入Bean容器，代理bean标签 @Value属性注入 @PropertySource指定外部属性文件 123456789101112131415161718192021222324@Configuration@PropertySource(&quot;classpath:jdbc.properties&quot;)public class DruidConfig &#123; @Value(&quot;$&#123;jdbc.url&#125;&quot;) String url; @Value(&quot;$&#123;jdbc.driver-class-name&#125;&quot;) String driverClassName; @Value(&quot;$&#123;jdbc.username&#125;&quot;) String username; @Value(&quot;$&#123;jdbc.password&#125;&quot;) String password; @Bean public DataSource dataSource()&#123; DruidDataSource dataSource=new DruidDataSource(); dataSource.setUsername(username); dataSource.setPassword(password); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); return dataSource; &#125;&#125; SpringBoot 方式适合多个使用 12345678@ConfigurationProperties(prefix = &quot;jdbc&quot;)@Datapublic class ConfigProperties &#123; String url; String driverclassname; String username; String password;&#125; 123456789101112131415@Configuration@EnableConfigurationProperties(ConfigProperties.class)public class DruidConfig &#123; @Bean public DataSource dataSource(ConfigProperties configProperties)&#123; DruidDataSource dataSource=new DruidDataSource(); dataSource.setUsername(configProperties.getUsername()); dataSource.setPassword(configProperties.getPassword()); dataSource.setUrl(configProperties.getUrl()); dataSource.setDriverClassName(configProperties.getDriverclassname()); return dataSource; &#125;&#125; 适合单个使用 123456789@Configurationpublic class DruidConfig &#123; @Bean @ConfigurationProperties(prefix = &quot;jdbc&quot;) public DataSource dataSource()&#123; return new DruidDataSource(); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2Frabbitmq%20jwt%2F</url>
    <content type="text"><![CDATA[@Controller @RequestMapping(&quot;/good&quot;) public class GoodController { @Autowired TemplateEngine templateEngine; @Resource private AmqpTemplate amqpTemplate; @RequestMapping(&quot;/message/{type}/{msg}&quot;) @ResponseBody public String testSend(@PathVariable String type,@PathVariable String msg) throws InterruptedException { // this.amqpTemplate.convertAndSend(&quot;spring.test.exchange&quot;,&quot;a.b&quot;, msg); this.amqpTemplate.convertAndSend(&quot;good.exchange&quot;,&quot;good.&quot;+type, msg); return &quot;&quot;; } @RequestMapping(&quot;/{id}&quot;) public String toHtml(@PathVariable Long id, Model model){ System.out.println(&quot;来过&quot;); Map&lt;String,Object&gt; map=new HashMap&lt;&gt;(); map.put(&quot;message&quot;,&quot;hello&quot;); updateToNginxPath(map,&quot;good&quot;,&quot;good&quot;+id); model.addAllAttributes(map); return &quot;good&quot;; } public void updateToNginxPath(Map map,String templateName,String localName){ PrintWriter writer = null; try { Context context = new Context(); context.setVariables(map); File file = new File(&quot;E:\\nginx-1.15.12\\nginx-1.15.12\\html\\&quot; + localName + &quot;.html&quot;); writer = new PrintWriter(file); templateEngine.process(templateName, context, writer); } catch (Exception e) { e.printStackTrace(); } finally { if (writer != null) { writer.close(); } } } } @Component public class Consumer { @RabbitListener(bindings = @QueueBinding( value = @Queue(value = &quot;add.queue&quot;, durable = &quot;true&quot;), exchange = @Exchange( value = &quot;good.exchange&quot;, ignoreDeclarationExceptions = &quot;true&quot;, type = ExchangeTypes.TOPIC ), key = &quot;good.add&quot;)) public void add(String msg){ System.out.println(&quot;add：&quot; + msg); } @RabbitListener(bindings = @QueueBinding( value = @Queue(value = &quot;delete.queue&quot;, durable = &quot;true&quot;), exchange = @Exchange( value = &quot;good.exchange&quot;, ignoreDeclarationExceptions = &quot;true&quot;, type = ExchangeTypes.TOPIC ), key = &quot;good.delete&quot;)) public void delete(String msg){ System.out.println(&quot;delete：&quot; + msg); } } spring.thymeleaf.mode=HTML5 spring.thymeleaf.cache=false spring.rabbitmq.host=192.168.170.128 spring.rabbitmq.username=hehe spring.rabbitmq.password=hehe spring.rabbitmq.virtual-host=/steambuy spring.rabbitmq.publisher-confirms=true @Cookievalue public class Test { public static void main(String[] args) { JwtBuilder claim = Jwts.builder().setId(&quot;12312&quot;).setSubject(&quot;dadaddadsad&quot;) .setIssuedAt(new Date()).signWith(SignatureAlgorithm.HS256, &quot;steambuy&quot;).setExpiration(new Date(new Date().getTime() + 60000)).claim(&quot;role&quot;, &quot;&quot;); System.out.println(claim.compact()); Claims steambuy = (Claims)Jwts.parser().setSigningKey(&quot;steambuy&quot;).parse(claim.compact()).getBody(); System.out.println(steambuy.getId()+steambuy.getSubject()); } } &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;joda-time&lt;/groupId&gt; &lt;artifactId&gt;joda-time&lt;/artifactId&gt; &lt;/dependency&gt;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2Fframe(1)%2F</url>
    <content type="text"><![CDATA[Mybatis缓存 一级缓存是SqlSession级别的缓存，缓存的数据只在SqlSession内有效,在操作数据库的时候需要先创建SqlSession会话对象，在对象中有一个HashMap用于存储缓存数据，此HashMap是当前会话对象私有的 二级缓存是mapper级别的缓存，同一个namespace公用这一个缓存，所以对SqlSession是共享的；使用 LRU 机制清理缓存，通过 cacheEnabled 参数开启。 1.当一个sqlseesion执行了一次select后，在关闭此session的时候，会将查询结果缓存到二级缓存 2.当另一个sqlsession执行select时，首先会在他自己的一级缓存中找，如果没找到，就回去二级缓存中找，找到了就返回，就不用去数据库了，从而减少了数据库压力提高了性能 Sqlsessionfactorybean实现了FactoryBean和InitializingBean接口 FactoryBean 一旦某个bean实现次接口，那么通过getbean方法获取bean时，其实时获取此类的getObject()返回的实例 InitializingBean 会在初始化时调用其afterPropertiesSet方法来进行bean的逻辑初始化 MapperFactoryBeanMapperFactoryBean初始化 ，在DaoSupport类中实现 checkDaoConfig 对Dao配置的验证 initDao 对dao的初始工作，initdao时模板方法]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpring%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[可能是最漂亮的Spring事务管理详解 Spring编程式和声明式事务实例讲解 Spring 事务实现方式 https://blog.csdn.net/liaohaojian/article/details/70139151 事务管理机制Spring并不直接管理事务，而是提供了多种事务管理器，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。Spring事务管理器的接口是org.springframework.transaction.PlatformTransactionManager，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了 PlatformTransactionManager接口 12345public interface PlatformTransactionManager &#123; TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;&#125; jdbc事务 123&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;/bean&gt; JPA 123&lt;bean id="transactionManager" class="org.springframework.orm.jpa.JpaTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; jta 123&lt;bean id="transactionManager" class="org.springframework.transaction.jta.JtaTransactionManager"&gt; &lt;property name="transactionManagerName" value="java:/TransactionManager" /&gt; &lt;/bean&gt; 基本事务属性的定义TransactionDefinition 123456public interface TransactionDefinition &#123; int getPropagationBehavior(); // 返回事务的传播行为 int getIsolationLevel(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getTimeout(); // 返回事务必须在多少秒内完成 boolean isReadOnly(); // 事务是否只读，事务管理器能够根据这个返回值进行优化，确保事务是只读的&#125; 传播行为事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为： 传播行为 含义 PROPAGATION_REQUIRED 表示当前方法必须运行在事务中。如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务 PROPAGATION_SUPPORTS 表示当前方法不需要事务上下文，但是如果存在当前事务的话，那么该方法会在这个事务中运行 PROPAGATION_MANDATORY 表示该方法必须在事务中运行，如果当前事务不存在，则会抛出一个异常 PROPAGATION_REQUIRED_NEW 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NOT_SUPPORTED 表示该方法不应该运行在事务中。如果存在当前事务，在该方法运行期间，当前事务将被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NEVER 表示当前方法不应该运行在事务上下文中。如果当前正有一个事务在运行，则会抛出异常 PROPAGATION_NESTED 表示如果当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。如果当前事务不存在，那么其行为与PROPAGATION_REQUIRED一样。注意各厂商对这种传播行为的支持是有所差异的。可以参考资源管理器的文档来确认它们是否支持嵌套事务 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 （1）PROPAGATION_REQUIRED 如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。 首选的事务传播行为 123456//事务属性 PROPAGATION_REQUIREDmethodA&#123; …… methodB(); ……&#125; 1234//事务属性 PROPAGATION_REQUIREDmethodB&#123; ……&#125; 使用spring声明式事务，spring使用AOP来支持声明式事务，会根据事务属性，自动在方法调用之前决定是否开启一个事务，并在方法执行之后决定事务提交或回滚事务。 调用MethodA时环境中没有事务，所以开启一个新的事务.当在MethodA中调用MethodB时，环境中已经有了一个事务，所以methodB就加入当前事务 （2）PROPAGATION_SUPPORTS 如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行。 123456789//事务属性 PROPAGATION_REQUIREDmethodA()&#123; methodB();&#125;//事务属性 PROPAGATION_SUPPORTSmethodB()&#123; ……&#125; 单纯的调用methodB时，methodB方法是非事务的执行的。当调用methdA时,methodB则加入了methodA的事务中,事务地执行， （3）PROPAGATION_MANDATORY 如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常。 强制 必须的嵌在一个事务里执行，否则会抛异常 org.springframework.transaction.IllegalTransactionStateException: No existing transaction found for transaction marked with propagation ‘mandatory’ 123456789//事务属性 PROPAGATION_REQUIREDmethodA()&#123; methodB();&#125;//事务属性 PROPAGATION_MANDATORY methodB()&#123; ……&#125; （4）PROPAGATION_REQUIRES_NEW 总是开启一个新的事务。如果一个事务已经存在，则将这个存在的事务挂起。 A,B相互独立 1234567891011//事务属性 PROPAGATION_REQUIREDmethodA()&#123; doSomeThingA(); methodB(); doSomeThingB();&#125;//事务属性 PROPAGATION_REQUIRES_NEWmethodB()&#123; ……&#125; 调用A方法相当于 1234567891011121314151617181920212223242526272829main()&#123; TransactionManager tm = null; try&#123; //获得一个JTA事务管理器 tm = getTransactionManager(); tm.begin();//开启一个新的事务 Transaction ts1 = tm.getTransaction(); doSomeThing(); tm.suspend();//挂起当前事务 try&#123; tm.begin();//重新开启第二个事务 Transaction ts2 = tm.getTransaction(); methodB(); ts2.commit();//提交第二个事务 &#125; Catch(RunTimeException ex) &#123; ts2.rollback();//回滚第二个事务 &#125; finally &#123; //释放资源 &#125; //methodB执行完后，恢复第一个事务 tm.resume(ts1); doSomeThingB(); ts1.commit();//提交第一个事务 &#125; catch(RunTimeException ex) &#123; ts1.rollback();//回滚第一个事务 &#125; finally &#123; //释放资源 &#125;&#125; 在同一个Service类中，spring并不重新创建新事务，如果是两不同的Service，就会创建新事务了 spring boot 会自动装配一个 DataSourceTransactionManager 事务管理器 使用PROPAGATION_REQUIRES_NEW,需要使用 JtaTransactionManager作为事务管理器。 （5）PROPAGATION_NOT_SUPPORTED 总是非事务地执行，并挂起任何存在的事务。使用PROPAGATION_NOT_SUPPORTED,也需要使用JtaTransactionManager作为事务管理器。（代码示例同上，可同理推出） （6）PROPAGATION_NEVER 总是非事务地执行，如果存在一个活动事务，则抛出异常。 （7）PROPAGATION_NESTED如果一个活动的事务存在，则运行在一个嵌套的事务中. 如果没有活动事务, 则按PROPAGATION_REQUIRED 属性执行。使用PROPAGATION_NESTED，还需要把PlatformTransactionManager的nestedTransactionAllowed属性设为true;而 nestedTransactionAllowed属性值默认为false。 B事务依赖于A事务。A事务失败时，会回滚B事务所做的动作。而B事务操作失败并不会引起A事务的回滚。 如果单独调用methodB方法，则按REQUIRED属性执行。 1234567891011//事务属性 PROPAGATION_REQUIREDmethodA()&#123; doSomeThingA(); methodB(); doSomeThingB();&#125;//事务属性 PROPAGATION_NESTEDmethodB()&#123; ……&#125; 效果 1234567891011121314151617181920212223main()&#123; Connection con = null; Savepoint savepoint = null; try&#123; con = getConnection(); con.setAutoCommit(false); doSomeThingA(); savepoint = con2.setSavepoint(); try&#123; methodB(); &#125; catch(RuntimeException ex) &#123; con.rollback(savepoint); &#125; finally &#123; //释放资源 &#125; doSomeThingB(); con.commit(); &#125; catch(RuntimeException ex) &#123; con.rollback(); &#125; finally &#123; //释放资源 &#125;&#125; 当methodB方法调用之前，调用setSavepoint方法，保存当前的状态到savepoint。如果methodB方法调用失败，则恢复到之前保存的状态。这时的事务并没有进行提交，如果后续的代码(doSomeThingB()方法)调用失败，则回滚包括methodB方法的所有操作。 隔离级别事务的第二个维度就是隔离级别（isolation level）。隔离级别定义了一个事务可能受其他并发事务影响的程度。并发事务引起的问题在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty reads）——一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读（Nonrepeatable read）——不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了修改。 幻读（Phantom read）——幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）新增或者删除了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了或少了一些记录。 不可重复读与幻读的区别 对于前者, 只需要锁住满足条件的记录。对于后者, 要锁住满足条件及其相近的记录。 隔离级别 含义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. ISOLATION_READ_UNCOMMITTED 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 ISOLATION_READ_COMMITTED 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 ISOLATION_REPEATABLE_READ 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 ISOLATION_SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的 只读事务的第三个特性是它是否为只读事务。如果事务只对后端的数据库进行该操作，数据库可以利用事务的只读特性来进行一些特定的优化。通过将事务设置为只读，你就可以给数据库一个机会，让它应用它认为合适的优化措施。 事务超时为了使应用程序很好地运行，事务不能运行太长的时间。因为事务可能涉及对后端数据库的锁定，所以长时间的事务会不必要的占用数据库资源。事务超时就是事务的一个定时器，在特定时间内事务如果没有执行完毕，那么就会自动回滚，而不是一直等待其结束。 回滚规则事务五边形的最后一个方面是一组规则，这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚（这一行为与EJB的回滚行为是一致的）但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。 事务状态上面讲到的调用PlatformTransactionManager接口的getTransaction()的方法得到的是TransactionStatus接口的一个实现，这个接口的内容如下： 1234567public interface TransactionStatus&#123; boolean isNewTransaction(); // 是否是新的事物 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成&#125; 可以发现这个接口描述的是一些处理事务提供简单的控制事务执行和查询事务状态的方法，在回滚或提交的时候需要应用对应的事务状态。 Spring事务机制主要包括声明式事务和编程式事务，编程式事务允许用户在代码中精确定义事务的边界，而声明式事务（基于AOP）有助于用户将操作与事务规则进行解耦。 简单地说，编程式事务侵入到了业务代码里面，但是提供了更加详细的事务管理；而声明式事务由于基于AOP，所以既能起到事务管理的作用，又可以不影响业务代码的具体实现。实际开发中使用声明式事务比较多 编程式事务Spring提供两种方式的编程式事务管理，分别是：使用TransactionTemplate和直接使用PlatformTransactionManager。 声明式事务配置方式根据代理机制的不同，总结了五种Spring事务的配置方式，配置文件如下： （1）每个Bean都有一个代理 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd&quot;&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id=&quot;userDaoTarget&quot; class=&quot;com.bluesky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;userDao&quot; class=&quot;org.springframework.transaction.interceptor.TransactionProxyFactoryBean&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;property name=&quot;target&quot; ref=&quot;userDaoTarget&quot; /&gt; &lt;property name=&quot;proxyInterfaces&quot; value=&quot;com.bluesky.spring.dao.GeneratorDao&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; （2）所有Bean共享一个代理基类 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd&quot;&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;transactionBase&quot; class=&quot;org.springframework.transaction.interceptor.TransactionProxyFactoryBean&quot; lazy-init=&quot;true&quot; abstract=&quot;true&quot;&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id=&quot;userDaoTarget&quot; class=&quot;com.bluesky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;userDao&quot; parent=&quot;transactionBase&quot; &gt; &lt;property name=&quot;target&quot; ref=&quot;userDaoTarget&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; （3）使用拦截器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd&quot;&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;transactionInterceptor&quot; class=&quot;org.springframework.transaction.interceptor.TransactionInterceptor&quot;&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator&quot;&gt; &lt;property name=&quot;beanNames&quot;&gt; &lt;list&gt; &lt;value&gt;*Dao&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;interceptorNames&quot;&gt; &lt;list&gt; &lt;value&gt;transactionInterceptor&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id=&quot;userDao&quot; class=&quot;com.bluesky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; （4）使用tx标签配置的拦截器 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd&quot;&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package=&quot;com.bluesky&quot; /&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;interceptorPointCuts&quot; expression=&quot;execution(* com.bluesky.spring.dao.*.*(..))&quot; /&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;interceptorPointCuts&quot; /&gt; &lt;/aop:config&gt; &lt;/beans&gt; （5）全注解 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd&quot;&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package=&quot;com.bluesky&quot; /&gt; &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; &lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot; /&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; 此时在DAO上需加上@Transactional注解，如下： 12345678910111213141516171819package com.bluesky.spring.dao;import java.util.List;import org.hibernate.SessionFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.orm.hibernate3.support.HibernateDaoSupport;import org.springframework.stereotype.Component;import com.bluesky.spring.domain.User;@Transactional@Component(&quot;userDao&quot;)public class UserDaoImpl extends HibernateDaoSupport implements UserDao &#123; public List&lt;User&gt; listUsers() &#123; return this.getSession().createQuery(&quot;from User&quot;).list(); &#125; &#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring中的设计模式]]></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpring%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1. 简单工厂 又叫做静态工厂方法（StaticFactory Method）模式，但不属于23种GOF设计模式之一。 简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。 Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 2. 工厂方法（Factory Method） 定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。 Spring中的FactoryBean就是典型的工厂方法模式。如下图： 3. 单例（Singleton） 保证一个类仅有一个实例，并提供一个访问它的全局访问点。 Spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为Spring管理的是是任意的Java对象。 4. 适配器（Adapter） 将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 Spring中在对于AOP的处理中有Adapter模式的例子，见如下图： 由于Advisor链需要的是MethodInterceptor（拦截器）对象，所以每一个Advisor中的Advice都要适配成对应的MethodInterceptor对象。 5.包装器（Decorator） 动态地给一个对象添加一些额外的职责。就增加功能来说，Decorator模式相比生成子类更为灵活。 Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。基本上都是动态地给一个对象添加一些额外的职责。 6. 代理（Proxy） 为其他对象提供一种代理以控制对这个对象的访问。 从结构上来看和Decorator模式类似，但Proxy是控制，更像是一种对功能的限制，而Decorator是增加职责。 Spring的Proxy模式在aop中有体现，比如JdkDynamicAopProxy和Cglib2AopProxy。 7.观察者（Observer） 定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。 Spring中Observer模式常用的地方是listener的实现。如ApplicationListener。 8. 策略（Strategy） 定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。 Spring中在实例化对象的时候用到Strategy模式，见如下图： 在SimpleInstantiationStrategy中有如下代码说明了策略模式的使用情况： 9.模板方法（Template Method） 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 Template Method模式一般是需要继承的。这里想要探讨另一种对Template Method的理解。Spring中的JdbcTemplate，在用这个类时并不想去继承这个类，因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。怎么办？那我们就用回调对象吧。在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。这可能是Template Method不需要继承的另一种实现方式吧。 以下是一个具体的例子： JdbcTemplate中的execute方法： JdbcTemplate执行execute方法： 知识只有共享才能传播，才能推崇出新的知识，才能学到更多，这里写的每一篇文字/博客，基本都是从网上查询了一下资料然后记录下来，也有些是原滋原味搬了过来，也有时加了一些自己的想法]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringMVC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[SpringMVC 简单介绍SpringMVC 框架是以请求为驱动，围绕 Servlet 设计，将请求发给控制器，然后通过模型对象，分派器来展示请求结果视图。其中核心类是 DispatcherServlet，它是一个 Servlet，顶层是实现的Servlet接口。 SpringMVC 工作原理（重要）DispatcherServlet的执行链 流程说明（重要）： （1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 （2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 （3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 （4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。 （5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 （6）ViewResolver 会根据逻辑 View 查找实际的 View。 （7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。 （8）把 View 返回给请求者（浏览器） SpringMVC 重要组件说明1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供（重要） 作用：Spring MVC 的入口函数。接收请求，响应结果，相当于转发器，中央处理器。有了 DispatcherServlet 减少了其它组件之间的耦合度。用户请求到达前端控制器，它就相当于mvc模式中的c，DispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，DispatcherServlet的存在降低了组件之间的耦合性。 什么是DispatcherServlet？通过上面讲到的前端控制器模式，我们可以很轻易的知道DispatcherServlet是基于Spring的Web应用程序的中心点。它需要传入请求，并在处理程序映射，拦截器，控制器和视图解析器的帮助下，生成对客户端的响应。所以，我们可以分析这个类的细节，并总结出一些核心要点。 下面是处理一个请求时DispatcherServlet执行的步骤： 1. 策略初始化DispatcherServlet是一个位于org.springframework.web.servlet包中的类，并扩展了同一个包中的抽象类FrameworkServlet。它包含一些解析器的私有静态字段(用于本地化，视图，异常或上传文件)，映射处理器:handlerMapping和处理适配器:handlerAdapter(进入这个类的第一眼就能看到的)。DispatcherServlet非常重要的一个核心点就是是初始化策略的方法(protected void initStrategies（ApplicationContext context）)。在调用onRefresh方法时调用此方法。最后一次调用是在FrameworkServlet中通过initServletBean和initWebApplicationContext方法进行的(initServletBean方法中调用initWebApplicationContext，后者调用onRefresh(wac))。initServletBean通过所提供的这些策略生成我们所需要的应用程序上下文。其中每个策略都会产生一类在DispatcherServlet中用来处理传入请求的对象。 123456789101112131415161718192021222324&gt;/**&gt; * This implementation calls &#123;@link #initStrategies&#125;.&gt; */&gt;@Override&gt;protected void onRefresh(ApplicationContext context) &#123;&gt; initStrategies(context);&gt;&#125;&gt;&gt;/**&gt; * Initialize the strategy objects that this servlet uses.&gt; * &lt;p&gt;May be overridden in subclasses in order to initialize further strategy objects.&gt; */&gt;protected void initStrategies(ApplicationContext context) &#123;&gt; initMultipartResolver(context);&gt; initLocaleResolver(context);&gt; initThemeResolver(context);&gt; initHandlerMappings(context);&gt; initHandlerAdapters(context);&gt; initHandlerExceptionResolvers(context);&gt; initRequestToViewNameTranslator(context);&gt; initViewResolvers(context);&gt; initFlashMapManager(context);&gt;&#125;&gt; 2.请求预处理FrameworkServlet抽象类扩展了同一个包下的HttpServletBean，HttpServletBean扩展了javax.servlet.http.HttpServlet。点开这个类源码可以看到，HttpServlet是一个抽象类，其方法定义主要用来处理每种类型的HTTP请求：doGet（GET请求），doPost（POST），doPut（PUT），doDelete（DELETE），doTrace（TRACE），doHead（HEAD），doOptions（OPTIONS）。FrameworkServlet通过将每个传入的请求调度到processRequest(HttpServletRequest request，HttpServletResponse response)来覆盖它们。processRequest是一个protected和final的方法，它构造出LocaleContext和ServletRequestAttributes对象，两者都可以在initContextHolders(request, localeContext, requestAttributes)之后访问。 3.请求处理由上面所看到的，在processRequest的代码中，调用initContextHolders方法后，调用protected void doService(HttpServletRequest request，HttpServletResponse response)。doService将一些附加参数放入request（如Flash映射:request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap())，上下文信息:request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext())等）中，并调用protected void doDispatch(HttpServletRequest request，HttpServletResponse response)。 doDispatch方法最重要的部分是处理(handler)的检索。doDispatch调用getHandler()方法来分析处理后的请求并返回HandlerExecutionChain实例。此实例包含handler mapping 和interceptors(拦截器)。DispatcherServlet做的另一件事是应用预处理程序拦截器（*applyPreHandle()*）。如果至少有一个返回false，则请求处理停止。否则，servlet使用与handler adapter适配(其实理解成这也是个handler就对了)相应的handler mapping`来生成视图对象。 HandlerMapping 继承RequestMappingHandlerMapping类 ，读取自定义的注解然后匹配看是否有映射关系 protected void registerHandlerMethod(Object handler，Method method，RequestMappingInfo mapping): protected boolean isHandler(Class beanType): 检查bean是否符合给定处理程序的条件。 protected RequestMappingInfo getMappingForMethod(Method method，Class handlerType): 为给定的Method实例提供映射的方法，该方法表示处理的方法（例如，使用@RequestMapping注解的controller的方法上所对应的URL）。 protected HandlerMethod handleNoMatch(Set requestMappingInfos, String lookupPath, HttpServletRequest request) : 在给定的HttpServletRequest对象找不到匹配的处理方法时被调用。 protected void handleMatch(RequestMappingInfo info, String lookupPath, HttpServletRequest request) : 当为给定的HttpServletRequest对象找到匹配的处理方法时调用。 HandlerAdapter接口，它只有3种方法： boolean supports(Object handler):检查传入参数的对象是否可以由此适配器处理 ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) : 将请求翻译成视图。 long getLastModified(HttpServletRequest request, Object handler):返回给定HttpServletRequest的最后修改日期，以毫秒为单位。 4.视图解析获取ModelAndView实例以查看呈现后，doDispatch方法调用private void applyDefaultViewName(HttpServletRequest request，ModelAndView mv)。默认视图名称根据定义的bean名称，即viewNameTranslator。默认情况下，它的实现是org.springframework.web.servlet.RequestToViewNameTranslator。这个默认实现只是简单的将URL转换为视图名称，例如(直接从RequestToViewNameTranslator获取):http:// localhost:8080/admin/index.html将生成视图admin / index。 5.处理调度请求 - 视图渲染现在，servlet知道应该是哪个视图被渲染。它通过private void processDispatchResult(HttpServletRequest request，HttpServletResponse response，@Nullable HandlerExecutionChain mappedHandler, @Nullable ModelAndView mv,@Nullable Exception exception)方法来进行最后一步操作 - 视图渲染。 首先，processDispatchResult检查它们是否有参数传递异常。有一些异常的话，它定义了一个新的视图，专门用来定位错误页面。如果没有任何异常，该方法将检查ModelAndView实例，如果它不为null，则调用render方法。 渲染方法protected void render(ModelAndView mv, HttpServletRequest request, HttpServletResponse response) throws Exception。跳进此方法内部，根据定义的视图策略，它会查找得到一个View类实例。它将负责显示响应。如果没有找到View，则会抛出一个ServletException异常。有的话，DispatcherServlet会调用其render方法来显示结果。 其实可以说成是后置拦截器(进入拦截器前置拦截处理-&gt;controller处理-&gt;出拦截器之前的此拦截器的后置处理)，也就是在请求处理的最后一个步骤中被调用。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供 作用：根据请求的url查找Handler。HandlerMapping负责根据用户请求找到Handler即处理器（Controller），SpringMVC提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行HandlerHandler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view）View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…） 注意：处理器Handler（也就是我们平常说的Controller控制器）以及视图层view都是需要我们自己手动开发的。其他的一些组件比如：前端控制器DispatcherServlet、处理器映射器HandlerMapping、处理器适配器HandlerAdapter等等都是框架提供给我们的，不需要自己手动开发。 DispatcherServlet详细解析首先看下源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package org.springframework.web.servlet; @SuppressWarnings("serial")public class DispatcherServlet extends FrameworkServlet &#123; public static final String MULTIPART_RESOLVER_BEAN_NAME = "multipartResolver"; public static final String LOCALE_RESOLVER_BEAN_NAME = "localeResolver"; public static final String THEME_RESOLVER_BEAN_NAME = "themeResolver"; public static final String HANDLER_MAPPING_BEAN_NAME = "handlerMapping"; public static final String HANDLER_ADAPTER_BEAN_NAME = "handlerAdapter"; public static final String HANDLER_EXCEPTION_RESOLVER_BEAN_NAME = "handlerExceptionResolver"; public static final String REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME = "viewNameTranslator"; public static final String VIEW_RESOLVER_BEAN_NAME = "viewResolver"; public static final String FLASH_MAP_MANAGER_BEAN_NAME = "flashMapManager"; public static final String WEB_APPLICATION_CONTEXT_ATTRIBUTE = DispatcherServlet.class.getName() + ".CONTEXT"; public static final String LOCALE_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".LOCALE_RESOLVER"; public static final String THEME_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_RESOLVER"; public static final String THEME_SOURCE_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_SOURCE"; public static final String INPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".INPUT_FLASH_MAP"; public static final String OUTPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".OUTPUT_FLASH_MAP"; public static final String FLASH_MAP_MANAGER_ATTRIBUTE = DispatcherServlet.class.getName() + ".FLASH_MAP_MANAGER"; public static final String EXCEPTION_ATTRIBUTE = DispatcherServlet.class.getName() + ".EXCEPTION"; public static final String PAGE_NOT_FOUND_LOG_CATEGORY = "org.springframework.web.servlet.PageNotFound"; private static final String DEFAULT_STRATEGIES_PATH = "DispatcherServlet.properties"; protected static final Log pageNotFoundLogger = LogFactory.getLog(PAGE_NOT_FOUND_LOG_CATEGORY); private static final Properties defaultStrategies; static &#123; try &#123; ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, DispatcherServlet.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); &#125; catch (IOException ex) &#123; throw new IllegalStateException("Could not load 'DispatcherServlet.properties': " + ex.getMessage()); &#125; &#125; /** Detect all HandlerMappings or just expect "handlerMapping" bean? */ private boolean detectAllHandlerMappings = true; /** Detect all HandlerAdapters or just expect "handlerAdapter" bean? */ private boolean detectAllHandlerAdapters = true; /** Detect all HandlerExceptionResolvers or just expect "handlerExceptionResolver" bean? */ private boolean detectAllHandlerExceptionResolvers = true; /** Detect all ViewResolvers or just expect "viewResolver" bean? */ private boolean detectAllViewResolvers = true; /** Throw a NoHandlerFoundException if no Handler was found to process this request? **/ private boolean throwExceptionIfNoHandlerFound = false; /** Perform cleanup of request attributes after include request? */ private boolean cleanupAfterInclude = true; /** MultipartResolver used by this servlet */ private MultipartResolver multipartResolver; /** LocaleResolver used by this servlet */ private LocaleResolver localeResolver; /** ThemeResolver used by this servlet */ private ThemeResolver themeResolver; /** List of HandlerMappings used by this servlet */ private List&lt;HandlerMapping&gt; handlerMappings; /** List of HandlerAdapters used by this servlet */ private List&lt;HandlerAdapter&gt; handlerAdapters; /** List of HandlerExceptionResolvers used by this servlet */ private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers; /** RequestToViewNameTranslator used by this servlet */ private RequestToViewNameTranslator viewNameTranslator; private FlashMapManager flashMapManager; /** List of ViewResolvers used by this servlet */ private List&lt;ViewResolver&gt; viewResolvers; public DispatcherServlet() &#123; super(); &#125; public DispatcherServlet(WebApplicationContext webApplicationContext) &#123; super(webApplicationContext); &#125; @Override protected void onRefresh(ApplicationContext context) &#123; initStrategies(context); &#125; protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context); &#125;&#125; DispatcherServlet类中的属性beans： HandlerMapping：用于handlers映射请求和一系列的对于拦截器的前处理和后处理，大部分用@Controller注解。 HandlerAdapter：帮助DispatcherServlet处理映射请求处理程序的适配器，而不用考虑实际调用的是 哪个处理程序。- - - 在Web MVC框架中，每个DispatcherServlet都拥自己的WebApplicationContext，它继承了ApplicationContext。WebApplicationContext包含了其上下文和Servlet实例之间共享的所有的基础框架beans。 HandlerMapping DefaultAnnotationHandlerMapping类通过注解把URL映射到Controller类。 HandlerAdapter AnnotationMethodHandlerAdapter：通过注解，把请求URL映射到Controller类的方法上。 从技术上讲，前端控制器模式由一个捕获所有传入请求的类组成。之后，分析每个请求以知道哪个控制器以及哪个方法应该来处理该请求。 DispatcherServlet的执行链前端控制器模式有自己的执行链。这意味着它有自己的逻辑来处理请求并将视图返回给客户端： 请求由客户端发送。它到达作为Spring的默认前端控制器的DispatcherServlet类。 DispatcherServlet使用请求处理程序映射来发现将分析请求的控制器(controller )。接口org.springframework.web.servlet.HandlerMapping的实现返回一个包含org.springframework.web.servlet.HandlerExecutionChain类的实例。此实例包含可在控制器调用之前或之后调用的处理程序拦截器数组。你可以在Spring中有关于拦截器的文章中了解更多的信息。如果在所有定义的处理程序映射中找不到HandlerExecutionChain，这意味着Spring无法将URL与对应的控制器进行匹配。这样的话会抛出一个错误。 现在系统进行拦截器预处理并调用由映射处理器找到的相应的controller(其实就是在找到的controller之前进行一波拦截处理)。在controller处理请求后，DispatcherServlet开始拦截器的后置处理。在此步骤结束时，它从controller接收ModelAndView实例(整个过程其实就是 request请求-&gt;进入interceptors-&gt;controller-&gt;从interceptors出来-&gt;ModelAndView接收)。 DispatcherServlet现在将使用的该视图的名称发送到视图解析器。这个解析器将决定前台的展现内容。接着，它将此视图返回给DispatcherServlet，其实也就是一个“视图生成后可调用”的拦截器。 最后一个操作是视图的渲染并作为对客户端request请求的响应。 DispatcherServlet 一个调度器servlet。它是一个处理所有传入请求并将视图呈现给用户的类。在重写之前，你应该熟悉执行链，handler mapping 或handler adapter等概念。请记住，第一步要做的是定义在调度过程中我们要调用的所有元素。handler mapping 是将传入请求(也就是它的URL)映射到适当的controller。最后提到的元素，一个handler适配器，就是一个对象，它将通过其内包装的handler mapping将请求发送到controller。此调度产生的结果是ModelAndView类的一个实例，后面被用于生成和渲染视图。 策略初始化protected void onRefresh(ApplicationContext context) 调用initStrategies(context); 初始化策略对象 protected void initStrategies(ApplicationContext context)调用initLocaleResolver(context); initLocaleResolver(context); 捕获异常，采用默认策略 调用 getDefaultStrategy 请求预处理FrameworkServlet继承HttpServletBean HttpServletBean继承HttpServlet FrameworkServlet通过将每个传入的请求调度到processRequest(HttpServletRequest request，HttpServletResponse response)来覆盖它们。processRequest是一个protected和final的方法，它构造出LocaleContext和ServletRequestAttributes对象，两者都可以在initContextHolders(request, localeContext, requestAttributes)之后访问 protected final void doGet(HttpServletRequest request, HttpServletResponse response)调用processRequest(request, response); protected final void processRequest(HttpServletRequest request, HttpServletResponse response)调用private void initContextHolders(HttpServletRequest request,​ @Nullable LocaleContext localeContext, @Nullable RequestAttributes requestAttributes) 请求处理调用doService(request, response); doService将一些附加参数放入request（映射，上下文信息） doDispatch方法最重要的部分是处理(handler)的检索。doDispatch调用getHandler()方法来分析处理后的请求并返回HandlerExecutionChain实例。此实例包含handler mapping 和interceptors(拦截器)。DispatcherServlet做的另一件事是应用预处理程序拦截器（*applyPreHandle()*）。如果至少有一个返回false，则请求处理停止。否则，servlet使用与handler adapter适配(其实理解成这也是个handler就对了)相应的handler mapping`来生成视图对象。 DispatcherServlet 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Override protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; if (logger.isDebugEnabled()) &#123; String resumed = WebAsyncUtils.getAsyncManager(request).hasConcurrentResult() ? " resumed" : ""; logger.debug("DispatcherServlet with name '" + getServletName() + "'" + resumed + " processing " + request.getMethod() + " request for [" + getRequestUri(request) + "]"); &#125; // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX)) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); if (this.flashMapManager != null) &#123; FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); &#125; try &#123; doDispatch(request, response); &#125; finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. mappedHandler = getHandler(processedRequest); if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request. HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = "GET".equals(method); if (isGet || "HEAD".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug("Last-Modified value for [" + getRequestUri(request) + "] is: " + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(processedRequest, mv); mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException("Handler dispatch failed", err); &#125; processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException("Handler processing failed", err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125; &#125; HandlerAdapter取可处理request的Handler，调用的相应的Handler mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); 调用controller applyDefaultViewName(processedRequest, mv);视图解析 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);视图渲染 handlehandler mappings 处理程序映射 request 匹配 controller handler adapter 处理器适配器 从handler mappings 获取映射的controller和方法并调用必须实现handlerAdaper Interceptor过滤器只能在servlet容器下使用。Spring容器不一定运行在web环境中，在这种情况下过滤器就不好使了，而拦截器依然可以在Spring容器中调用。 @ModelAttribute注解用于将动态请求参数转换为Java注解中指定的对象 ,Spring会尝试将所有请求参数匹配到Article类的字段中 必须将BindingResult实例直接放在经过验证的对象之后 modelarrtibute]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpringBean%2F</url>
    <content type="text"><![CDATA[singleton ioc容器中仅存在一个bean实例 prototype 每次调用都会返回新的实例 request 没次请求都会创建新的bean session 同一个session 共享一个bean globalsession 用于portlet环境 @Scope 限定作用域 相关接口、方法说明Bean自身方法：init-method/destroy-method，通过为配置文件bean定义中添加相应属性指定相应执行方法。Bean级生命周期接口：BeanNameAware、BeanFactoryAware、InitializingBean和DiposableBean这些接口的方法。每个Bean选择实现，可选择各自的个性化操作。容器级生命周期接口方法：这个包括了InstantiationAwareBeanPostProcessor 和 BeanPostProcessor 这两个接口实现（前者继承自后者），一般称它们的实现类为“后处理器”（其实我觉得这个名称对新手有误导的意思），这些接口是每个bean实例化或初始化时候都会调用。 工厂后处理器接口方法：这些方法也是容器级别的，但它们是在上下文装置配置文件之后调用，例如BeanFactoryPostProcessor、 CustomAutowireConfigurer等。 创建Bean 调用BeanPostProcessor的前置初始化方法postProcessBeforeInitialization。 如果实现了InitializingBean接口，则会调用afterPropertiesSet方法。 调用Bean自身定义的init方法。 调用BeanPostProcessor的后置初始化方法postProcessAfterInitialization。 创建过程完毕。 Bean具体生命周期 BeanFactoryPostProcessor#postProcessBeanFactory postProcessBeforeInstantiation(Class&lt;？&gt;c,String beanName)所有bean对象（注1）实例化之前执行，具体点就是执行每个bean类构造函数之前。 bean实例化，调用构造函数 postProcessAfterInstantiation(Object bean,String beanName)bean类实例化之后，初始化之前调用 postProcessPropertyValue属性注入之前调用 setBeanName(String beanName)属性注入后调用，该方法作用是让bean类知道自己所在的Bean的name或id属性。实现：bean类实现BeanNameAware接口，重写该方法。 setBeanFactory(BeanFactory factory)setBeanName后调用，该方法作用是让bean类知道自己所在的BeanFactory的属性（传入bean所在BeanFactory类型参数）。实现：bean类实现BeanFactoryAware接口，实现该方法。 postProcessBeforeInitialization(Object bean,String beanName)BeanPostProcessor作用是对bean实例化、初始化做些预处理操作（注2）。实现：写一个类，实现BeanPostProcessor接口，注意返回类型为Object，默认返回null，需要返回参数中bean InstantiationAwareBeanPostProcessor#postProcessBeforeInstantiation实现：同第2步，实现该方法，注意点同第8步。（注3） afterPropertiesSet()实现：bean类实现InitializingBean接口，重写该方法。初始化工作，但实现该接口这种方法和Spring耦合，不推荐（这一点DisposableBean一样）。 init()调用Bean自身定义的init方法。 postProcessAfterInitialization(Object bean,Strign beanName)实现：同第8步，注意点相同。 postProcessAfterInitialization(Object bean,Strign beanName)实现：同第2步，注意点同第9步。程序执行，bean工作 destroy()bean销毁前执行实现：bean类实现DisposableBean接口 xml_destroy()实现：spring bean配置文件中配置bean属性destroy-method=”xml_destroy”，Bean自身定制的destroy方法。 注1：这里的bean类指的是普通bean类，不包括这里实现了各类接口（就是2.2提到的这些接口）而在配置文件中声明的bean。注2：如果有多个BeanPostProcessor实现类，其执行顺序参考：BeanPostProcessor详解。 注3：InstantiationAwareBeanPostProcessor接口继承自BeanPostProcessor接口，是它的子接口，扩展了两个方法，即bean实例化前后操作，当然前者也会有bean初始化前后操作，当它们两同时存在的话，开发者又同时对两者的postProcessBeforeInitialization、postProcessAfterInitialization方法实现了，先回去执行BeanPostProcessor的方法，再去执行InstantiationAwareBeanPostProcessor的。 Spring Bean 生命周期前言Spring Bean 的生命周期在整个 Spring 中占有很重要的位置，掌握这些可以加深对 Spring 的理解。 首先看下生命周期图： 再谈生命周期之前有一点需要先明确： Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 注解方式在 bean 初始化时会经历几个阶段，首先可以使用注解 @PostConstruct, @PreDestroy 来在 bean 的创建和销毁阶段进行调用: 1234567891011121314@Componentpublic class AnnotationBean &#123; private final static Logger LOGGER = LoggerFactory.getLogger(AnnotationBean.class); @PostConstruct public void start()&#123; LOGGER.info("AnnotationBean start"); &#125; @PreDestroy public void destroy()&#123; LOGGER.info("AnnotationBean destroy"); &#125;&#125; InitializingBean, DisposableBean 接口还可以实现 InitializingBean,DisposableBean 这两个接口，也是在初始化以及销毁阶段调用： 12345678910111213@Servicepublic class SpringLifeCycleService implements InitializingBean,DisposableBean&#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleService.class); @Override public void afterPropertiesSet() throws Exception &#123; LOGGER.info("SpringLifeCycleService start"); &#125; @Override public void destroy() throws Exception &#123; LOGGER.info("SpringLifeCycleService destroy"); &#125;&#125; 自定义初始化和销毁方法也可以自定义方法用于在初始化、销毁阶段调用: 123456789101112131415161718192021222324@Configurationpublic class LifeCycleConfig &#123; @Bean(initMethod = "start", destroyMethod = "destroy") public SpringLifeCycle create()&#123; SpringLifeCycle springLifeCycle = new SpringLifeCycle() ; return springLifeCycle ; &#125;&#125;public class SpringLifeCycle&#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycle.class); public void start()&#123; LOGGER.info("SpringLifeCycle start"); &#125; public void destroy()&#123; LOGGER.info("SpringLifeCycle destroy"); &#125;&#125; 以上是在 SpringBoot 中可以这样配置，如果是原始的基于 XML 也是可以使用: 12&lt;bean class="com.crossoverjie.spring.SpringLifeCycle" init-method="start" destroy-method="destroy"&gt;&lt;/bean&gt; 来达到同样的效果。 实现 *Aware 接口*Aware 接口可以用于在初始化 bean 时获得 Spring 中的一些对象，如获取 Spring 上下文等。 123456789101112@Componentpublic class SpringLifeCycleAware implements ApplicationContextAware &#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleAware.class); private ApplicationContext applicationContext ; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext ; LOGGER.info("SpringLifeCycleAware start"); &#125;&#125; 这样在 springLifeCycleAware 这个 bean 初始化会就会调用 setApplicationContext 方法，并可以获得 applicationContext 对象。 BeanPostProcessor 增强处理器实现 BeanPostProcessor 接口，Spring 中所有 bean 在做初始化时都会调用该接口中的两个方法，可以用于对一些特殊的 bean 进行处理： 12345678910111213141516171819202122232425262728293031323334@Componentpublic class SpringLifeCycleProcessor implements BeanPostProcessor &#123; private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleProcessor.class); /** * 预初始化 初始化之前调用 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if ("annotationBean".equals(beanName))&#123; LOGGER.info("SpringLifeCycleProcessor start beanName=&#123;&#125;",beanName); &#125; return bean; &#125; /** * 后初始化 bean 初始化完成调用 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if ("annotationBean".equals(beanName))&#123; LOGGER.info("SpringLifeCycleProcessor end beanName=&#123;&#125;",beanName); &#125; return bean; &#125;&#125; 执行之后观察结果： 123456789101112131415018-03-21 00:40:24.856 [restartedMain] INFO c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor start beanName=annotationBean2018-03-21 00:40:24.860 [restartedMain] INFO c.c.spring.annotation.AnnotationBean - AnnotationBean start2018-03-21 00:40:24.861 [restartedMain] INFO c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor end beanName=annotationBean2018-03-21 00:40:24.864 [restartedMain] INFO c.c.s.aware.SpringLifeCycleAware - SpringLifeCycleAware start2018-03-21 00:40:24.867 [restartedMain] INFO c.c.s.service.SpringLifeCycleService - SpringLifeCycleService start2018-03-21 00:40:24.887 [restartedMain] INFO c.c.spring.SpringLifeCycle - SpringLifeCycle start2018-03-21 00:40:25.062 [restartedMain] INFO o.s.b.d.a.OptionalLiveReloadServer - LiveReload server is running on port 357292018-03-21 00:40:25.122 [restartedMain] INFO o.s.j.e.a.AnnotationMBeanExporter - Registering beans for JMX exposure on startup2018-03-21 00:40:25.140 [restartedMain] INFO com.crossoverjie.Application - Started Application in 2.309 seconds (JVM running for 3.681)2018-03-21 00:40:25.143 [restartedMain] INFO com.crossoverjie.Application - start ok!2018-03-21 00:40:25.153 [Thread-8] INFO o.s.c.a.AnnotationConfigApplicationContext - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@3913adad: startup date [Wed Mar 21 00:40:23 CST 2018]; root of context hierarchy2018-03-21 00:40:25.155 [Thread-8] INFO o.s.j.e.a.AnnotationMBeanExporter - Unregistering JMX-exposed beans on shutdown2018-03-21 00:40:25.156 [Thread-8] INFO c.c.spring.SpringLifeCycle - SpringLifeCycle destroy2018-03-21 00:40:25.156 [Thread-8] INFO c.c.s.service.SpringLifeCycleService - SpringLifeCycleService destroy2018-03-21 00:40:25.156 [Thread-8] INFO c.c.spring.annotation.AnnotationBean - AnnotationBean destroy 直到 Spring 上下文销毁时则会调用自定义的销毁方法以及实现了 DisposableBean 的 destroy() 方法。 二 bean的生命周期Spring Bean是Spring应用中最最重要的部分了。所以来看看Spring容器在初始化一个bean的时候会做那些事情，顺序是怎样的，在容器关闭的时候，又会做哪些事情。 spring版本：4.2.3.RELEASE鉴于Spring源码是用gradle构建的，我也决定舍弃我大maven，尝试下洪菊推荐过的gradle。运行beanLifeCycle模块下的junit test即可在控制台看到如下输出，可以清楚了解Spring容器在创建，初始化和销毁Bean的时候依次做了那些事情。 12345678910111213141516171819202122232425Spring容器初始化=====================================调用GiraffeService无参构造函数GiraffeService中利用set方法设置属性值调用setBeanName:: Bean Name defined in context=giraffeService调用setBeanClassLoader,ClassLoader Name = sun.misc.Launcher$AppClassLoader调用setBeanFactory,setBeanFactory:: giraffe bean singleton=true调用setEnvironment调用setResourceLoader:: Resource File Name=spring-beans.xml调用setApplicationEventPublisher调用setApplicationContext:: Bean Definition Names=[giraffeService, org.springframework.context.annotation.CommonAnnotationBeanPostProcessor#0, com.giraffe.spring.service.GiraffeServicePostProcessor#0]执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=giraffeService调用PostConstruct注解标注的方法执行InitializingBean接口的afterPropertiesSet方法执行配置的init-method执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=giraffeServiceSpring容器初始化完毕=====================================从容器中获取Beangiraffe Name=李光洙=====================================调用preDestroy注解标注的方法执行DisposableBean接口的destroy方法执行配置的destroy-methodSpring容器关闭 先来看看，Spring在Bean从创建到销毁的生命周期中可能做得事情。 initialization 和 destroy有时我们需要在Bean属性值set好之后和Bean销毁之前做一些事情，比如检查Bean中某个属性是否被正常的设置好值了。Spring框架提供了多种方法让我们可以在Spring Bean的生命周期中执行initialization和pre-destroy方法。 1.实现InitializingBean和DisposableBean接口 这两个接口都只包含一个方法。通过实现InitializingBean接口的afterPropertiesSet()方法可以在Bean属性值设置好之后做一些操作，实现DisposableBean接口的destroy()方法可以在销毁Bean之前做一些操作。 例子如下： 12345678910public class GiraffeService implements InitializingBean,DisposableBean &#123; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("执行InitializingBean接口的afterPropertiesSet方法"); &#125; @Override public void destroy() throws Exception &#123; System.out.println("执行DisposableBean接口的destroy方法"); &#125;&#125; 这种方法比较简单，但是不建议使用。因为这样会将Bean的实现和Spring框架耦合在一起。 2.在bean的配置文件中指定init-method和destroy-method方法 Spring允许我们创建自己的 init 方法和 destroy 方法，只要在 Bean 的配置文件中指定 init-method 和 destroy-method 的值就可以在 Bean 初始化时和销毁之前执行一些操作。 例子如下： 12345678910public class GiraffeService &#123; //通过&lt;bean&gt;的destroy-method属性指定的销毁方法 public void destroyMethod() throws Exception &#123; System.out.println("执行配置的destroy-method"); &#125; //通过&lt;bean&gt;的init-method属性指定的初始化方法 public void initMethod() throws Exception &#123; System.out.println("执行配置的init-method"); &#125;&#125; 配置文件中的配置： 12&lt;bean name=&quot;giraffeService&quot; class=&quot;com.giraffe.spring.service.GiraffeService&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt;&lt;/bean&gt; 需要注意的是自定义的init-method和post-method方法可以抛异常但是不能有参数。 这种方式比较推荐，因为可以自己创建方法，无需将Bean的实现直接依赖于spring的框架。 3.使用@PostConstruct和@PreDestroy注解 除了xml配置的方式，Spring 也支持用 @PostConstruct和 @PreDestroy注解来指定 init 和 destroy 方法。这两个注解均在javax.annotation 包中。为了注解可以生效，需要在配置文件中定义org.springframework.context.annotation.CommonAnnotationBeanPostProcessor或context:annotation-config 例子如下： 12345678910public class GiraffeService &#123; @PostConstruct public void initPostConstruct()&#123; System.out.println("执行PostConstruct注解标注的方法"); &#125; @PreDestroy public void preDestroy()&#123; System.out.println("执行preDestroy注解标注的方法"); &#125;&#125; 配置文件： 12 &lt;bean class="org.springframework.context.annotation.CommonAnnotationBeanPostProcessor" /&gt; 实现*Aware接口 在Bean中使用Spring框架的一些对象有些时候我们需要在 Bean 的初始化中使用 Spring 框架自身的一些对象来执行一些操作，比如获取 ServletContext 的一些参数，获取 ApplicaitionContext 中的 BeanDefinition 的名字，获取 Bean 在容器中的名字等等。为了让 Bean 可以获取到框架自身的一些对象，Spring 提供了一组名为*Aware的接口。 这些接口均继承于org.springframework.beans.factory.Aware标记接口，并提供一个将由 Bean 实现的set*方法,Spring通过基于setter的依赖注入方式使相应的对象可以被Bean使用。网上说，这些接口是利用观察者模式实现的，类似于servlet listeners，目前还不明白，不过这也不在本文的讨论范围内。介绍一些重要的Aware接口： ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 12345678910111213141516171819202122232425262728293031323334353637383940public class GiraffeService implements ApplicationContextAware, ApplicationEventPublisherAware, BeanClassLoaderAware, BeanFactoryAware, BeanNameAware, EnvironmentAware, ImportAware, ResourceLoaderAware&#123; @Override public void setBeanClassLoader(ClassLoader classLoader) &#123; System.out.println("执行setBeanClassLoader,ClassLoader Name = " + classLoader.getClass().getName()); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; System.out.println("执行setBeanFactory,setBeanFactory:: giraffe bean singleton=" + beanFactory.isSingleton("giraffeService")); &#125; @Override public void setBeanName(String s) &#123; System.out.println("执行setBeanName:: Bean Name defined in context=" + s); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; System.out.println("执行setApplicationContext:: Bean Definition Names=" + Arrays.toString(applicationContext.getBeanDefinitionNames())); &#125; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; System.out.println("执行setApplicationEventPublisher"); &#125; @Override public void setEnvironment(Environment environment) &#123; System.out.println("执行setEnvironment"); &#125; @Override public void setResourceLoader(ResourceLoader resourceLoader) &#123; Resource resource = resourceLoader.getResource("classpath:spring-beans.xml"); System.out.println("执行setResourceLoader:: Resource File Name=" + resource.getFilename()); &#125; @Override public void setImportMetadata(AnnotationMetadata annotationMetadata) &#123; System.out.println("执行setImportMetadata"); &#125;&#125; BeanPostProcessor上面的*Aware接口是针对某个实现这些接口的Bean定制初始化的过程，Spring同样可以针对容器中的所有Bean，或者某些Bean定制初始化过程，只需提供一个实现BeanPostProcessor接口的类即可。 该接口中包含两个方法，postProcessBeforeInitialization和postProcessAfterInitialization。 postProcessBeforeInitialization方法会在容器中的Bean初始化之前执行， postProcessAfterInitialization方法在容器中的Bean初始化之后执行。 例子如下： 123456789101112public class CustomerBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=" + beanName); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=" + beanName); return bean; &#125;&#125; 要将BeanPostProcessor的Bean像其他Bean一样定义在配置文件中 1&lt;bean class="com.giraffe.spring.service.CustomerBeanPostProcessor"/&gt; 总结所以。。。结合第一节控制台输出的内容，Spring Bean的生命周期是这样纸的： Bean容器找到配置文件中 Spring Bean 的定义。 Bean容器利用Java Reflection API创建一个Bean的实例。 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 与上面的类似，如果实现了其他*Aware接口，就调用相应的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessAfterInitialization()方法 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 用图表示一下(图来源:http://www.jianshu.com/p/d00539babca5)： 与之比较类似的中文版本: 其实很多时候我们并不会真的去实现上面说描述的那些接口，那么下面我们就除去那些接口，针对bean的单例和非单例来描述下bean的生命周期： 单例管理的对象当scope=”singleton”，即默认情况下，会在启动容器时（即实例化容器时）时实例化。但我们可以指定Bean节点的lazy-init=”true”来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。如下配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" lazy-init="true"/&gt; 如果想对所有的默认单例bean都应用延迟初始化，可以在根节点beans设置default-lazy-init属性为true，如下所示： 1&lt;beans default-lazy-init="true" …&gt; 默认情况下，Spring 在读取 xml 文件的时候，就会创建对象。在创建对象的时候先调用构造器，然后调用 init-method 属性值中所指定的方法。对象在被销毁的时候，会调用 destroy-method 属性值中所指定的方法（例如调用Container.destroy()方法的时候）。写一个测试类，代码如下： 1234567891011121314151617181920212223public class LifeBean &#123; private String name; public LifeBean()&#123; System.out.println("LifeBean()构造函数"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; System.out.println("setName()"); this.name = name; &#125; public void init()&#123; System.out.println("this is init of lifeBean"); &#125; public void destory()&#123; System.out.println("this is destory of lifeBean " + this); &#125; &#125; life.xml配置如下： 12&lt;bean id="life_singleton" class="com.bean.LifeBean" scope="singleton" init-method="init" destroy-method="destory" lazy-init="true"/&gt; 测试代码： 12345678910public class LifeTest &#123; @Test public void test() &#123; AbstractApplicationContext container = new ClassPathXmlApplicationContext("life.xml"); LifeBean life1 = (LifeBean)container.getBean("life"); System.out.println(life1); container.close(); &#125;&#125; 运行结果： 12345LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@573f2bb1……this is destory of lifeBean com.bean.LifeBean@573f2bb1 非单例管理的对象当scope=”prototype”时，容器也会延迟初始化 bean，Spring 读取xml 文件的时候，并不会立刻创建对象，而是在第一次请求该 bean 时才初始化（如调用getBean方法时）。在第一次请求每一个 prototype 的bean 时，Spring容器都会调用其构造器创建这个对象，然后调用init-method属性值中所指定的方法。对象销毁的时候，Spring 容器不会帮我们调用任何方法，因为是非单例，这个类型的对象有很多个，Spring容器一旦把这个对象交给你之后，就不再管理这个对象了。 为了测试prototype bean的生命周期life.xml配置如下： 1&lt;bean id="life_prototype" class="com.bean.LifeBean" scope="prototype" init-method="init" destroy-method="destory"/&gt; 测试程序： 123456789101112public class LifeTest &#123; @Test public void test() &#123; AbstractApplicationContext container = new ClassPathXmlApplicationContext("life.xml"); LifeBean life1 = (LifeBean)container.getBean("life_singleton"); System.out.println(life1); LifeBean life3 = (LifeBean)container.getBean("life_prototype"); System.out.println(life3); container.close(); &#125;&#125; 运行结果： 12345678LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@573f2bb1LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@5ae9a829……this is destory of lifeBean com.bean.LifeBean@573f2bb1 可以发现，对于作用域为 prototype 的 bean ，其destroy方法并没有被调用。如果 bean 的 scope 设为prototype时，当容器关闭时，destroy 方法不会被调用。对于 prototype 作用域的 bean，有一点非常重要，那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法。但对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责（让Spring容器释放被prototype作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用）。谈及prototype作用域的bean时，在某些方面你可以将Spring容器的角色看作是Java new操作的替代者，任何迟于该时间点的生命周期事宜都得交由客户端来处理。 Spring 容器可以管理 singleton 作用域下 bean 的生命周期，在此作用域下，Spring 能够精确地知道bean何时被创建，何时初始化完成，以及何时被销毁。而对于 prototype 作用域的bean，Spring只负责创建，当容器创建了 bean 的实例后，bean 的实例就交给了客户端的代码管理，Spring容器将不再跟踪其生命周期，并且不会管理那些被配置成prototype作用域的bean的生命周期。 三 说明本文的完成结合了下面两篇文章，并做了相应修改： https://blog.csdn.net/fuzhongmin05/article/details/73389779 https://yemengying.com/2016/07/14/spring-bean-life-cycle/]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpring%20IOC%2F</url>
    <content type="text"><![CDATA[自己动手实现的 Spring IOC 和 AOP - 下篇 Spring IOC 如何实现 https://www.cnblogs.com/ITtangtang/p/3978349.html IOC：Spring IOC 容器源码分析 强烈推荐，内容详尽，而且便于阅读。 Spring IOC设计原理解析:本文乃学习整理参考而来 一、 什么是Ioc/DI？ 二、 Spring IOC体系结构 (1) BeanFactory (2) BeanDefinition 三、 IoC容器的初始化 1、 XmlBeanFactory(屌丝IOC)的整个流程 2、 FileSystemXmlApplicationContext 的IOC容器流程 1、高富帅IOC解剖 2、 设置资源加载器和资源定位 3、AbstractApplicationContext的refresh函数载入Bean定义过程： 4、AbstractApplicationContext子类的refreshBeanFactory()方法： 5、AbstractRefreshableApplicationContext子类的loadBeanDefinitions方法： 6、AbstractBeanDefinitionReader读取Bean定义资源： 7、资源加载器获取要读入的资源： 8、XmlBeanDefinitionReader加载Bean定义资源： 9、DocumentLoader将Bean定义资源转换为Document对象： 10、XmlBeanDefinitionReader解析载入的Bean定义资源文件： 11、DefaultBeanDefinitionDocumentReader对Bean定义的Document对象解析： 12、BeanDefinitionParserDelegate解析Bean定义资源文件中的元素： 13、BeanDefinitionParserDelegate解析元素： 14、解析元素的子元素： 15、解析子元素： 16、解析过后的BeanDefinition在IoC容器中的注册： 17、DefaultListableBeanFactory向IoC容器注册解析后的BeanDefinition： 总结： 四、IOC容器的依赖注入 1、依赖注入发生的时间 2、AbstractBeanFactory通过getBean向IoC容器获取被管理的Bean： 3、AbstractAutowireCapableBeanFactory创建Bean实例对象： 4、createBeanInstance方法创建Bean的java实例对象： 5、SimpleInstantiationStrategy类使用默认的无参构造方法创建Bean实例化对象： 6、populateBean方法对Bean属性的依赖注入： 7、BeanDefinitionValueResolver解析属性值： 8、BeanWrapperImpl对Bean属性的依赖注入： 五、IoC容器的高级特性 1、介绍 2、Spring IoC容器的lazy-init属性实现预实例化： (1) .refresh() (2).finishBeanFactoryInitialization处理预实例化Bean： (3) .DefaultListableBeanFactory对配置lazy-init属性单态Bean的预实例化： 3、FactoryBean的实现： (1).FactoryBean的源码如下： (2). AbstractBeanFactory的getBean方法调用FactoryBean： (3)、AbstractBeanFactory生产Bean实例对象： (4).工厂Bean的实现类getObject方法创建Bean实例对象： 4.BeanPostProcessor后置处理器的实现： (1).BeanPostProcessor的源码如下： (2).AbstractAutowireCapableBeanFactory类对容器生成的Bean添加后置处理器： (3).initializeBean方法为容器产生的Bean实例对象添加BeanPostProcessor后置处理器： (4).AdvisorAdapterRegistrationManager在Bean对象初始化后注册通知适配器： 5.Spring IoC容器autowiring实现原理： (1). AbstractAutoWireCapableBeanFactory对Bean实例进行属性依赖注入： (2).Spring IoC容器根据Bean名称或者类型进行autowiring自动依赖注入： (3).DefaultSingletonBeanRegistry的registerDependentBean方法对属性注入： ​ 三、IoC容器的初始化？​ IoC容器的初始化包括BeanDefinition的Resource定位、载入和注册这三个基本的过程。我们以ApplicationContext为例讲解，ApplicationContext系列容器也许是我们最熟悉的，因为web项目中使用的XmlWebApplicationContext就属于这个继承体系，还有ClasspathXmlApplicationContext等，其继承体系如下图所示： ApplicationContext允许上下文嵌套，通过保持父上下文可以维持一个上下文体系。对于bean的查找可以在这个上下文体系中发生，首先检查当前上下文，其次是父上下文，逐级向上，这样为不同的Spring应用提供了一个共享的bean定义环境。 下面我们分别简单地演示一下两种ioc容器的创建过程 1、XmlBeanFactory(屌丝IOC)的整个流程 通过XmlBeanFactory的源码，我们可以发现: ;) 123456789101112131415161718public class XmlBeanFactory extends DefaultListableBeanFactory&#123; private final XmlBeanDefinitionReader reader; public XmlBeanFactory(Resource resource)throws BeansException&#123; this(resource, null); &#125; public XmlBeanFactory(Resource resource, BeanFactory parentBeanFactory) throws BeansException&#123; super(parentBeanFactory); this.reader = new XmlBeanDefinitionReader(this); this.reader.loadBeanDefinitions(resource); &#125;&#125; ;) ;) 12345678//根据Xml配置文件创建Resource资源对象，该对象中包含了BeanDefinition的信息 ClassPathResource resource =new ClassPathResource(&quot;application-context.xml&quot;);//创建DefaultListableBeanFactory DefaultListableBeanFactory factory =new DefaultListableBeanFactory();//创建XmlBeanDefinitionReader读取器，用于载入BeanDefinition。之所以需要BeanFactory作为参数，是因为会将读取的信息回调配置给factory XmlBeanDefinitionReader reader =new XmlBeanDefinitionReader(factory);//XmlBeanDefinitionReader执行载入BeanDefinition的方法，最后会完成Bean的载入和注册。完成后Bean就成功的放置到IOC容器当中，以后我们就可以从中取得Bean来使用 reader.loadBeanDefinitions(resource); ;) 通过前面的源码，this.reader = new XmlBeanDefinitionReader(this); 中其中this 传的是factory对象 2、FileSystemXmlApplicationContext 的IOC容器流程 1、高富帅IOC解剖 1 ApplicationContext =new FileSystemXmlApplicationContext(xmlPath); 先看其构造函数： 调用构造函数： ;) 12345678/*** Create a new FileSystemXmlApplicationContext, loading the definitions* from the given XML files and automatically refreshing the context.* @param configLocations array of file paths* @throws BeansException if context creation failed */public FileSystemXmlApplicationContext(String... configLocations) throws BeansException &#123; this(configLocations, true, null); &#125; ;) 实际调用 ;) 12345678public FileSystemXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125; &#125; ;) 2、设置资源加载器和资源定位 通过分析FileSystemXmlApplicationContext的源代码可以知道，在创建FileSystemXmlApplicationContext容器时，构造方法做以下两项重要工作： 首先，调用父类容器的构造方法(super(parent)方法)为容器设置好Bean资源加载器。 然后，再调用父类AbstractRefreshableConfigApplicationContext的setConfigLocations(configLocations)方法设置Bean定义资源文件的定位路径。 通过追踪FileSystemXmlApplicationContext的继承体系，发现其父类的父类AbstractApplicationContext中初始化IoC容器所做的主要源码如下： ;) 123456789101112131415161718192021public abstract class AbstractApplicationContext extends DefaultResourceLoader implements ConfigurableApplicationContext, DisposableBean &#123; //静态初始化块，在整个容器创建过程中只执行一次 static &#123; //为了避免应用程序在Weblogic8.1关闭时出现类加载异常加载问题，加载IoC容 //器关闭事件(ContextClosedEvent)类 ContextClosedEvent.class.getName(); &#125; //FileSystemXmlApplicationContext调用父类构造方法调用的就是该方法 public AbstractApplicationContext(ApplicationContext parent) &#123; this.parent = parent; this.resourcePatternResolver = getResourcePatternResolver(); &#125; //获取一个Spring Source的加载器用于读入Spring Bean定义资源文件 protected ResourcePatternResolver getResourcePatternResolver() &#123; // AbstractApplicationContext继承DefaultResourceLoader，也是一个S //Spring资源加载器，其getResource(String location)方法用于载入资源 return new PathMatchingResourcePatternResolver(this); &#125; …… &#125; ;) AbstractApplicationContext构造方法中调用PathMatchingResourcePatternResolver的构造方法创建Spring资源加载器： 12345public PathMatchingResourcePatternResolver(ResourceLoader resourceLoader) &#123; Assert.notNull(resourceLoader, &quot;ResourceLoader must not be null&quot;); //设置Spring的资源加载器 this.resourceLoader = resourceLoader; &#125; 在设置容器的资源加载器之后，接下来FileSystemXmlApplicationContet执行setConfigLocations方法通过调用其父类AbstractRefreshableConfigApplicationContext的方法进行对Bean定义资源文件的定位，该方法的源码如下： ;) 123456789101112131415161718192021//处理单个资源文件路径为一个字符串的情况 public void setConfigLocation(String location) &#123; //String CONFIG_LOCATION_DELIMITERS = &quot;,; /t/n&quot;; //即多个资源文件路径之间用” ,; /t/n”分隔，解析成数组形式 setConfigLocations(StringUtils.tokenizeToStringArray(location, CONFIG_LOCATION_DELIMITERS)); &#125; //解析Bean定义资源文件的路径，处理多个资源文件字符串数组 public void setConfigLocations(String[] locations) &#123; if (locations != null) &#123; Assert.noNullElements(locations, &quot;Config locations must not be null&quot;); this.configLocations = new String[locations.length]; for (int i = 0; i &lt; locations.length; i++) &#123; // resolvePath为同一个类中将字符串解析为路径的方法 this.configLocations[i] = resolvePath(locations[i]).trim(); &#125; &#125; else &#123; this.configLocations = null; &#125; &#125; ;) 通过这两个方法的源码我们可以看出，我们既可以使用一个字符串来配置多个Spring Bean定义资源文件，也可以使用字符串数组，即下面两种方式都是可以的： a. ClasspathResource res = new ClasspathResource(“a.xml,b.xml,……”); 多个资源文件路径之间可以是用” ,; /t/n”等分隔。 b. ClasspathResource res = new ClasspathResource(newString[]{“a.xml”,”b.xml”,……}); 至此，Spring IoC容器在初始化时将配置的Bean定义资源文件定位为Spring封装的Resource。 3、AbstractApplicationContext的refresh函数载入Bean定义过程： Spring IoC容器对Bean定义资源的载入是从refresh()函数开始的，refresh()是一个模板方法，refresh()方法的作用是：在创建IoC容器前，如果已经有容器存在，则需要把已有的容器销毁和关闭，以保证在refresh之后使用的是新建立起来的IoC容器。refresh的作用类似于对IoC容器的重启，在新建立好的容器中对容器进行初始化，对Bean定义资源进行载入 FileSystemXmlApplicationContext通过调用其父类AbstractApplicationContext的refresh()函数启动整个IoC容器对Bean定义的载入过程： ;) 1234567891011121314151617181920212223242526272829303132333435363738391 public void refresh() throws BeansException, IllegalStateException &#123; 2 synchronized (this.startupShutdownMonitor) &#123; 3 //调用容器准备刷新的方法，获取容器的当时时间，同时给容器设置同步标识 4 prepareRefresh(); 5 //告诉子类启动refreshBeanFactory()方法，Bean定义资源文件的载入从 6 //子类的refreshBeanFactory()方法启动 7 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); 8 //为BeanFactory配置容器特性，例如类加载器、事件处理器等 9 prepareBeanFactory(beanFactory); 10 try &#123; 11 //为容器的某些子类指定特殊的BeanPost事件处理器 12 postProcessBeanFactory(beanFactory); 13 //调用所有注册的BeanFactoryPostProcessor的Bean 14 invokeBeanFactoryPostProcessors(beanFactory); 15 //为BeanFactory注册BeanPost事件处理器. 16 //BeanPostProcessor是Bean后置处理器，用于监听容器触发的事件 17 registerBeanPostProcessors(beanFactory); 18 //初始化信息源，和国际化相关. 19 initMessageSource(); 20 //初始化容器事件传播器. 21 initApplicationEventMulticaster(); 22 //调用子类的某些特殊Bean初始化方法 23 onRefresh(); 24 //为事件传播器注册事件监听器. 25 registerListeners(); 26 //初始化所有剩余的单态Bean. 27 finishBeanFactoryInitialization(beanFactory); 28 //初始化容器的生命周期事件处理器，并发布容器的生命周期事件 29 finishRefresh(); 30 &#125; 31 catch (BeansException ex) &#123; 32 //销毁以创建的单态Bean 33 destroyBeans(); 34 //取消refresh操作，重置容器的同步标识. 35 cancelRefresh(ex); 36 throw ex; 37 &#125; 38 &#125; 39 &#125; ;) refresh()方法主要为IoC容器Bean的生命周期管理提供条件，Spring IoC容器载入Bean定义资源文件从其子类容器的refreshBeanFactory()方法启动，所以整个refresh()中“ConfigurableListableBeanFactory beanFactory =obtainFreshBeanFactory();”这句以后代码的都是注册容器的信息源和生命周期事件，载入过程就是从这句代码启动。 refresh()方法的作用是：在创建IoC容器前，如果已经有容器存在，则需要把已有的容器销毁和关闭，以保证在refresh之后使用的是新建立起来的IoC容器。refresh的作用类似于对IoC容器的重启，在新建立好的容器中对容器进行初始化，对Bean定义资源进行载入 AbstractApplicationContext的obtainFreshBeanFactory()方法调用子类容器的refreshBeanFactory()方法，启动容器载入Bean定义资源文件的过程，代码如下： ;) 123456789protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //这里使用了委派设计模式，父类定义了抽象的refreshBeanFactory()方法，具体实现调用子类容器的refreshBeanFactory()方法 refreshBeanFactory(); ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean factory for &quot; + getDisplayName() + &quot;: &quot; + beanFactory); &#125; return beanFactory; &#125; ;) AbstractApplicationContext子类的refreshBeanFactory()方法： AbstractApplicationContext类中只抽象定义了refreshBeanFactory()方法，容器真正调用的是其子类AbstractRefreshableApplicationContext实现的 refreshBeanFactory()方法，方法的源码如下： ;) 1234567891011121314151617181920211 protected final void refreshBeanFactory() throws BeansException &#123; 2 if (hasBeanFactory()) &#123;//如果已经有容器，销毁容器中的bean，关闭容器 3 destroyBeans(); 4 closeBeanFactory(); 5 &#125; 6 try &#123; 7 //创建IoC容器 8 DefaultListableBeanFactory beanFactory = createBeanFactory(); 9 beanFactory.setSerializationId(getId()); 10 //对IoC容器进行定制化，如设置启动参数，开启注解的自动装配等 11 customizeBeanFactory(beanFactory); 12 //调用载入Bean定义的方法，主要这里又使用了一个委派模式，在当前类中只定义了抽象的loadBeanDefinitions方法，具体的实现调用子类容器 13 loadBeanDefinitions(beanFactory); 14 synchronized (this.beanFactoryMonitor) &#123; 15 this.beanFactory = beanFactory; 16 &#125; 17 &#125; 18 catch (IOException ex) &#123; 19 throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); 20 &#125; 21 &#125; ;) 在这个方法中，先判断BeanFactory是否存在，如果存在则先销毁beans并关闭beanFactory，接着创建DefaultListableBeanFactory，并调用loadBeanDefinitions(beanFactory)装载bean 定义。 5、AbstractRefreshableApplicationContext子类的loadBeanDefinitions方法： AbstractRefreshableApplicationContext中只定义了抽象的loadBeanDefinitions方法，容器真正调用的是其子类AbstractXmlApplicationContext对该方法的实现，AbstractXmlApplicationContext的主要源码如下： loadBeanDefinitions方法同样是抽象方法，是由其子类实现的，也即在AbstractXmlApplicationContext中。 ;) 12345678910111213141516171819202122232425262728293031323334353637383940411 public abstract class AbstractXmlApplicationContext extends AbstractRefreshableConfigApplicationContext &#123; 2 …… 3 //实现父类抽象的载入Bean定义方法 4 @Override 5 protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; 6 //创建XmlBeanDefinitionReader，即创建Bean读取器，并通过回调设置到容器中去，容 器使用该读取器读取Bean定义资源 7 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); 8 //为Bean读取器设置Spring资源加载器，AbstractXmlApplicationContext的 9 //祖先父类AbstractApplicationContext继承DefaultResourceLoader，因此，容器本身也是一个资源加载器 10 beanDefinitionReader.setResourceLoader(this); 11 //为Bean读取器设置SAX xml解析器 12 beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); 13 //当Bean读取器读取Bean定义的Xml资源文件时，启用Xml的校验机制 14 initBeanDefinitionReader(beanDefinitionReader); 15 //Bean读取器真正实现加载的方法 16 loadBeanDefinitions(beanDefinitionReader); 17 &#125; 18 //Xml Bean读取器加载Bean定义资源 19 protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; 20 //获取Bean定义资源的定位 21 Resource[] configResources = getConfigResources(); 22 if (configResources != null) &#123; 23 //Xml Bean读取器调用其父类AbstractBeanDefinitionReader读取定位 24 //的Bean定义资源 25 reader.loadBeanDefinitions(configResources); 26 &#125; 27 //如果子类中获取的Bean定义资源定位为空，则获取FileSystemXmlApplicationContext构造方法中setConfigLocations方法设置的资源 28 String[] configLocations = getConfigLocations(); 29 if (configLocations != null) &#123; 30 //Xml Bean读取器调用其父类AbstractBeanDefinitionReader读取定位 31 //的Bean定义资源 32 reader.loadBeanDefinitions(configLocations); 33 &#125; 34 &#125; 35 //这里又使用了一个委托模式，调用子类的获取Bean定义资源定位的方法 36 //该方法在ClassPathXmlApplicationContext中进行实现，对于我们 37 //举例分析源码的FileSystemXmlApplicationContext没有使用该方法 38 protected Resource[] getConfigResources() &#123; 39 return null; 40 &#125; …… 41&#125; ;) Xml Bean读取器(XmlBeanDefinitionReader)调用其父类AbstractBeanDefinitionReader的 reader.loadBeanDefinitions方法读取Bean定义资源。 由于我们使用FileSystemXmlApplicationContext作为例子分析，因此getConfigResources的返回值为null，因此程序执行reader.loadBeanDefinitions(configLocations)分支。 6、AbstractBeanDefinitionReader读取Bean定义资源： AbstractBeanDefinitionReader的loadBeanDefinitions方法源码如下： 可以到org.springframework.beans.factory.support看一下BeanDefinitionReader的结构 在其抽象父类AbstractBeanDefinitionReader中定义了载入过程 ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556571 //重载方法，调用下面的loadBeanDefinitions(String, Set&lt;Resource&gt;);方法 2 public int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; 3 return loadBeanDefinitions(location, null); 4 &#125; 5 public int loadBeanDefinitions(String location, Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; 6 //获取在IoC容器初始化过程中设置的资源加载器 7 ResourceLoader resourceLoader = getResourceLoader(); 8 if (resourceLoader == null) &#123; 9 throw new BeanDefinitionStoreException( 10 &quot;Cannot import bean definitions from location [&quot; + location + &quot;]: no ResourceLoader available&quot;); 11 &#125; 12 if (resourceLoader instanceof ResourcePatternResolver) &#123; 13 try &#123; 14 //将指定位置的Bean定义资源文件解析为Spring IoC容器封装的资源 15 //加载多个指定位置的Bean定义资源文件 16 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); 17 //委派调用其子类XmlBeanDefinitionReader的方法，实现加载功能 18 int loadCount = loadBeanDefinitions(resources); 19 if (actualResources != null) &#123; 20 for (Resource resource : resources) &#123; 21 actualResources.add(resource); 22 &#125; 23 &#125; 24 if (logger.isDebugEnabled()) &#123; 25 logger.debug(&quot;Loaded &quot; + loadCount + &quot; bean definitions from location pattern [&quot; + location + &quot;]&quot;); 26 &#125; 27 return loadCount; 28 &#125; 29 catch (IOException ex) &#123; 30 throw new BeanDefinitionStoreException( 31 &quot;Could not resolve bean definition resource pattern [&quot; + location + &quot;]&quot;, ex); 32 &#125; 33 &#125; 34 else &#123; 35 //将指定位置的Bean定义资源文件解析为Spring IoC容器封装的资源 36 //加载单个指定位置的Bean定义资源文件 37 Resource resource = resourceLoader.getResource(location); 38 //委派调用其子类XmlBeanDefinitionReader的方法，实现加载功能 39 int loadCount = loadBeanDefinitions(resource); 40 if (actualResources != null) &#123; 41 actualResources.add(resource); 42 &#125; 43 if (logger.isDebugEnabled()) &#123; 44 logger.debug(&quot;Loaded &quot; + loadCount + &quot; bean definitions from location [&quot; + location + &quot;]&quot;); 45 &#125; 46 return loadCount; 47 &#125; 48 &#125; 49 //重载方法，调用loadBeanDefinitions(String); 50 public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; 51 Assert.notNull(locations, &quot;Location array must not be null&quot;); 52 int counter = 0; 53 for (String location : locations) &#123; 54 counter += loadBeanDefinitions(location); 55 &#125; 56 return counter; &#125; ;) loadBeanDefinitions(Resource…resources)方法和上面分析的3个方法类似，同样也是调用XmlBeanDefinitionReader的loadBeanDefinitions方法。 从对AbstractBeanDefinitionReader的loadBeanDefinitions方法源码分析可以看出该方法做了以下两件事： 首先，调用资源加载器的获取资源方法resourceLoader.getResource(location)，获取到要加载的资源。 其次，真正执行加载功能是其子类XmlBeanDefinitionReader的loadBeanDefinitions方法。 看到第8、16行，结合上面的ResourceLoader与ApplicationContext的继承关系图，可以知道此时调用的是DefaultResourceLoader中的getSource()方法定位Resource，因为FileSystemXmlApplicationContext本身就是DefaultResourceLoader的实现类，所以此时又回到了FileSystemXmlApplicationContext中来。 7、资源加载器获取要读入的资源： XmlBeanDefinitionReader通过调用其父类DefaultResourceLoader的getResource方法获取要加载的资源，其源码如下 ;) 123456789101112131415161718191 //获取Resource的具体实现方法 2 public Resource getResource(String location) &#123; 3 Assert.notNull(location, &quot;Location must not be null&quot;); 4 //如果是类路径的方式，那需要使用ClassPathResource 来得到bean 文件的资源对象 5 if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; 6 return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); 7 &#125; 8 try &#123; 9 // 如果是URL 方式，使用UrlResource 作为bean 文件的资源对象 10 URL url = new URL(location); 11 return new UrlResource(url); 12 &#125; 13 catch (MalformedURLException ex) &#123; 14 &#125; 15 //如果既不是classpath标识，又不是URL标识的Resource定位，则调用 16 //容器本身的getResourceByPath方法获取Resource 17 return getResourceByPath(location); 18 19 &#125; ;) FileSystemXmlApplicationContext容器提供了getResourceByPath方法的实现，就是为了处理既不是classpath标识，又不是URL标识的Resource定位这种情况。 ;) 1234567protected Resource getResourceByPath(String path) &#123; if (path != null &amp;&amp; path.startsWith(&quot;/&quot;)) &#123; path = path.substring(1); &#125; //这里使用文件系统资源对象来定义bean 文件 return new FileSystemResource(path); &#125; ;) 这样代码就回到了 FileSystemXmlApplicationContext 中来，他提供了FileSystemResource 来完成从文件系统得到配置文件的资源定义。 这样，就可以从文件系统路径上对IOC 配置文件进行加载 - 当然我们可以按照这个逻辑从任何地方加载，在Spring 中我们看到它提供 的各种资源抽象，比如ClassPathResource, URLResource,FileSystemResource 等来供我们使用。上面我们看到的是定位Resource 的一个过程，而这只是加载过程的一部分. ​ 8、XmlBeanDefinitionReader加载Bean定义资源： ​ ​ Bean定义的Resource得到了 ​ 继续回到XmlBeanDefinitionReader的loadBeanDefinitions(Resource …)方法看到代表bean文件的资源定义以后的载入过程。 ;) 123456789101112131415161718192021222324252627282930313233343536373839401 //XmlBeanDefinitionReader加载资源的入口方法 2 public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; 3 //将读入的XML资源进行特殊编码处理 4 return loadBeanDefinitions(new EncodedResource(resource)); 5 &#125; //这里是载入XML形式Bean定义资源文件方法6 public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; 7 ....... 8 try &#123; 9 //将资源文件转为InputStream的IO流 10 InputStream inputStream = encodedResource.getResource().getInputStream(); 11 try &#123; 12 //从InputStream中得到XML的解析源 13 InputSource inputSource = new InputSource(inputStream); 14 if (encodedResource.getEncoding() != null) &#123; 15 inputSource.setEncoding(encodedResource.getEncoding()); 16 &#125; 17 //这里是具体的读取过程 18 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); 19 &#125; 20 finally &#123; 21 //关闭从Resource中得到的IO流 22 inputStream.close(); 23 &#125; 24 &#125; 25 ......... 26&#125; 27 //从特定XML文件中实际载入Bean定义资源的方法 28 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) 29 throws BeanDefinitionStoreException &#123; 30 try &#123; 31 int validationMode = getValidationModeForResource(resource); 32 //将XML文件转换为DOM对象，解析过程由documentLoader实现 33 Document doc = this.documentLoader.loadDocument( 34 inputSource, this.entityResolver, this.errorHandler, validationMode, this.namespaceAware); 35 //这里是启动对Bean定义解析的详细过程，该解析过程会用到Spring的Bean配置规则36 return registerBeanDefinitions(doc, resource); 37 &#125; 38 ....... &#125; ;) 通过源码分析，载入Bean定义资源文件的最后一步是将Bean定义资源转换为Document对象，该过程由documentLoader实现 ​ 9、DocumentLoader将Bean定义资源转换为Document对象： ​ DocumentLoader将Bean定义资源转换成Document对象的源码如下： ;) 12345678910111213141516171819202122232425262728293031323334353637381 //使用标准的JAXP将载入的Bean定义资源转换成document对象 2 public Document loadDocument(InputSource inputSource, EntityResolver entityResolver, 3 ErrorHandler errorHandler, int validationMode, boolean namespaceAware) throws Exception &#123; 4 //创建文件解析器工厂 5 DocumentBuilderFactory factory = createDocumentBuilderFactory(validationMode, namespaceAware); 6 if (logger.isDebugEnabled()) &#123; 7 logger.debug(&quot;Using JAXP provider [&quot; + factory.getClass().getName() + &quot;]&quot;); 8 &#125; 9 //创建文档解析器 10 DocumentBuilder builder = createDocumentBuilder(factory, entityResolver, errorHandler); 11 //解析Spring的Bean定义资源 12 return builder.parse(inputSource); 13 &#125; 14 protected DocumentBuilderFactory createDocumentBuilderFactory(int validationMode, boolean namespaceAware) 15 throws ParserConfigurationException &#123; 16 //创建文档解析工厂 17 DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance(); 18 factory.setNamespaceAware(namespaceAware); 19 //设置解析XML的校验 20 if (validationMode != XmlValidationModeDetector.VALIDATION_NONE) &#123; 21 factory.setValidating(true); 22 if (validationMode == XmlValidationModeDetector.VALIDATION_XSD) &#123; 23 factory.setNamespaceAware(true); 24 try &#123; 25 factory.setAttribute(SCHEMA_LANGUAGE_ATTRIBUTE, XSD_SCHEMA_LANGUAGE); 26 &#125; 27 catch (IllegalArgumentException ex) &#123; 28 ParserConfigurationException pcex = new ParserConfigurationException( 29 &quot;Unable to validate using XSD: Your JAXP provider [&quot; + factory + 30 &quot;] does not support XML Schema. Are you running on Java 1.4 with Apache Crimson? &quot; + 31 &quot;Upgrade to Apache Xerces (or Java 1.5) for full XSD support.&quot;); 32 pcex.initCause(ex); 33 throw pcex; 34 &#125; 35 &#125; 36 &#125; 37 return factory; 38 &#125; ;) 该解析过程调用JavaEE标准的JAXP标准进行处理。 至此Spring IoC容器根据定位的Bean定义资源文件，将其加载读入并转换成为Document对象过程完成。 接下来我们要继续分析Spring IoC容器将载入的Bean定义资源文件转换为Document对象之后，是如何将其解析为Spring IoC管理的Bean对象并将其注册到容器中的。 10、XmlBeanDefinitionReader解析载入的Bean定义资源文件： XmlBeanDefinitionReader类中的doLoadBeanDefinitions方法是从特定XML文件中实际载入Bean定义资源的方法，该方法在载入Bean定义资源之后将其转换为Document对象，接下来调用registerBeanDefinitions启动Spring IoC容器对Bean定义的解析过程，registerBeanDefinitions方法源码如下： ;) 1234567891011121314151 //按照Spring的Bean语义要求将Bean定义资源解析并转换为容器内部数据结构 2 public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; 3 //得到BeanDefinitionDocumentReader来对xml格式的BeanDefinition解析 4 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); 5 //获得容器中注册的Bean数量 6 int countBefore = getRegistry().getBeanDefinitionCount(); 7 //解析过程入口，这里使用了委派模式，BeanDefinitionDocumentReader只是个接口，//具体的解析实现过程有实现类DefaultBeanDefinitionDocumentReader完成 8 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); 9 //统计解析的Bean数量 10 return getRegistry().getBeanDefinitionCount() - countBefore; 11 &#125; 12 //创建BeanDefinitionDocumentReader对象，解析Document对象 13 protected BeanDefinitionDocumentReader createBeanDefinitionDocumentReader() &#123; 14 return BeanDefinitionDocumentReader.class.cast(BeanUtils.instantiateClass(this.documentReaderClass)); &#125; ;) Bean定义资源的载入解析分为以下两个过程： 首先，通过调用XML解析器将Bean定义资源文件转换得到Document对象，但是这些Document对象并没有按照Spring的Bean规则进行解析。这一步是载入的过程 其次，在完成通用的XML解析之后，按照Spring的Bean规则对Document对象进行解析。 按照Spring的Bean规则对Document对象解析的过程是在接口BeanDefinitionDocumentReader的实现类DefaultBeanDefinitionDocumentReader中实现的。 11、DefaultBeanDefinitionDocumentReader对Bean定义的Document对象解析： BeanDefinitionDocumentReader接口通过registerBeanDefinitions方法调用其实现类DefaultBeanDefinitionDocumentReader对Document对象进行解析，解析的代码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881 //根据Spring DTD对Bean的定义规则解析Bean定义Document对象 2 public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; 3 //获得XML描述符 4 this.readerContext = readerContext; 5 logger.debug(&quot;Loading bean definitions&quot;); 6 //获得Document的根元素 7 Element root = doc.getDocumentElement(); 8 //具体的解析过程由BeanDefinitionParserDelegate实现， 9 //BeanDefinitionParserDelegate中定义了Spring Bean定义XML文件的各种元素 10 BeanDefinitionParserDelegate delegate = createHelper(readerContext, root); 11 //在解析Bean定义之前，进行自定义的解析，增强解析过程的可扩展性 12 preProcessXml(root); 13 //从Document的根元素开始进行Bean定义的Document对象 14 parseBeanDefinitions(root, delegate); 15 //在解析Bean定义之后，进行自定义的解析，增加解析过程的可扩展性 16 postProcessXml(root); 17 &#125; 18 //创建BeanDefinitionParserDelegate，用于完成真正的解析过程 19 protected BeanDefinitionParserDelegate createHelper(XmlReaderContext readerContext, Element root) &#123; 20 BeanDefinitionParserDelegate delegate = new BeanDefinitionParserDelegate(readerContext); 21 //BeanDefinitionParserDelegate初始化Document根元素 22 delegate.initDefaults(root); 23 return delegate; 24 &#125; 25 //使用Spring的Bean规则从Document的根元素开始进行Bean定义的Document对象 26 protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; 27 //Bean定义的Document对象使用了Spring默认的XML命名空间 28 if (delegate.isDefaultNamespace(root)) &#123; 29 //获取Bean定义的Document对象根元素的所有子节点 30 NodeList nl = root.getChildNodes(); 31 for (int i = 0; i &lt; nl.getLength(); i++) &#123; 32 Node node = nl.item(i); 33 //获得Document节点是XML元素节点 34 if (node instanceof Element) &#123; 35 Element ele = (Element) node; 36 //Bean定义的Document的元素节点使用的是Spring默认的XML命名空间 37 if (delegate.isDefaultNamespace(ele)) &#123; 38 //使用Spring的Bean规则解析元素节点 39 parseDefaultElement(ele, delegate); 40 &#125; 41 else &#123; 42 //没有使用Spring默认的XML命名空间，则使用用户自定义的解//析规则解析元素节点 43 delegate.parseCustomElement(ele); 44 &#125; 45 &#125; 46 &#125; 47 &#125; 48 else &#123; 49 //Document的根节点没有使用Spring默认的命名空间，则使用用户自定义的 50 //解析规则解析Document根节点 51 delegate.parseCustomElement(root); 52 &#125; 53 &#125; 54 //使用Spring的Bean规则解析Document元素节点 55 private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; 56 //如果元素节点是&lt;Import&gt;导入元素，进行导入解析 57 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; 58 importBeanDefinitionResource(ele); 59 &#125; 60 //如果元素节点是&lt;Alias&gt;别名元素，进行别名解析 61 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; 62 processAliasRegistration(ele); 63 &#125; 64 //元素节点既不是导入元素，也不是别名元素，即普通的&lt;Bean&gt;元素， 65 //按照Spring的Bean规则解析元素 66 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; 67 processBeanDefinition(ele, delegate); 68 &#125; 69 &#125; 70 //解析&lt;Import&gt;导入元素，从给定的导入路径加载Bean定义资源到Spring IoC容器中 71 protected void importBeanDefinitionResource(Element ele) &#123; 72 //获取给定的导入元素的location属性 73 String location = ele.getAttribute(RESOURCE_ATTRIBUTE); 74 //如果导入元素的location属性值为空，则没有导入任何资源，直接返回 75 if (!StringUtils.hasText(location)) &#123; 76 getReaderContext().error(&quot;Resource location must not be empty&quot;, ele); 77 return; 78 &#125; 79 //使用系统变量值解析location属性值 80 location = SystemPropertyUtils.resolvePlaceholders(location); 81 Set&lt;Resource&gt; actualResources = new LinkedHashSet&lt;Resource&gt;(4); 82 //标识给定的导入元素的location是否是绝对路径 83 boolean absoluteLocation = false; 84 try &#123; 85 absoluteLocation = ResourcePatternUtils.isUrl(location) || ResourceUtils.toURI(location).isAbsolute(); 86 &#125; 87 catch (URISyntaxException ex) &#123; 88 //给定的导入元素的location不是绝对路径 89 &#125; 90 //给定的导入元素的location是绝对路径 91 if (absoluteLocation) &#123; 92 try &#123; 93 //使用资源读入器加载给定路径的Bean定义资源 94 int importCount = getReaderContext().getReader().loadBeanDefinitions(location, actualResources); 95 if (logger.isDebugEnabled()) &#123; 96 logger.debug(&quot;Imported &quot; + importCount + &quot; bean definitions from URL location [&quot; + location + &quot;]&quot;); 97 &#125; 98 &#125; 99 catch (BeanDefinitionStoreException ex) &#123; 100 getReaderContext().error( 101 &quot;Failed to import bean definitions from URL location [&quot; + location + &quot;]&quot;, ele, ex); 102 &#125; 103 &#125; 104 else &#123; 105 //给定的导入元素的location是相对路径 106 try &#123; 107 int importCount; 108 //将给定导入元素的location封装为相对路径资源 109 Resource relativeResource = getReaderContext().getResource().createRelative(location); 110 //封装的相对路径资源存在 111 if (relativeResource.exists()) &#123; 112 //使用资源读入器加载Bean定义资源 113 importCount = getReaderContext().getReader().loadBeanDefinitions(relativeResource); 114 actualResources.add(relativeResource); 115 &#125; 116 //封装的相对路径资源不存在 117 else &#123; 118 //获取Spring IoC容器资源读入器的基本路径 119 String baseLocation = getReaderContext().getResource().getURL().toString(); 120 //根据Spring IoC容器资源读入器的基本路径加载给定导入 121 //路径的资源 122 importCount = getReaderContext().getReader().loadBeanDefinitions( 123 StringUtils.applyRelativePath(baseLocation, location), actualResources); 124 &#125; 125 if (logger.isDebugEnabled()) &#123; 126 logger.debug(&quot;Imported &quot; + importCount + &quot; bean definitions from relative location [&quot; + location + &quot;]&quot;); 127 &#125; 128 &#125; 129 catch (IOException ex) &#123; 130 getReaderContext().error(&quot;Failed to resolve current resource location&quot;, ele, ex); 131 &#125; 132 catch (BeanDefinitionStoreException ex) &#123; 133 getReaderContext().error(&quot;Failed to import bean definitions from relative location [&quot; + location + &quot;]&quot;, 134 ele, ex); 135 &#125; 136 &#125; 137 Resource[] actResArray = actualResources.toArray(new Resource[actualResources.size()]); 138 //在解析完&lt;Import&gt;元素之后，发送容器导入其他资源处理完成事件 139 getReaderContext().fireImportProcessed(location, actResArray, extractSource(ele)); 140 &#125; 141 //解析&lt;Alias&gt;别名元素，为Bean向Spring IoC容器注册别名 142 protected void processAliasRegistration(Element ele) &#123; 143 //获取&lt;Alias&gt;别名元素中name的属性值 144 String name = ele.getAttribute(NAME_ATTRIBUTE); 145 //获取&lt;Alias&gt;别名元素中alias的属性值 146 String alias = ele.getAttribute(ALIAS_ATTRIBUTE); 147 boolean valid = true; 148 //&lt;alias&gt;别名元素的name属性值为空 149 if (!StringUtils.hasText(name)) &#123; 150 getReaderContext().error(&quot;Name must not be empty&quot;, ele); 151 valid = false; 152 &#125; 153 //&lt;alias&gt;别名元素的alias属性值为空 154 if (!StringUtils.hasText(alias)) &#123; 155 getReaderContext().error(&quot;Alias must not be empty&quot;, ele); 156 valid = false; 157 &#125; 158 if (valid) &#123; 159 try &#123; 160 //向容器的资源读入器注册别名 161 getReaderContext().getRegistry().registerAlias(name, alias); 162 &#125; 163 catch (Exception ex) &#123; 164 getReaderContext().error(&quot;Failed to register alias &apos;&quot; + alias + 165 &quot;&apos; for bean with name &apos;&quot; + name + &quot;&apos;&quot;, ele, ex); 166 &#125; 167 //在解析完&lt;Alias&gt;元素之后，发送容器别名处理完成事件 168 getReaderContext().fireAliasRegistered(name, alias, extractSource(ele)); 169 &#125; 170 &#125; 171 //解析Bean定义资源Document对象的普通元素 172 protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; 173 // BeanDefinitionHolder是对BeanDefinition的封装，即Bean定义的封装类 174 //对Document对象中&lt;Bean&gt;元素的解析由BeanDefinitionParserDelegate实现 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); 175 if (bdHolder != null) &#123; 176 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); 177 try &#123; 178 //向Spring IoC容器注册解析得到的Bean定义，这是Bean定义向IoC容器注册的入口 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); 179 &#125; 180 catch (BeanDefinitionStoreException ex) &#123; 181 getReaderContext().error(&quot;Failed to register bean definition with name &apos;&quot; + 182 bdHolder.getBeanName() + &quot;&apos;&quot;, ele, ex); 183 &#125; 184 //在完成向Spring IoC容器注册解析得到的Bean定义之后，发送注册事件 185 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); 186 &#125; 187 &#125; ;) 通过上述Spring IoC容器对载入的Bean定义Document解析可以看出，我们使用Spring时，在Spring配置文件中可以使用元素来导入IoC容器所需要的其他资源，Spring IoC容器在解析时会首先将指定导入的资源加载进容器中。使用别名时，Spring IoC容器首先将别名元素所定义的别名注册到容器中。 对于既不是元素，又不是元素的元素，即Spring配置文件中普通的元素的解析由BeanDefinitionParserDelegate类的parseBeanDefinitionElement方法来实现。 12、BeanDefinitionParserDelegate解析Bean定义资源文件中的元素： Bean定义资源文件中的和元素解析在DefaultBeanDefinitionDocumentReader中已经完成，对Bean定义资源文件中使用最多的元素交由BeanDefinitionParserDelegate来解析，其解析实现的源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251 //解析&lt;Bean&gt;元素的入口 2 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; 3 return parseBeanDefinitionElement(ele, null); 4 &#125; 5 //解析Bean定义资源文件中的&lt;Bean&gt;元素，这个方法中主要处理&lt;Bean&gt;元素的id，name 6 //和别名属性 7 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) &#123; 8 //获取&lt;Bean&gt;元素中的id属性值 9 String id = ele.getAttribute(ID_ATTRIBUTE); 10 //获取&lt;Bean&gt;元素中的name属性值 11 String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); 12 ////获取&lt;Bean&gt;元素中的alias属性值 13 List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); 14 //将&lt;Bean&gt;元素中的所有name属性值存放到别名中 15 if (StringUtils.hasLength(nameAttr)) &#123; 16 String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, BEAN_NAME_DELIMITERS); 17 aliases.addAll(Arrays.asList(nameArr)); 18 &#125; 19 String beanName = id; 20 //如果&lt;Bean&gt;元素中没有配置id属性时，将别名中的第一个值赋值给beanName 21 if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; 22 beanName = aliases.remove(0); 23 if (logger.isDebugEnabled()) &#123; 24 logger.debug(&quot;No XML &apos;id&apos; specified - using &apos;&quot; + beanName + 25 &quot;&apos; as bean name and &quot; + aliases + &quot; as aliases&quot;); 26 &#125; 27 &#125; 28 //检查&lt;Bean&gt;元素所配置的id或者name的唯一性，containingBean标识&lt;Bean&gt; 29 //元素中是否包含子&lt;Bean&gt;元素 30 if (containingBean == null) &#123; 31 //检查&lt;Bean&gt;元素所配置的id、name或者别名是否重复 32 checkNameUniqueness(beanName, aliases, ele); 33 &#125; 34 //详细对&lt;Bean&gt;元素中配置的Bean定义进行解析的地方 35 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); 36 if (beanDefinition != null) &#123; 37 if (!StringUtils.hasText(beanName)) &#123; 38 try &#123; 39 if (containingBean != null) &#123; 40 //如果&lt;Bean&gt;元素中没有配置id、别名或者name，且没有包含子//&lt;Bean&gt;元素，为解析的Bean生成一个唯一beanName并注册 41 beanName = BeanDefinitionReaderUtils.generateBeanName( 42 beanDefinition, this.readerContext.getRegistry(), true); 43 &#125; 44 else &#123; 45 //如果&lt;Bean&gt;元素中没有配置id、别名或者name，且包含了子//&lt;Bean&gt;元素，为解析的Bean使用别名向IoC容器注册 46 beanName = this.readerContext.generateBeanName(beanDefinition); 47 //为解析的Bean使用别名注册时，为了向后兼容 //Spring1.2/2.0，给别名添加类名后缀 48 String beanClassName = beanDefinition.getBeanClassName(); 49 if (beanClassName != null &amp;&amp; 50 beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; 51 !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; 52 aliases.add(beanClassName); 53 &#125; 54 &#125; 55 if (logger.isDebugEnabled()) &#123; 56 logger.debug(&quot;Neither XML &apos;id&apos; nor &apos;name&apos; specified - &quot; + 57 &quot;using generated bean name [&quot; + beanName + &quot;]&quot;); 58 &#125; 59 &#125; 60 catch (Exception ex) &#123; 61 error(ex.getMessage(), ele); 62 return null; 63 &#125; 64 &#125; 65 String[] aliasesArray = StringUtils.toStringArray(aliases); 66 return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); 67 &#125; 68 //当解析出错时，返回null 69 return null; 70 &#125; 71 //详细对&lt;Bean&gt;元素中配置的Bean定义其他属性进行解析，由于上面的方法中已经对//Bean的id、name和别名等属性进行了处理，该方法中主要处理除这三个以外的其他属性数据 72 public AbstractBeanDefinition parseBeanDefinitionElement( 73 Element ele, String beanName, BeanDefinition containingBean) &#123; 74 //记录解析的&lt;Bean&gt; 75 this.parseState.push(new BeanEntry(beanName)); 76 //这里只读取&lt;Bean&gt;元素中配置的class名字，然后载入到BeanDefinition中去 77 //只是记录配置的class名字，不做实例化，对象的实例化在依赖注入时完成 78 String className = null; 79 if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; 80 className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); 81 &#125; 82 try &#123; 83 String parent = null; 84 //如果&lt;Bean&gt;元素中配置了parent属性，则获取parent属性的值 85 if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; 86 parent = ele.getAttribute(PARENT_ATTRIBUTE); 87 &#125; 88 //根据&lt;Bean&gt;元素配置的class名称和parent属性值创建BeanDefinition 89 //为载入Bean定义信息做准备 90 AbstractBeanDefinition bd = createBeanDefinition(className, parent); 91 //对当前的&lt;Bean&gt;元素中配置的一些属性进行解析和设置，如配置的单态(singleton)属性等 92 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); 93 //为&lt;Bean&gt;元素解析的Bean设置description信息 bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); 94 //对&lt;Bean&gt;元素的meta(元信息)属性解析 95 parseMetaElements(ele, bd); 96 //对&lt;Bean&gt;元素的lookup-method属性解析 97 parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); 98 //对&lt;Bean&gt;元素的replaced-method属性解析 99 parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); 100 //解析&lt;Bean&gt;元素的构造方法设置 101 parseConstructorArgElements(ele, bd); 102 //解析&lt;Bean&gt;元素的&lt;property&gt;设置 103 parsePropertyElements(ele, bd); 104 //解析&lt;Bean&gt;元素的qualifier属性 105 parseQualifierElements(ele, bd); 106 //为当前解析的Bean设置所需的资源和依赖对象 107 bd.setResource(this.readerContext.getResource()); 108 bd.setSource(extractSource(ele)); 109 return bd; 110 &#125; 111 catch (ClassNotFoundException ex) &#123; 112 error(&quot;Bean class [&quot; + className + &quot;] not found&quot;, ele, ex); 113 &#125; 114 catch (NoClassDefFoundError err) &#123; 115 error(&quot;Class that bean class [&quot; + className + &quot;] depends on not found&quot;, ele, err); 116 &#125; 117 catch (Throwable ex) &#123; 118 error(&quot;Unexpected failure during bean definition parsing&quot;, ele, ex); 119 &#125; 120 finally &#123; 121 this.parseState.pop(); 122 &#125; 123 //解析&lt;Bean&gt;元素出错时，返回null 124 return null; 125 &#125; ;) 只要使用过Spring，对Spring配置文件比较熟悉的人，通过对上述源码的分析，就会明白我们在Spring配置文件中元素的中配置的属性就是通过该方法解析和设置到Bean中去的。 注意：在解析元素过程中没有创建和实例化Bean对象，只是创建了Bean对象的定义类BeanDefinition，将元素中的配置信息设置到BeanDefinition中作为记录，当依赖注入时才使用这些记录信息创建和实例化具体的Bean对象。 上面方法中一些对一些配置如元信息(meta)、qualifier等的解析，我们在Spring中配置时使用的也不多，我们在使用Spring的元素时，配置最多的是属性，因此我们下面继续分析源码，了解Bean的属性在解析时是如何设置的。 13、BeanDefinitionParserDelegate解析元素： BeanDefinitionParserDelegate在解析调用parsePropertyElements方法解析元素中的属性子元素，解析源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021 //解析&lt;Bean&gt;元素中的&lt;property&gt;子元素 2 public void parsePropertyElements(Element beanEle, BeanDefinition bd) &#123; 3 //获取&lt;Bean&gt;元素中所有的子元素 4 NodeList nl = beanEle.getChildNodes(); 5 for (int i = 0; i &lt; nl.getLength(); i++) &#123; 6 Node node = nl.item(i); 7 //如果子元素是&lt;property&gt;子元素，则调用解析&lt;property&gt;子元素方法解析 8 if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, PROPERTY_ELEMENT)) &#123; 9 parsePropertyElement((Element) node, bd); 10 &#125; 11 &#125; 12 &#125; 13 //解析&lt;property&gt;元素 14 public void parsePropertyElement(Element ele, BeanDefinition bd) &#123; 15 //获取&lt;property&gt;元素的名字 16 String propertyName = ele.getAttribute(NAME_ATTRIBUTE); 17 if (!StringUtils.hasLength(propertyName)) &#123; 18 error(&quot;Tag &apos;property&apos; must have a &apos;name&apos; attribute&quot;, ele); 19 return; 20 &#125; 21 this.parseState.push(new PropertyEntry(propertyName)); 22 try &#123; 23 //如果一个Bean中已经有同名的property存在，则不进行解析，直接返回。 24 //即如果在同一个Bean中配置同名的property，则只有第一个起作用 25 if (bd.getPropertyValues().contains(propertyName)) &#123; 26 error(&quot;Multiple &apos;property&apos; definitions for property &apos;&quot; + propertyName + &quot;&apos;&quot;, ele); 27 return; 28 &#125; 29 //解析获取property的值 30 Object val = parsePropertyValue(ele, bd, propertyName); 31 //根据property的名字和值创建property实例 32 PropertyValue pv = new PropertyValue(propertyName, val); 33 //解析&lt;property&gt;元素中的属性 34 parseMetaElements(ele, pv); 35 pv.setSource(extractSource(ele)); 36 bd.getPropertyValues().addPropertyValue(pv); 37 &#125; 38 finally &#123; 39 this.parseState.pop(); 40 &#125; 41 &#125; 42 //解析获取property值 43 public Object parsePropertyValue(Element ele, BeanDefinition bd, String propertyName) &#123; 44 String elementName = (propertyName != null) ? 45 &quot;&lt;property&gt; element for property &apos;&quot; + propertyName + &quot;&apos;&quot; : 46 &quot;&lt;constructor-arg&gt; element&quot;; 47 //获取&lt;property&gt;的所有子元素，只能是其中一种类型:ref,value,list等 48 NodeList nl = ele.getChildNodes(); 49 Element subElement = null; 50 for (int i = 0; i &lt; nl.getLength(); i++) &#123; 51 Node node = nl.item(i); 52 //子元素不是description和meta属性 53 if (node instanceof Element &amp;&amp; !nodeNameEquals(node, DESCRIPTION_ELEMENT) &amp;&amp; 54 !nodeNameEquals(node, META_ELEMENT)) &#123; 55 if (subElement != null) &#123; 56 error(elementName + &quot; must not contain more than one sub-element&quot;, ele); 57 &#125; 58 else &#123;//当前&lt;property&gt;元素包含有子元素 59 subElement = (Element) node; 60 &#125; 61 &#125; 62 &#125; 63 //判断property的属性值是ref还是value，不允许既是ref又是value 64 boolean hasRefAttribute = ele.hasAttribute(REF_ATTRIBUTE); 65 boolean hasValueAttribute = ele.hasAttribute(VALUE_ATTRIBUTE); 66 if ((hasRefAttribute &amp;&amp; hasValueAttribute) || 67 ((hasRefAttribute || hasValueAttribute) &amp;&amp; subElement != null)) &#123; 68 error(elementName + 69 &quot; is only allowed to contain either &apos;ref&apos; attribute OR &apos;value&apos; attribute OR sub-element&quot;, ele); 70 &#125; 71 //如果属性是ref，创建一个ref的数据对象RuntimeBeanReference，这个对象 72 //封装了ref信息 73 if (hasRefAttribute) &#123; 74 String refName = ele.getAttribute(REF_ATTRIBUTE); 75 if (!StringUtils.hasText(refName)) &#123; 76 error(elementName + &quot; contains empty &apos;ref&apos; attribute&quot;, ele); 77 &#125; 78 //一个指向运行时所依赖对象的引用 79 RuntimeBeanReference ref = new RuntimeBeanReference(refName); 80 //设置这个ref的数据对象是被当前的property对象所引用 81 ref.setSource(extractSource(ele)); 82 return ref; 83 &#125; 84 //如果属性是value，创建一个value的数据对象TypedStringValue，这个对象 85 //封装了value信息 86 else if (hasValueAttribute) &#123; 87 //一个持有String类型值的对象 88 TypedStringValue valueHolder = new TypedStringValue(ele.getAttribute(VALUE_ATTRIBUTE)); 89 //设置这个value数据对象是被当前的property对象所引用 90 valueHolder.setSource(extractSource(ele)); 91 return valueHolder; 92 &#125; 93 //如果当前&lt;property&gt;元素还有子元素 94 else if (subElement != null) &#123; 95 //解析&lt;property&gt;的子元素 96 return parsePropertySubElement(subElement, bd); 97 &#125; 98 else &#123; 99 //propery属性中既不是ref，也不是value属性，解析出错返回null error(elementName + &quot; must specify a ref or value&quot;, ele); 100 return null; 101 &#125; &#125; ;) 通过对上述源码的分析，我们可以了解在Spring配置文件中，元素中元素的相关配置是如何处理的： a. ref被封装为指向依赖对象一个引用。 b.value配置都会封装成一个字符串类型的对象。 c.ref和value都通过“解析的数据类型属性值.setSource(extractSource(ele));”方法将属性值/引用与所引用的属性关联起来。 在方法的最后对于元素的子元素通过parsePropertySubElement 方法解析，我们继续分析该方法的源码，了解其解析过程。 14、解析元素的子元素： 在BeanDefinitionParserDelegate类中的parsePropertySubElement方法对中的子元素解析，源码如下： ;) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384851 //解析&lt;property&gt;元素中ref,value或者集合等子元素 2 public Object parsePropertySubElement(Element ele, BeanDefinition bd, String defaultValueType) &#123; 3 //如果&lt;property&gt;没有使用Spring默认的命名空间，则使用用户自定义的规则解析//内嵌元素 4 if (!isDefaultNamespace(ele)) &#123; 5 return parseNestedCustomElement(ele, bd); 6 &#125; 7 //如果子元素是bean，则使用解析&lt;Bean&gt;元素的方法解析 8 else if (nodeNameEquals(ele, BEAN_ELEMENT)) &#123; 9 BeanDefinitionHolder nestedBd = parseBeanDefinitionElement(ele, bd); 10 if (nestedBd != null) &#123; 11 nestedBd = decorateBeanDefinitionIfRequired(ele, nestedBd, bd); 12 &#125; 13 return nestedBd; 14 &#125; 15 //如果子元素是ref，ref中只能有以下3个属性：bean、local、parent 16 else if (nodeNameEquals(ele, REF_ELEMENT)) &#123; 17 //获取&lt;property&gt;元素中的bean属性值，引用其他解析的Bean的名称 18 //可以不再同一个Spring配置文件中，具体请参考Spring对ref的配置规则 19 String refName = ele.getAttribute(BEAN_REF_ATTRIBUTE); 20 boolean toParent = false; 21 if (!StringUtils.hasLength(refName)) &#123; 22 //获取&lt;property&gt;元素中的local属性值，引用同一个Xml文件中配置 23 //的Bean的id，local和ref不同，local只能引用同一个配置文件中的Bean 24 refName = ele.getAttribute(LOCAL_REF_ATTRIBUTE); 25 if (!StringUtils.hasLength(refName)) &#123; 26 //获取&lt;property&gt;元素中parent属性值，引用父级容器中的Bean 27 refName = ele.getAttribute(PARENT_REF_ATTRIBUTE); 28 toParent = true; 29 if (!StringUtils.hasLength(refName)) &#123; 30 error(&quot;&apos;bean&apos;, &apos;local&apos; or &apos;parent&apos; is required for &lt;ref&gt; element&quot;, ele); 31 return null; 32 &#125; 33 &#125; 34 &#125; 35 //没有配置ref的目标属性值 36 if (!StringUtils.hasText(refName)) &#123; 37 error(&quot;&lt;ref&gt; element contains empty target attribute&quot;, ele); 38 return null; 39 &#125; 40 //创建ref类型数据，指向被引用的对象 41 RuntimeBeanReference ref = new RuntimeBeanReference(refName, toParent); 42 //设置引用类型值是被当前子元素所引用 43 ref.setSource(extractSource(ele)); 44 return ref; 45 &#125; 46 //如果子元素是&lt;idref&gt;，使用解析ref元素的方法解析 47 else if (nodeNameEquals(ele, IDREF_ELEMENT)) &#123; 48 return parseIdRefElement(ele); 49 &#125; 50 //如果子元素是&lt;value&gt;，使用解析value元素的方法解析 51 else if (nodeNameEquals(ele, VALUE_ELEMENT)) &#123; 52 return parseValueElement(ele, defaultValueType); 53 &#125; 54 //如果子元素是null，为&lt;property&gt;设置一个封装null值的字符串数据 55 else if (nodeNameEquals(ele, NULL_ELEMENT)) &#123; 56 TypedStringValue nullHolder = new TypedStringValue(null); 57 nullHolder.setSource(extractSource(ele)); 58 return nullHolder; 59 &#125; 60 //如果子元素是&lt;array&gt;，使用解析array集合子元素的方法解析 61 else if (nodeNameEquals(ele, ARRAY_ELEMENT)) &#123; 62 return parseArrayElement(ele, bd); 63 &#125; 64 //如果子元素是&lt;list&gt;，使用解析list集合子元素的方法解析 65 else if (nodeNameEquals(ele, LIST_ELEMENT)) &#123; 66 return parseListElement(ele, bd); 67 &#125; 68 //如果子元素是&lt;set&gt;，使用解析set集合子元素的方法解析 69 else if (nodeNameEquals(ele, SET_ELEMENT)) &#123; 70 return parseSetElement(ele, bd); 71 &#125; 72 //如果子元素是&lt;map&gt;，使用解析map集合子元素的方法解析 73 else if (nodeNameEquals(ele, MAP_ELEMENT)) &#123; 74 return parseMapElement(ele, bd); 75 &#125; 76 //如果子元素是&lt;props&gt;，使用解析props集合子元素的方法解析 77 else if (nodeNameEquals(ele, PROPS_ELEMENT)) &#123; 78 return parsePropsElement(ele); 79 &#125; 80 //既不是ref，又不是value，也不是集合，则子元素配置错误，返回null 81 else &#123; 82 error(&quot;Unknown property sub-element: [&quot; + ele.getNodeName() + &quot;]&quot;, ele); 83 return null; 84 &#125; &#125; ;) 通过上述源码分析，我们明白了在Spring配置文件中，对元素中配置的Array、List、Set、Map、Prop等各种集合子元素的都通过上述方法解析，生成对应的数据对象，比如ManagedList、ManagedArray、ManagedSet等，这些Managed类是Spring对象BeanDefiniton的数据封装，对集合数据类型的具体解析有各自的解析方法实现，解析方法的命名非常规范，一目了然，我们对集合元素的解析方法进行源码分析，了解其实现过程。 15、解析子元素： 在BeanDefinitionParserDelegate类中的parseListElement方法就是具体实现解析元素中的集合子元素，源码如下： ;) 12345678910111213141516171819202122232425262728291 //解析&lt;list&gt;集合子元素 2 public List parseListElement(Element collectionEle, BeanDefinition bd) &#123; 3 //获取&lt;list&gt;元素中的value-type属性，即获取集合元素的数据类型 4 String defaultElementType = collectionEle.getAttribute(VALUE_TYPE_ATTRIBUTE); 5 //获取&lt;list&gt;集合元素中的所有子节点 6 NodeList nl = collectionEle.getChildNodes(); 7 //Spring中将List封装为ManagedList 8 ManagedList&lt;Object&gt; target = new ManagedList&lt;Object&gt;(nl.getLength()); 9 target.setSource(extractSource(collectionEle)); 10 //设置集合目标数据类型 11 target.setElementTypeName(defaultElementType); 12 target.setMergeEnabled(parseMergeAttribute(collectionEle)); 13 //具体的&lt;list&gt;元素解析 14 parseCollectionElements(nl, target, bd, defaultElementType); 15 return target; 16 &#125; 17 //具体解析&lt;list&gt;集合元素，&lt;array&gt;、&lt;list&gt;和&lt;set&gt;都使用该方法解析 18 protected void parseCollectionElements( 19 NodeList elementNodes, Collection&lt;Object&gt; target, BeanDefinition bd, String defaultElementType) &#123; 20 //遍历集合所有节点 21 for (int i = 0; i &lt; elementNodes.getLength(); i++) &#123; 22 Node node = elementNodes.item(i); 23 //节点不是description节点 24 if (node instanceof Element &amp;&amp; !nodeNameEquals(node, DESCRIPTION_ELEMENT)) &#123; 25 //将解析的元素加入集合中，递归调用下一个子元素 26 target.add(parsePropertySubElement((Element) node, bd, defaultElementType)); 27 &#125; 28 &#125; &#125; ;) 经过对Spring Bean定义资源文件转换的Document对象中的元素层层解析，Spring IoC现在已经将XML形式定义的Bean定义资源文件转换为Spring IoC所识别的数据结构——BeanDefinition，它是Bean定义资源文件中配置的POJO对象在Spring IoC容器中的映射，我们可以通过AbstractBeanDefinition为入口，荣IoC容器进行索引、查询和操作。 通过Spring IoC容器对Bean定义资源的解析后，IoC容器大致完成了管理Bean对象的准备工作，即初始化过程，但是最为重要的依赖注入还没有发生，现在在IoC容器中BeanDefinition存储的只是一些静态信息，接下来需要向容器注册Bean定义信息才能全部完成IoC容器的初始化过程 16、解析过后的BeanDefinition在IoC容器中的注册： 让我们继续跟踪程序的执行顺序，接下来会到我们第3步中分析DefaultBeanDefinitionDocumentReader对Bean定义转换的Document对象解析的流程中，在其parseDefaultElement方法中完成对Document对象的解析后得到封装BeanDefinition的BeanDefinitionHold对象，然后调用BeanDefinitionReaderUtils的registerBeanDefinition方法向IoC容器注册解析的Bean，BeanDefinitionReaderUtils的注册的源码如下： ;) 123456789101112131415//将解析的BeanDefinitionHold注册到容器中 public static void registerBeanDefinition(BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; //获取解析的BeanDefinition的名称 String beanName = definitionHolder.getBeanName(); //向IoC容器注册BeanDefinition registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); //如果解析的BeanDefinition有别名，向容器为其注册别名 String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String aliase : aliases) &#123; registry.registerAlias(beanName, aliase); &#125; &#125; &#125; ;) 当调用BeanDefinitionReaderUtils向IoC容器注册解析的BeanDefinition时，真正完成注册功能的是DefaultListableBeanFactory。 17、DefaultListableBeanFactory向IoC容器注册解析后的BeanDefinition： DefaultListableBeanFactory中使用一个HashMap的集合对象存放IoC容器中注册解析的BeanDefinition，向IoC容器注册的主要源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344451 //存储注册的俄BeanDefinition 2 private final Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String, BeanDefinition&gt;(); 3 //向IoC容器注册解析的BeanDefiniton 4 public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) 5 throws BeanDefinitionStoreException &#123; 6 Assert.hasText(beanName, &quot;Bean name must not be empty&quot;); 7 Assert.notNull(beanDefinition, &quot;BeanDefinition must not be null&quot;); 8 //校验解析的BeanDefiniton 9 if (beanDefinition instanceof AbstractBeanDefinition) &#123; 10 try &#123; 11 ((AbstractBeanDefinition) beanDefinition).validate(); 12 &#125; 13 catch (BeanDefinitionValidationException ex) &#123; 14 throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, 15 &quot;Validation of bean definition failed&quot;, ex); 16 &#125; 17 &#125; 18 //注册的过程中需要线程同步，以保证数据的一致性 19 synchronized (this.beanDefinitionMap) &#123; 20 Object oldBeanDefinition = this.beanDefinitionMap.get(beanName); 21 //检查是否有同名的BeanDefinition已经在IoC容器中注册，如果已经注册， 22 //并且不允许覆盖已注册的Bean，则抛出注册失败异常 23 if (oldBeanDefinition != null) &#123; 24 if (!this.allowBeanDefinitionOverriding) &#123; 25 throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, 26 &quot;Cannot register bean definition [&quot; + beanDefinition + &quot;] for bean &apos;&quot; + beanName + 27 &quot;&apos;: There is already [&quot; + oldBeanDefinition + &quot;] bound.&quot;); 28 &#125; 29 else &#123;//如果允许覆盖，则同名的Bean，后注册的覆盖先注册的 30 if (this.logger.isInfoEnabled()) &#123; 31 this.logger.info(&quot;Overriding bean definition for bean &apos;&quot; + beanName + 32 &quot;&apos;: replacing [&quot; + oldBeanDefinition + &quot;] with [&quot; + beanDefinition + &quot;]&quot;); 33 &#125; 34 &#125; 35 &#125; 36 //IoC容器中没有已经注册同名的Bean，按正常注册流程注册 37 else &#123; 38 this.beanDefinitionNames.add(beanName); 39 this.frozenBeanDefinitionNames = null; 40 &#125; 41 this.beanDefinitionMap.put(beanName, beanDefinition); 42 //重置所有已经注册过的BeanDefinition的缓存 43 resetBeanDefinition(beanName); 44 &#125; &#125; ;) 至此，Bean定义资源文件中配置的Bean被解析过后，已经注册到IoC容器中，被容器管理起来，真正完成了IoC容器初始化所做的全部工作。现 在IoC容器中已经建立了整个Bean的配置信息，这些BeanDefinition信息已经可以使用，并且可以被检索，IoC容器的作用就是对这些注册的Bean定义信息进行处理和维护。这些的注册的Bean定义信息是IoC容器控制反转的基础，正是有了这些注册的数据，容器才可以进行依赖注入。 总结： ​ 现在通过上面的代码，总结一下IOC容器初始化的基本步骤： u 初始化的入口在容器实现中的 refresh()调用来完成 u 对 bean 定义载入 IOC 容器使用的方法是 loadBeanDefinition,其中的大致过程如下：通过 ResourceLoader 来完成资源文件位置的定位，DefaultResourceLoader 是默认的实现，同时上下文本身就给出了 ResourceLoader 的实现，可以从类路径，文件系统, URL 等方式来定为资源位置。如果是 XmlBeanFactory作为 IOC 容器，那么需要为它指定 bean 定义的资源，也就是说 bean 定义文件时通过抽象成 Resource 来被 IOC 容器处理的，容器通过 BeanDefinitionReader来完成定义信息的解析和 Bean 信息的注册,往往使用的是XmlBeanDefinitionReader 来解析 bean 的 xml 定义文件 - 实际的处理过程是委托给 BeanDefinitionParserDelegate 来完成的，从而得到 bean 的定义信息，这些信息在 Spring 中使用 BeanDefinition 对象来表示 - 这个名字可以让我们想到loadBeanDefinition,RegisterBeanDefinition 这些相关的方法 - 他们都是为处理 BeanDefinitin 服务的， 容器解析得到 BeanDefinitionIoC 以后，需要把它在 IOC 容器中注册，这由 IOC 实现 BeanDefinitionRegistry 接口来实现。注册过程就是在 IOC 容器内部维护的一个HashMap 来保存得到的 BeanDefinition 的过程。这个 HashMap 是 IoC 容器持有 bean 信息的场所，以后对 bean 的操作都是围绕这个HashMap 来实现的. u 然后我们就可以通过 BeanFactory 和 ApplicationContext 来享受到 Spring IOC 的服务了,在使用 IOC 容器的时候，我们注意到除了少量粘合代码，绝大多数以正确 IoC 风格编写的应用程序代码完全不用关心如何到达工厂，因为容器将把这些对象与容器管理的其他对象钩在一起。基本的策略是把工厂放到已知的地方，最好是放在对预期使用的上下文有意义的地方，以及代码将实际需要访问工厂的地方。 Spring 本身提供了对声明式载入 web 应用程序用法的应用程序上下文,并将其存储在ServletContext 中的框架实现。具体可以参见以后的文章 在使用 Spring IOC 容器的时候我们还需要区别两个概念: ​ Beanfactory 和 Factory bean，其中 BeanFactory 指的是 IOC 容器的编程抽象，比如 ApplicationContext， XmlBeanFactory 等，这些都是 IOC 容器的具体表现，需要使用什么样的容器由客户决定,但 Spring 为我们提供了丰富的选择。 FactoryBean 只是一个可以在 IOC而容器中被管理的一个 bean,是对各种处理过程和资源使用的抽象,Factory bean 在需要时产生另一个对象，而不返回 FactoryBean本身,我们可以把它看成是一个抽象工厂，对它的调用返回的是工厂生产的产品。所有的 Factory bean 都实现特殊的org.springframework.beans.factory.FactoryBean 接口，当使用容器中 factory bean 的时候，该容器不会返回 factory bean 本身,而是返回其生成的对象。Spring 包括了大部分的通用资源和服务访问抽象的 Factory bean 的实现，其中包括:对 JNDI 查询的处理，对代理对象的处理，对事务性代理的处理，对 RMI 代理的处理等，这些我们都可以看成是具体的工厂,看成是SPRING 为我们建立好的工厂。也就是说 Spring 通过使用抽象工厂模式为我们准备了一系列工厂来生产一些特定的对象,免除我们手工重复的工作，我们要使用时只需要在 IOC 容器里配置好就能很方便的使用了 ​ 四、IOC容器的依赖注入1、依赖注入发生的时间 当Spring IoC容器完成了Bean定义资源的定位、载入和解析注册以后，IoC容器中已经管理类Bean定义的相关数据，但是此时IoC容器还没有对所管理的Bean进行依赖注入，依赖注入在以下两种情况发生： (1).用户第一次通过getBean方法向IoC容索要Bean时，IoC容器触发依赖注入。 (2).当用户在Bean定义资源中为元素配置了lazy-init属性，即让容器在解析注册Bean定义时进行预实例化，触发依赖注入。 BeanFactory接口定义了Spring IoC容器的基本功能规范，是Spring IoC容器所应遵守的最底层和最基本的编程规范。BeanFactory接口中定义了几个getBean方法，就是用户向IoC容器索取管理的Bean的方法，我们通过分析其子类的具体实现，理解Spring IoC容器在用户索取Bean时如何完成依赖注入。 在BeanFactory中我们看到getBean（String…）函数，它的具体实现在AbstractBeanFactory中 2、AbstractBeanFactory通过getBean向IoC容器获取被管理的Bean： AbstractBeanFactory的getBean相关方法的源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691 //获取IoC容器中指定名称的Bean 2 public Object getBean(String name) throws BeansException &#123; 3 //doGetBean才是真正向IoC容器获取被管理Bean的过程 4 return doGetBean(name, null, null, false); 5 &#125; 6 //获取IoC容器中指定名称和类型的Bean 7 public &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException &#123; 8 //doGetBean才是真正向IoC容器获取被管理Bean的过程 9 return doGetBean(name, requiredType, null, false); 10 &#125; 11 //获取IoC容器中指定名称和参数的Bean 12 public Object getBean(String name, Object... args) throws BeansException &#123; 13 //doGetBean才是真正向IoC容器获取被管理Bean的过程 14 return doGetBean(name, null, args, false); 15 &#125; 16 //获取IoC容器中指定名称、类型和参数的Bean 17 public &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType, Object... args) throws BeansException &#123; 18 //doGetBean才是真正向IoC容器获取被管理Bean的过程 19 return doGetBean(name, requiredType, args, false); 20 &#125; 21 //真正实现向IoC容器获取Bean的功能，也是触发依赖注入功能的地方 22 @SuppressWarnings(&quot;unchecked&quot;) 23 protected &lt;T&gt; T doGetBean( 24 final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) 25 throws BeansException &#123; 26 //根据指定的名称获取被管理Bean的名称，剥离指定名称中对容器的相关依赖 27 //如果指定的是别名，将别名转换为规范的Bean名称 28 final String beanName = transformedBeanName(name); 29 Object bean; 30 //先从缓存中取是否已经有被创建过的单态类型的Bean，对于单态模式的Bean整 31 //个IoC容器中只创建一次，不需要重复创建 32 Object sharedInstance = getSingleton(beanName); 33 //IoC容器创建单态模式Bean实例对象 34 if (sharedInstance != null &amp;&amp; args == null) &#123; 35 if (logger.isDebugEnabled()) &#123; 36 //如果指定名称的Bean在容器中已有单态模式的Bean被创建，直接返回 37 //已经创建的Bean 38 if (isSingletonCurrentlyInCreation(beanName)) &#123; 39 logger.debug(&quot;Returning eagerly cached instance of singleton bean &apos;&quot; + beanName + 40 &quot;&apos; that is not fully initialized yet - a consequence of a circular reference&quot;); 41 &#125; 42 else &#123; 43 logger.debug(&quot;Returning cached instance of singleton bean &apos;&quot; + beanName + &quot;&apos;&quot;); 44 &#125; 45 &#125; 46 //获取给定Bean的实例对象，主要是完成FactoryBean的相关处理 47 //注意：BeanFactory是管理容器中Bean的工厂，而FactoryBean是 48 //创建创建对象的工厂Bean，两者之间有区别 49 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); 50 &#125; 51 else &#123;//缓存没有正在创建的单态模式Bean 52 //缓存中已经有已经创建的原型模式Bean，但是由于循环引用的问题导致实 53 //例化对象失败 54 if (isPrototypeCurrentlyInCreation(beanName)) &#123; 55 throw new BeanCurrentlyInCreationException(beanName); 56 &#125; 57 //对IoC容器中是否存在指定名称的BeanDefinition进行检查，首先检查是否 58 //能在当前的BeanFactory中获取的所需要的Bean，如果不能则委托当前容器 59 //的父级容器去查找，如果还是找不到则沿着容器的继承体系向父级容器查找 60 BeanFactory parentBeanFactory = getParentBeanFactory(); 61 //当前容器的父级容器存在，且当前容器中不存在指定名称的Bean 62 if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; 63 //解析指定Bean名称的原始名称 64 String nameToLookup = originalBeanName(name); 65 if (args != null) &#123; 66 //委派父级容器根据指定名称和显式的参数查找 67 return (T) parentBeanFactory.getBean(nameToLookup, args); 68 &#125; 69 else &#123; 70 //委派父级容器根据指定名称和类型查找 71 return parentBeanFactory.getBean(nameToLookup, requiredType); 72 &#125; 73 &#125; 74 //创建的Bean是否需要进行类型验证，一般不需要 75 if (!typeCheckOnly) &#123; 76 //向容器标记指定的Bean已经被创建 77 markBeanAsCreated(beanName); 78 &#125; 79 //根据指定Bean名称获取其父级的Bean定义，主要解决Bean继承时子类 80 //合并父类公共属性问题 81 final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); 82 checkMergedBeanDefinition(mbd, beanName, args); 83 //获取当前Bean所有依赖Bean的名称 84 String[] dependsOn = mbd.getDependsOn(); 85 //如果当前Bean有依赖Bean 86 if (dependsOn != null) &#123; 87 for (String dependsOnBean : dependsOn) &#123; 88 //递归调用getBean方法，获取当前Bean的依赖Bean 89 getBean(dependsOnBean); 90 //把被依赖Bean注册给当前依赖的Bean 91 registerDependentBean(dependsOnBean, beanName); 92 &#125; 93 &#125; 94 //创建单态模式Bean的实例对象 95 if (mbd.isSingleton()) &#123; 96 //这里使用了一个匿名内部类，创建Bean实例对象，并且注册给所依赖的对象 97 sharedInstance = getSingleton(beanName, new ObjectFactory() &#123; 98 public Object getObject() throws BeansException &#123; 99 try &#123; 100 //创建一个指定Bean实例对象，如果有父级继承，则合并子//类和父类的定义 101 return createBean(beanName, mbd, args); 102 &#125; 103 catch (BeansException ex) &#123; 104 //显式地从容器单态模式Bean缓存中清除实例对象 105 destroySingleton(beanName); 106 throw ex; 107 &#125; 108 &#125; 109 &#125;); 110 //获取给定Bean的实例对象 111 bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); 112 &#125; 113 //IoC容器创建原型模式Bean实例对象 114 else if (mbd.isPrototype()) &#123; 115 //原型模式(Prototype)是每次都会创建一个新的对象 116 Object prototypeInstance = null; 117 try &#123; 118 //回调beforePrototypeCreation方法，默认的功能是注册当前创//建的原型对象 119 beforePrototypeCreation(beanName); 120 //创建指定Bean对象实例 121 prototypeInstance = createBean(beanName, mbd, args); 122 &#125; 123 finally &#123; 124 //回调afterPrototypeCreation方法，默认的功能告诉IoC容器指//定Bean的原型对象不再创建了 125 afterPrototypeCreation(beanName); 126 &#125; 127 //获取给定Bean的实例对象 128 bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); 129 &#125; 130 //要创建的Bean既不是单态模式，也不是原型模式，则根据Bean定义资源中 131 //配置的生命周期范围，选择实例化Bean的合适方法，这种在Web应用程序中 132 //比较常用，如：request、session、application等生命周期 133 else &#123; 134 String scopeName = mbd.getScope(); 135 final Scope scope = this.scopes.get(scopeName); 136 //Bean定义资源中没有配置生命周期范围，则Bean定义不合法 137 if (scope == null) &#123; 138 throw new IllegalStateException(&quot;No Scope registered for scope &apos;&quot; + scopeName + &quot;&apos;&quot;); 139 &#125; 140 try &#123; 141 //这里又使用了一个匿名内部类，获取一个指定生命周期范围的实例 142 Object scopedInstance = scope.get(beanName, new ObjectFactory() &#123; 143 public Object getObject() throws BeansException &#123; 144 beforePrototypeCreation(beanName); 145 try &#123; 146 return createBean(beanName, mbd, args); 147 &#125; 148 finally &#123; 149 afterPrototypeCreation(beanName); 150 &#125; 151 &#125; 152 &#125;); 153 //获取给定Bean的实例对象 154 bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); 155 &#125; 156 catch (IllegalStateException ex) &#123; 157 throw new BeanCreationException(beanName, 158 &quot;Scope &apos;&quot; + scopeName + &quot;&apos; is not active for the current thread; &quot; + 159 &quot;consider defining a scoped proxy for this bean if you intend to refer to it from a singleton&quot;, 160 ex); 161 &#125; 162 &#125; 163 &#125; 164 //对创建的Bean实例对象进行类型检查 165 if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isAssignableFrom(bean.getClass())) &#123; 166 throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); 167 &#125; 168 return (T) bean; 169 &#125; ;) 通过上面对向IoC容器获取Bean方法的分析，我们可以看到在Spring中，如果Bean定义的单态模式(Singleton)，则容器在创建之前先从缓存中查找，以确保整个容器中只存在一个实例对象。如果Bean定义的是原型模式(Prototype)，则容器每次都会创建一个新的实例对象。除此之外，Bean定义还可以扩展为指定其生命周期范围。 上面的源码只是定义了根据Bean定义的模式，采取的不同创建Bean实例对象的策略，具体的Bean实例对象的创建过程由实现了ObejctFactory接口的匿名内部类的createBean方法完成，ObejctFactory使用委派模式，具体的Bean实例创建过程交由其实现类AbstractAutowireCapableBeanFactory完成，我们继续分析AbstractAutowireCapableBeanFactory的createBean方法的源码，理解其创建Bean实例的具体实现过程。 3、AbstractAutowireCapableBeanFactory创建Bean实例对象： AbstractAutowireCapableBeanFactory类实现了ObejctFactory接口，创建容器指定的Bean实例对象，同时还对创建的Bean实例对象进行初始化处理。其创建Bean实例对象的方法源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311 //创建Bean实例对象 2 protected Object createBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) 3 throws BeanCreationException &#123; 4 if (logger.isDebugEnabled()) &#123; 5 logger.debug(&quot;Creating instance of bean &apos;&quot; + beanName + &quot;&apos;&quot;); 6 &#125; 7 //判断需要创建的Bean是否可以实例化，即是否可以通过当前的类加载器加载 8 resolveBeanClass(mbd, beanName); 9 //校验和准备Bean中的方法覆盖 10 try &#123; 11 mbd.prepareMethodOverrides(); 12 &#125; 13 catch (BeanDefinitionValidationException ex) &#123; 14 throw new BeanDefinitionStoreException(mbd.getResourceDescription(), 15 beanName, &quot;Validation of method overrides failed&quot;, ex); 16 &#125; 17 try &#123; 18 //如果Bean配置了初始化前和初始化后的处理器，则试图返回一个需要创建//Bean的代理对象 19 Object bean = resolveBeforeInstantiation(beanName, mbd); 20 if (bean != null) &#123; 21 return bean; 22 &#125; 23 &#125; 24 catch (Throwable ex) &#123; 25 throw new BeanCreationException(mbd.getResourceDescription(), beanName, 26 &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex); 27 &#125; 28 //创建Bean的入口 29 Object beanInstance = doCreateBean(beanName, mbd, args); 30 if (logger.isDebugEnabled()) &#123; 31 logger.debug(&quot;Finished creating instance of bean &apos;&quot; + beanName + &quot;&apos;&quot;); 32 &#125; 33 return beanInstance; 34 &#125; 35 //真正创建Bean的方法 36 protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) &#123; 37 //封装被创建的Bean对象 38 BeanWrapper instanceWrapper = null; 39 if (mbd.isSingleton())&#123;//单态模式的Bean，先从容器中缓存中获取同名Bean 40 instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); 41 &#125; 42 if (instanceWrapper == null) &#123; 43 //创建实例对象 44 instanceWrapper = createBeanInstance(beanName, mbd, args); 45 &#125; 46 final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); 47 //获取实例化对象的类型 48 Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); 49 //调用PostProcessor后置处理器 50 synchronized (mbd.postProcessingLock) &#123; 51 if (!mbd.postProcessed) &#123; 52 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); 53 mbd.postProcessed = true; 54 &#125; 55 &#125; 56 // Eagerly cache singletons to be able to resolve circular references 57 //向容器中缓存单态模式的Bean对象，以防循环引用 58 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; 59 isSingletonCurrentlyInCreation(beanName)); 60 if (earlySingletonExposure) &#123; 61 if (logger.isDebugEnabled()) &#123; 62 logger.debug(&quot;Eagerly caching bean &apos;&quot; + beanName + 63 &quot;&apos; to allow for resolving potential circular references&quot;); 64 &#125; 65 //这里是一个匿名内部类，为了防止循环引用，尽早持有对象的引用 66 addSingletonFactory(beanName, new ObjectFactory() &#123; 67 public Object getObject() throws BeansException &#123; 68 return getEarlyBeanReference(beanName, mbd, bean); 69 &#125; 70 &#125;); 71 &#125; 72 //Bean对象的初始化，依赖注入在此触发 73 //这个exposedObject在初始化完成之后返回作为依赖注入完成后的Bean 74 Object exposedObject = bean; 75 try &#123; 76 //将Bean实例对象封装，并且Bean定义中配置的属性值赋值给实例对象 77 populateBean(beanName, mbd, instanceWrapper); 78 if (exposedObject != null) &#123; 79 //初始化Bean对象 80 exposedObject = initializeBean(beanName, exposedObject, mbd); 81 &#125; 82 &#125; 83 catch (Throwable ex) &#123; 84 if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; 85 throw (BeanCreationException) ex; 86 &#125; 87 else &#123; 88 throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex); 89 &#125; 90 &#125; 91 if (earlySingletonExposure) &#123; 92 //获取指定名称的已注册的单态模式Bean对象 93 Object earlySingletonReference = getSingleton(beanName, false); 94 if (earlySingletonReference != null) &#123; 95 //根据名称获取的以注册的Bean和正在实例化的Bean是同一个 96 if (exposedObject == bean) &#123; 97 //当前实例化的Bean初始化完成 98 exposedObject = earlySingletonReference; 99 &#125; 100 //当前Bean依赖其他Bean，并且当发生循环引用时不允许新创建实例对象 101 else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; 102 String[] dependentBeans = getDependentBeans(beanName); 103 Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;String&gt;(dependentBeans.length); 104 //获取当前Bean所依赖的其他Bean 105 for (String dependentBean : dependentBeans) &#123; 106 //对依赖Bean进行类型检查 107 if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; 108 actualDependentBeans.add(dependentBean); 109 &#125; 110 &#125; 111 if (!actualDependentBeans.isEmpty()) &#123; 112 throw new BeanCurrentlyInCreationException(beanName, 113 &quot;Bean with name &apos;&quot; + beanName + &quot;&apos; has been injected into other beans [&quot; + 114 StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + 115 &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + 116 &quot;wrapped. This means that said other beans do not use the final version of the &quot; + 117 &quot;bean. This is often the result of over-eager type matching - consider using &quot; + 118 &quot;&apos;getBeanNamesOfType&apos; with the &apos;allowEagerInit&apos; flag turned off, for example.&quot;); 119 &#125; 120 &#125; 121 &#125; 122 &#125; 123 //注册完成依赖注入的Bean 124 try &#123; 125 registerDisposableBeanIfNecessary(beanName, bean, mbd); 126 &#125; 127 catch (BeanDefinitionValidationException ex) &#123; 128 throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex); 129 &#125; 130 return exposedObject; &#125; ;) 通过对方法源码的分析，我们看到具体的依赖注入实现在以下两个方法中： (1).createBeanInstance：生成Bean所包含的java对象实例。 (2).populateBean ：对Bean属性的依赖注入进行处理。 下面继续分析这两个方法的代码实现。 4、createBeanInstance方法创建Bean的java实例对象： 在createBeanInstance方法中，根据指定的初始化策略，使用静态工厂、工厂方法或者容器的自动装配特性生成java实例对象，创建对象的源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071721 //创建Bean的实例对象 2 protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; 3 //检查确认Bean是可实例化的 4 Class beanClass = resolveBeanClass(mbd, beanName); 5 //使用工厂方法对Bean进行实例化 6 if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; 7 throw new BeanCreationException(mbd.getResourceDescription(), beanName, 8 &quot;Bean class isn&apos;t public, and non-public access not allowed: &quot; + beanClass.getName()); 9 &#125; 10 if (mbd.getFactoryMethodName() != null) &#123; 11 //调用工厂方法实例化 12 return instantiateUsingFactoryMethod(beanName, mbd, args); 13 &#125; 14 //使用容器的自动装配方法进行实例化 15 boolean resolved = false; 16 boolean autowireNecessary = false; 17 if (args == null) &#123; 18 synchronized (mbd.constructorArgumentLock) &#123; 19 if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; 20 resolved = true; 21 autowireNecessary = mbd.constructorArgumentsResolved; 22 &#125; 23 &#125; 24 &#125; 25 if (resolved) &#123; 26 if (autowireNecessary) &#123; 27 //配置了自动装配属性，使用容器的自动装配实例化 28 //容器的自动装配是根据参数类型匹配Bean的构造方法 29 return autowireConstructor(beanName, mbd, null, null); 30 &#125; 31 else &#123; 32 //使用默认的无参构造方法实例化 33 return instantiateBean(beanName, mbd); 34 &#125; 35 &#125; 36 //使用Bean的构造方法进行实例化 37 Constructor[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); 38 if (ctors != null || 39 mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || 40 mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; 41 //使用容器的自动装配特性，调用匹配的构造方法实例化 42 return autowireConstructor(beanName, mbd, ctors, args); 43 &#125; 44 //使用默认的无参构造方法实例化 45 return instantiateBean(beanName, mbd); 46 &#125; 47 //使用默认的无参构造方法实例化Bean对象 48 protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; 49 try &#123; 50 Object beanInstance; 51 final BeanFactory parent = this; 52 //获取系统的安全管理接口，JDK标准的安全管理API 53 if (System.getSecurityManager() != null) &#123; 54 //这里是一个匿名内置类，根据实例化策略创建实例对象 55 beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; 56 public Object run() &#123; 57 return getInstantiationStrategy().instantiate(mbd, beanName, parent); 58 &#125; 59 &#125;, getAccessControlContext()); 60 &#125; 61 else &#123; 62 //将实例化的对象封装起来 63 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); 64 &#125; 65 BeanWrapper bw = new BeanWrapperImpl(beanInstance); 66 initBeanWrapper(bw); 67 return bw; 68 &#125; 69 catch (Throwable ex) &#123; 70 throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Instantiation of bean failed&quot;, ex); 71 &#125; 72 &#125; ;) 经过对上面的代码分析，我们可以看出，对使用工厂方法和自动装配特性的Bean的实例化相当比较清楚，调用相应的工厂方法或者参数匹配的构造方法即可完成实例化对象的工作，但是对于我们最常使用的默认无参构造方法就需要使用相应的初始化策略(JDK的反射机制或者CGLIB)来进行初始化了，在方法getInstantiationStrategy().instantiate中就具体实现类使用初始策略实例化对象。 5、SimpleInstantiationStrategy类使用默认的无参构造方法创建Bean实例化对象： 在使用默认的无参构造方法创建Bean的实例化对象时，方法getInstantiationStrategy().instantiate调用了SimpleInstantiationStrategy类中的实例化Bean的方法，其源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041421 //使用初始化策略实例化Bean对象 2 public Object instantiate(RootBeanDefinition beanDefinition, String beanName, BeanFactory owner) &#123; 3 //如果Bean定义中没有方法覆盖，则就不需要CGLIB父类类的方法 4 if (beanDefinition.getMethodOverrides().isEmpty()) &#123; 5 Constructor&lt;?&gt; constructorToUse; 6 synchronized (beanDefinition.constructorArgumentLock) &#123; 7 //获取对象的构造方法或工厂方法 8 constructorToUse = (Constructor&lt;?&gt;) beanDefinition.resolvedConstructorOrFactoryMethod; 9 //如果没有构造方法且没有工厂方法 10 if (constructorToUse == null) &#123; 11 //使用JDK的反射机制，判断要实例化的Bean是否是接口 12 final Class clazz = beanDefinition.getBeanClass(); 13 if (clazz.isInterface()) &#123; 14 throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;); 15 &#125; 16 try &#123; 17 if (System.getSecurityManager() != null) &#123; 18 //这里是一个匿名内置类，使用反射机制获取Bean的构造方法 19 constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&gt;() &#123; 20 public Constructor run() throws Exception &#123; 21 return clazz.getDeclaredConstructor((Class[]) null); 22 &#125; 23 &#125;); 24 &#125; 25 else &#123; 26 constructorToUse = clazz.getDeclaredConstructor((Class[]) null); 27 &#125; 28 beanDefinition.resolvedConstructorOrFactoryMethod = constructorToUse; 29 &#125; 30 catch (Exception ex) &#123; 31 throw new BeanInstantiationException(clazz, &quot;No default constructor found&quot;, ex); 32 &#125; 33 &#125; 34 &#125; 35 //使用BeanUtils实例化，通过反射机制调用”构造方法.newInstance(arg)”来进行实例化 36 return BeanUtils.instantiateClass(constructorToUse); 37 &#125; 38 else &#123; 39 //使用CGLIB来实例化对象 40 return instantiateWithMethodInjection(beanDefinition, beanName, owner); 41 &#125; &#125; ;) 通过上面的代码分析，我们看到了如果Bean有方法被覆盖了，则使用JDK的反射机制进行实例化，否则，使用CGLIB进行实例化。 instantiateWithMethodInjection方法调用SimpleInstantiationStrategy的子类CglibSubclassingInstantiationStrategy使用CGLIB来进行初始化，其源码如下： ;) 12345678910111213141516171 //使用CGLIB进行Bean对象实例化 2 public Object instantiate(Constructor ctor, Object[] args) &#123; 3 //CGLIB中的类 4 Enhancer enhancer = new Enhancer(); 5 //将Bean本身作为其基类 6 enhancer.setSuperclass(this.beanDefinition.getBeanClass()); 7 enhancer.setCallbackFilter(new CallbackFilterImpl()); 8 enhancer.setCallbacks(new Callback[] &#123; 9 NoOp.INSTANCE, 10 new LookupOverrideMethodInterceptor(), 11 new ReplaceOverrideMethodInterceptor() 12 &#125;); 13 //使用CGLIB的create方法生成实例对象 14 return (ctor == null) ? 15 enhancer.create() : 16 enhancer.create(ctor.getParameterTypes(), args); 17 &#125; ;) CGLIB是一个常用的字节码生成器的类库，它提供了一系列API实现java字节码的生成和转换功能。我们在学习JDK的动态代理时都知道，JDK的动态代理只能针对接口，如果一个类没有实现任何接口，要对其进行动态代理只能使用CGLIB。 6、populateBean方法对Bean属性的依赖注入： 在第3步的分析中我们已经了解到Bean的依赖注入分为以下两个过程： (1).createBeanInstance：生成Bean所包含的java对象实例。 (2).populateBean ：对Bean属性的依赖注入进行处理。 第4、5步中我们已经分析了容器初始化生成Bean所包含的Java实例对象的过程，现在我们继续分析生成对象后，Spring IoC容器是如何将Bean的属性依赖关系注入Bean实例对象中并设置好的，属性依赖注入的代码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751 //将Bean属性设置到生成的实例对象上 2 protected void populateBean(String beanName, AbstractBeanDefinition mbd, BeanWrapper bw) &#123; 3 //获取容器在解析Bean定义资源时为BeanDefiniton中设置的属性值 4 PropertyValues pvs = mbd.getPropertyValues(); 5 //实例对象为null 6 if (bw == null) &#123; 7 //属性值不为空 8 if (!pvs.isEmpty()) &#123; 9 throw new BeanCreationException( 10 mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to null instance&quot;); 11 &#125; 12 else &#123; 13 //实例对象为null，属性值也为空，不需要设置属性值，直接返回 14 return; 15 &#125; 16 &#125; 17 //在设置属性之前调用Bean的PostProcessor后置处理器 18 boolean continueWithPropertyPopulation = true; 19 if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; 20 for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; 21 if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; 22 InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; 23 if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; 24 continueWithPropertyPopulation = false; 25 break; 26 &#125; 27 &#125; 28 &#125; 29 &#125; 30 if (!continueWithPropertyPopulation) &#123; 31 return; 32 &#125; 33 //依赖注入开始，首先处理autowire自动装配的注入 34 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || 35 mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; 36 MutablePropertyValues newPvs = new MutablePropertyValues(pvs); 37 //对autowire自动装配的处理，根据Bean名称自动装配注入 38 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; 39 autowireByName(beanName, mbd, bw, newPvs); 40 &#125; 41 //根据Bean类型自动装配注入 42 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; 43 autowireByType(beanName, mbd, bw, newPvs); 44 &#125; 45 pvs = newPvs; 46 &#125; 47 //检查容器是否持有用于处理单态模式Bean关闭时的后置处理器 48 boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); 49 //Bean实例对象没有依赖，即没有继承基类 50 boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); 51 if (hasInstAwareBpps || needsDepCheck) &#123; 52 //从实例对象中提取属性描述符 53 PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw); 54 if (hasInstAwareBpps) &#123; 55 for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; 56 if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; 57 InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; 58 //使用BeanPostProcessor处理器处理属性值 59 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); 60 if (pvs == null) &#123; 61 return; 62 &#125; 63 &#125; 64 &#125; 65 &#125; 66 if (needsDepCheck) &#123; 67 //为要设置的属性进行依赖检查 68 checkDependencies(beanName, mbd, filteredPds, pvs); 69 &#125; 70 &#125; 71 //对属性进行注入 72 applyPropertyValues(beanName, mbd, bw, pvs); 73 &#125; 74 //解析并注入依赖属性的过程 75 protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) &#123; 76 if (pvs == null || pvs.isEmpty()) &#123; 77 return; 78 &#125; 79 //封装属性值 80 MutablePropertyValues mpvs = null; 81 List&lt;PropertyValue&gt; original; 82 if (System.getSecurityManager()!= null) &#123; 83 if (bw instanceof BeanWrapperImpl) &#123; 84 //设置安全上下文，JDK安全机制 85 ((BeanWrapperImpl) bw).setSecurityContext(getAccessControlContext()); 86 &#125; 87 &#125; 88 if (pvs instanceof MutablePropertyValues) &#123; 89 mpvs = (MutablePropertyValues) pvs; 90 //属性值已经转换 91 if (mpvs.isConverted()) &#123; 92 try &#123; 93 //为实例化对象设置属性值 94 bw.setPropertyValues(mpvs); 95 return; 96 &#125; 97 catch (BeansException ex) &#123; 98 throw new BeanCreationException( 99 mbd.getResourceDescription(), beanName, &quot;Error setting property values&quot;, ex); 100 &#125; 101 &#125; 102 //获取属性值对象的原始类型值 103 original = mpvs.getPropertyValueList(); 104 &#125; 105 else &#123; 106 original = Arrays.asList(pvs.getPropertyValues()); 107 &#125; 108 //获取用户自定义的类型转换 109 TypeConverter converter = getCustomTypeConverter(); 110 if (converter == null) &#123; 111 converter = bw; 112 &#125; 113 //创建一个Bean定义属性值解析器，将Bean定义中的属性值解析为Bean实例对象 114 //的实际值 115 BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); 116 //为属性的解析值创建一个拷贝，将拷贝的数据注入到实例对象中 117 List&lt;PropertyValue&gt; deepCopy = new ArrayList&lt;PropertyValue&gt;(original.size()); 118 boolean resolveNecessary = false; 119 for (PropertyValue pv : original) &#123; 120 //属性值不需要转换 121 if (pv.isConverted()) &#123; 122 deepCopy.add(pv); 123 &#125; 124 //属性值需要转换 125 else &#123; 126 String propertyName = pv.getName(); 127 //原始的属性值，即转换之前的属性值 128 Object originalValue = pv.getValue(); 129 //转换属性值，例如将引用转换为IoC容器中实例化对象引用 130 Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); 131 //转换之后的属性值 132 Object convertedValue = resolvedValue; 133 //属性值是否可以转换 134 boolean convertible = bw.isWritableProperty(propertyName) &amp;&amp; 135 !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); 136 if (convertible) &#123; 137 //使用用户自定义的类型转换器转换属性值 138 convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); 139 &#125; 140 //存储转换后的属性值，避免每次属性注入时的转换工作 141 if (resolvedValue == originalValue) &#123; 142 if (convertible) &#123; 143 //设置属性转换之后的值 144 pv.setConvertedValue(convertedValue); 145 &#125; 146 deepCopy.add(pv); 147 &#125; 148 //属性是可转换的，且属性原始值是字符串类型，且属性的原始类型值不是 149 //动态生成的字符串，且属性的原始值不是集合或者数组类型 150 else if (convertible &amp;&amp; originalValue instanceof TypedStringValue &amp;&amp; 151 !((TypedStringValue) originalValue).isDynamic() &amp;&amp; 152 !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) &#123; 153 pv.setConvertedValue(convertedValue); 154 deepCopy.add(pv); 155 &#125; 156 else &#123; 157 resolveNecessary = true; 158 //重新封装属性的值 159 deepCopy.add(new PropertyValue(pv, convertedValue)); 160 &#125; 161 &#125; 162 &#125; 163 if (mpvs != null &amp;&amp; !resolveNecessary) &#123; 164 //标记属性值已经转换过 165 mpvs.setConverted(); 166 &#125; 167 //进行属性依赖注入 168 try &#123; 169 bw.setPropertyValues(new MutablePropertyValues(deepCopy)); 170 &#125; 171 catch (BeansException ex) &#123; 172 throw new BeanCreationException( 173 mbd.getResourceDescription(), beanName, &quot;Error setting property values&quot;, ex); 174 &#125; &#125; ;) 分析上述代码，我们可以看出，对属性的注入过程分以下两种情况： (1).属性值类型不需要转换时，不需要解析属性值，直接准备进行依赖注入。 (2).属性值需要进行类型转换时，如对其他对象的引用等，首先需要解析属性值，然后对解析后的属性值进行依赖注入。 对属性值的解析是在BeanDefinitionValueResolver类中的resolveValueIfNecessary方法中进行的，对属性值的依赖注入是通过bw.setPropertyValues方法实现的，在分析属性值的依赖注入之前，我们先分析一下对属性值的解析过程。 7、BeanDefinitionValueResolver解析属性值： 当容器在对属性进行依赖注入时，如果发现属性值需要进行类型转换，如属性值是容器中另一个Bean实例对象的引用，则容器首先需要根据属性值解析出所引用的对象，然后才能将该引用对象注入到目标实例对象的属性上去，对属性进行解析的由resolveValueIfNecessary方法实现，其源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881 //解析属性值，对注入类型进行转换 2 public Object resolveValueIfNecessary(Object argName, Object value) &#123; 3 //对引用类型的属性进行解析 4 if (value instanceof RuntimeBeanReference) &#123; 5 RuntimeBeanReference ref = (RuntimeBeanReference) value; 6 //调用引用类型属性的解析方法 7 return resolveReference(argName, ref); 8 &#125; 9 //对属性值是引用容器中另一个Bean名称的解析 10 else if (value instanceof RuntimeBeanNameReference) &#123; 11 String refName = ((RuntimeBeanNameReference) value).getBeanName(); 12 refName = String.valueOf(evaluate(refName)); 13 //从容器中获取指定名称的Bean 14 if (!this.beanFactory.containsBean(refName)) &#123; 15 throw new BeanDefinitionStoreException( 16 &quot;Invalid bean name &apos;&quot; + refName + &quot;&apos; in bean reference for &quot; + argName); 17 &#125; 18 return refName; 19 &#125; 20 //对Bean类型属性的解析，主要是Bean中的内部类 21 else if (value instanceof BeanDefinitionHolder) &#123; 22 BeanDefinitionHolder bdHolder = (BeanDefinitionHolder) value; 23 return resolveInnerBean(argName, bdHolder.getBeanName(), bdHolder.getBeanDefinition()); 24 &#125; 25 else if (value instanceof BeanDefinition) &#123; 26 BeanDefinition bd = (BeanDefinition) value; 27 return resolveInnerBean(argName, &quot;(inner bean)&quot;, bd); 28 &#125; 29 //对集合数组类型的属性解析 30 else if (value instanceof ManagedArray) &#123; 31 ManagedArray array = (ManagedArray) value; 32 //获取数组的类型 33 Class elementType = array.resolvedElementType; 34 if (elementType == null) &#123; 35 //获取数组元素的类型 36 String elementTypeName = array.getElementTypeName(); 37 if (StringUtils.hasText(elementTypeName)) &#123; 38 try &#123; 39 //使用反射机制创建指定类型的对象 40 elementType = ClassUtils.forName(elementTypeName, this.beanFactory.getBeanClassLoader()); 41 array.resolvedElementType = elementType; 42 &#125; 43 catch (Throwable ex) &#123; 44 throw new BeanCreationException( 45 this.beanDefinition.getResourceDescription(), this.beanName, 46 &quot;Error resolving array type for &quot; + argName, ex); 47 &#125; 48 &#125; 49 //没有获取到数组的类型，也没有获取到数组元素的类型，则直接设置数 50 //组的类型为Object 51 else &#123; 52 elementType = Object.class; 53 &#125; 54 &#125; 55 //创建指定类型的数组 56 return resolveManagedArray(argName, (List&lt;?&gt;) value, elementType); 57 &#125; 58 //解析list类型的属性值 59 else if (value instanceof ManagedList) &#123; 60 return resolveManagedList(argName, (List&lt;?&gt;) value); 61 &#125; 62 //解析set类型的属性值 63 else if (value instanceof ManagedSet) &#123; 64 return resolveManagedSet(argName, (Set&lt;?&gt;) value); 65 &#125; 66 //解析map类型的属性值 67 else if (value instanceof ManagedMap) &#123; 68 return resolveManagedMap(argName, (Map&lt;?, ?&gt;) value); 69 &#125; 70 //解析props类型的属性值，props其实就是key和value均为字符串的map 71 else if (value instanceof ManagedProperties) &#123; 72 Properties original = (Properties) value; 73 //创建一个拷贝，用于作为解析后的返回值 74 Properties copy = new Properties(); 75 for (Map.Entry propEntry : original.entrySet()) &#123; 76 Object propKey = propEntry.getKey(); 77 Object propValue = propEntry.getValue(); 78 if (propKey instanceof TypedStringValue) &#123; 79 propKey = evaluate((TypedStringValue) propKey); 80 &#125; 81 if (propValue instanceof TypedStringValue) &#123; 82 propValue = evaluate((TypedStringValue) propValue); 83 &#125; 84 copy.put(propKey, propValue); 85 &#125; 86 return copy; 87 &#125; 88 //解析字符串类型的属性值 89 else if (value instanceof TypedStringValue) &#123; 90 TypedStringValue typedStringValue = (TypedStringValue) value; 91 Object valueObject = evaluate(typedStringValue); 92 try &#123; 93 //获取属性的目标类型 94 Class&lt;?&gt; resolvedTargetType = resolveTargetType(typedStringValue); 95 if (resolvedTargetType != null) &#123; 96 //对目标类型的属性进行解析，递归调用 97 return this.typeConverter.convertIfNecessary(valueObject, resolvedTargetType); 98 &#125; 99 //没有获取到属性的目标对象，则按Object类型返回 100 else &#123; 101 return valueObject; 102 &#125; 103 &#125; 104 catch (Throwable ex) &#123; 105 throw new BeanCreationException( 106 this.beanDefinition.getResourceDescription(), this.beanName, 107 &quot;Error converting typed String value for &quot; + argName, ex); 108 &#125; 109 &#125; 110 else &#123; 111 return evaluate(value); 112 &#125; 113 &#125; 114 //解析引用类型的属性值 115 private Object resolveReference(Object argName, RuntimeBeanReference ref) &#123; 116 try &#123; 117 //获取引用的Bean名称 118 String refName = ref.getBeanName(); 119 refName = String.valueOf(evaluate(refName)); 120 //如果引用的对象在父类容器中，则从父类容器中获取指定的引用对象 121 if (ref.isToParent()) &#123; 122 if (this.beanFactory.getParentBeanFactory() == null) &#123; 123 throw new BeanCreationException( 124 this.beanDefinition.getResourceDescription(), this.beanName, 125 &quot;Can&apos;t resolve reference to bean &apos;&quot; + refName + 126 &quot;&apos; in parent factory: no parent factory available&quot;); 127 &#125; 128 return this.beanFactory.getParentBeanFactory().getBean(refName); 129 &#125; 130 //从当前的容器中获取指定的引用Bean对象，如果指定的Bean没有被实例化 131 //则会递归触发引用Bean的初始化和依赖注入 132 else &#123; 133 Object bean = this.beanFactory.getBean(refName); 134 //将当前实例化对象的依赖引用对象 135 this.beanFactory.registerDependentBean(refName, this.beanName); 136 return bean; 137 &#125; 138 &#125; 139 catch (BeansException ex) &#123; 140 throw new BeanCreationException( 141 this.beanDefinition.getResourceDescription(), this.beanName, 142 &quot;Cannot resolve reference to bean &apos;&quot; + ref.getBeanName() + &quot;&apos; while setting &quot; + argName, ex); 143 &#125; 144 &#125; 145 //解析array类型的属性 146 private Object resolveManagedArray(Object argName, List&lt;?&gt; ml, Class elementType) &#123; 147 //创建一个指定类型的数组，用于存放和返回解析后的数组 148 Object resolved = Array.newInstance(elementType, ml.size()); 149 for (int i = 0; i &lt; ml.size(); i++) &#123; 150 //递归解析array的每一个元素，并将解析后的值设置到resolved数组中，索引为i 151 Array.set(resolved, i, 152 resolveValueIfNecessary(new KeyedArgName(argName, i), ml.get(i))); 153 &#125; 154 return resolved; 155 &#125; 156 //解析list类型的属性 157 private List resolveManagedList(Object argName, List&lt;?&gt; ml) &#123; 158 List&lt;Object&gt; resolved = new ArrayList&lt;Object&gt;(ml.size()); 159 for (int i = 0; i &lt; ml.size(); i++) &#123; 160 //递归解析list的每一个元素 161 resolved.add( 162 resolveValueIfNecessary(new KeyedArgName(argName, i), ml.get(i))); 163 &#125; 164 return resolved; 165 &#125; 166 //解析set类型的属性 167 private Set resolveManagedSet(Object argName, Set&lt;?&gt; ms) &#123; 168 Set&lt;Object&gt; resolved = new LinkedHashSet&lt;Object&gt;(ms.size()); 169 int i = 0; 170 //递归解析set的每一个元素 171 for (Object m : ms) &#123; 172 resolved.add(resolveValueIfNecessary(new KeyedArgName(argName, i), m)); 173 i++; 174 &#125; 175 return resolved; 176 &#125; 177 //解析map类型的属性 178 private Map resolveManagedMap(Object argName, Map&lt;?, ?&gt; mm) &#123; 179 Map&lt;Object, Object&gt; resolved = new LinkedHashMap&lt;Object, Object&gt;(mm.size()); 180 //递归解析map中每一个元素的key和value 181 for (Map.Entry entry : mm.entrySet()) &#123; 182 Object resolvedKey = resolveValueIfNecessary(argName, entry.getKey()); 183 Object resolvedValue = resolveValueIfNecessary( 184 new KeyedArgName(argName, entry.getKey()), entry.getValue()); 185 resolved.put(resolvedKey, resolvedValue); 186 &#125; 187 return resolved; 188 &#125; ;) 通过上面的代码分析，我们明白了Spring是如何将引用类型，内部类以及集合类型等属性进行解析的，属性值解析完成后就可以进行依赖注入了，依赖注入的过程就是Bean对象实例设置到它所依赖的Bean对象属性上去，在第7步中我们已经说过，依赖注入是通过bw.setPropertyValues方法实现的，该方法也使用了委托模式，在BeanWrapper接口中至少定义了方法声明，依赖注入的具体实现交由其实现类BeanWrapperImpl来完成，下面我们就分析依BeanWrapperImpl中赖注入相关的源码。 8、BeanWrapperImpl对Bean属性的依赖注入： BeanWrapperImpl类主要是对容器中完成初始化的Bean实例对象进行属性的依赖注入，即把Bean对象设置到它所依赖的另一个Bean的属性中去，依赖注入的相关源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582591 //实现属性依赖注入功能 2 private void setPropertyValue(PropertyTokenHolder tokens, PropertyValue pv) throws BeansException &#123; 3 //PropertyTokenHolder主要保存属性的名称、路径，以及集合的size等信息 4 String propertyName = tokens.canonicalName; 5 String actualName = tokens.actualName; 6 //keys是用来保存集合类型属性的size 7 if (tokens.keys != null) &#123; 8 //将属性信息拷贝 9 PropertyTokenHolder getterTokens = new PropertyTokenHolder(); 10 getterTokens.canonicalName = tokens.canonicalName; 11 getterTokens.actualName = tokens.actualName; 12 getterTokens.keys = new String[tokens.keys.length - 1]; 13 System.arraycopy(tokens.keys, 0, getterTokens.keys, 0, tokens.keys.length - 1); 14 Object propValue; 15 try &#123; 16 //获取属性值，该方法内部使用JDK的内省( Introspector)机制，调用属性//的getter(readerMethod)方法，获取属性的值 17 propValue = getPropertyValue(getterTokens); 18 &#125; 19 catch (NotReadablePropertyException ex) &#123; 20 throw new NotWritablePropertyException(getRootClass(), this.nestedPath + propertyName, 21 &quot;Cannot access indexed value in property referenced &quot; + 22 &quot;in indexed property path &apos;&quot; + propertyName + &quot;&apos;&quot;, ex); 23 &#125; 24 //获取集合类型属性的长度 25 String key = tokens.keys[tokens.keys.length - 1]; 26 if (propValue == null) &#123; 27 throw new NullValueInNestedPathException(getRootClass(), this.nestedPath + propertyName, 28 &quot;Cannot access indexed value in property referenced &quot; + 29 &quot;in indexed property path &apos;&quot; + propertyName + &quot;&apos;: returned null&quot;); 30 &#125; 31 //注入array类型的属性值 32 else if (propValue.getClass().isArray()) &#123; 33 //获取属性的描述符 34 PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); 35 //获取数组的类型 36 Class requiredType = propValue.getClass().getComponentType(); 37 //获取数组的长度 38 int arrayIndex = Integer.parseInt(key); 39 Object oldValue = null; 40 try &#123; 41 //获取数组以前初始化的值 42 if (isExtractOldValueForEditor()) &#123; 43 oldValue = Array.get(propValue, arrayIndex); 44 &#125; 45 //将属性的值赋值给数组中的元素 46 Object convertedValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), requiredType, 47 new PropertyTypeDescriptor(pd, new MethodParameter(pd.getReadMethod(), -1), requiredType)); 48 Array.set(propValue, arrayIndex, convertedValue); 49 &#125; 50 catch (IndexOutOfBoundsException ex) &#123; 51 throw new InvalidPropertyException(getRootClass(), this.nestedPath + propertyName, 52 &quot;Invalid array index in property path &apos;&quot; + propertyName + &quot;&apos;&quot;, ex); 53 &#125; 54 &#125; 55 //注入list类型的属性值 56 else if (propValue instanceof List) &#123; 57 PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); 58 //获取list集合的类型 59 Class requiredType = GenericCollectionTypeResolver.getCollectionReturnType( 60 pd.getReadMethod(), tokens.keys.length); 61 List list = (List) propValue; 62 //获取list集合的size 63 int index = Integer.parseInt(key); 64 Object oldValue = null; 65 if (isExtractOldValueForEditor() &amp;&amp; index &lt; list.size()) &#123; 66 oldValue = list.get(index); 67 &#125; 68 //获取list解析后的属性值 69 Object convertedValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), requiredType, 70 new PropertyTypeDescriptor(pd, new MethodParameter(pd.getReadMethod(), -1), requiredType)); 71 if (index &lt; list.size()) &#123; 72 //为list属性赋值 73 list.set(index, convertedValue); 74 &#125; 75 //如果list的长度大于属性值的长度，则多余的元素赋值为null 76 else if (index &gt;= list.size()) &#123; 77 for (int i = list.size(); i &lt; index; i++) &#123; 78 try &#123; 79 list.add(null); 80 &#125; 81 catch (NullPointerException ex) &#123; 82 throw new InvalidPropertyException(getRootClass(), this.nestedPath + propertyName, 83 &quot;Cannot set element with index &quot; + index + &quot; in List of size &quot; + 84 list.size() + &quot;, accessed using property path &apos;&quot; + propertyName + 85 &quot;&apos;: List does not support filling up gaps with null elements&quot;); 86 &#125; 87 &#125; 88 list.add(convertedValue); 89 &#125; 90 &#125; 91 //注入map类型的属性值 92 else if (propValue instanceof Map) &#123; 93 PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); 94 //获取map集合key的类型 95 Class mapKeyType = GenericCollectionTypeResolver.getMapKeyReturnType( 96 pd.getReadMethod(), tokens.keys.length); 97 //获取map集合value的类型 98 Class mapValueType = GenericCollectionTypeResolver.getMapValueReturnType( 99 pd.getReadMethod(), tokens.keys.length); 100 Map map = (Map) propValue; 101 //解析map类型属性key值 102 Object convertedMapKey = convertIfNecessary(null, null, key, mapKeyType, 103 new PropertyTypeDescriptor(pd, new MethodParameter(pd.getReadMethod(), -1), mapKeyType)); 104 Object oldValue = null; 105 if (isExtractOldValueForEditor()) &#123; 106 oldValue = map.get(convertedMapKey); 107 &#125; 108 //解析map类型属性value值 109 Object convertedMapValue = convertIfNecessary( 110 propertyName, oldValue, pv.getValue(), mapValueType, 111 new TypeDescriptor(new MethodParameter(pd.getReadMethod(), -1, tokens.keys.length + 1))); 112 //将解析后的key和value值赋值给map集合属性 113 map.put(convertedMapKey, convertedMapValue); 114 &#125; 115 else &#123; 116 throw new InvalidPropertyException(getRootClass(), this.nestedPath + propertyName, 117 &quot;Property referenced in indexed property path &apos;&quot; + propertyName + 118 &quot;&apos; is neither an array nor a List nor a Map; returned value was [&quot; + pv.getValue() + &quot;]&quot;); 119 &#125; 120 &#125; 121 //对非集合类型的属性注入 122 else &#123; 123 PropertyDescriptor pd = pv.resolvedDescriptor; 124 if (pd == null || !pd.getWriteMethod().getDeclaringClass().isInstance(this.object)) &#123; 125 pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); 126 //无法获取到属性名或者属性没有提供setter(写方法)方法 127 if (pd == null || pd.getWriteMethod() == null) &#123; 128 //如果属性值是可选的，即不是必须的，则忽略该属性值 129 if (pv.isOptional()) &#123; 130 logger.debug(&quot;Ignoring optional value for property &apos;&quot; + actualName + 131 &quot;&apos; - property not found on bean class [&quot; + getRootClass().getName() + &quot;]&quot;); 132 return; 133 &#125; 134 //如果属性值是必须的，则抛出无法给属性赋值，因为每天提供setter方法异常 135 else &#123; 136 PropertyMatches matches = PropertyMatches.forProperty(propertyName, getRootClass()); 137 throw new NotWritablePropertyException( 138 getRootClass(), this.nestedPath + propertyName, 139 matches.buildErrorMessage(), matches.getPossibleMatches()); 140 &#125; 141 &#125; 142 pv.getOriginalPropertyValue().resolvedDescriptor = pd; 143 &#125; 144 Object oldValue = null; 145 try &#123; 146 Object originalValue = pv.getValue(); 147 Object valueToApply = originalValue; 148 if (!Boolean.FALSE.equals(pv.conversionNecessary)) &#123; 149 if (pv.isConverted()) &#123; 150 valueToApply = pv.getConvertedValue(); 151 &#125; 152 else &#123; 153 if (isExtractOldValueForEditor() &amp;&amp; pd.getReadMethod() != null) &#123; 154 //获取属性的getter方法(读方法)，JDK内省机制 155 final Method readMethod = pd.getReadMethod(); 156 //如果属性的getter方法不是public访问控制权限的，即访问控制权限比较严格， 157 //则使用JDK的反射机制强行访问非public的方法(暴力读取属性值) 158 if (!Modifier.isPublic(readMethod.getDeclaringClass().getModifiers()) &amp;&amp; 159 !readMethod.isAccessible()) &#123; 160 if (System.getSecurityManager()!= null) &#123; 161 //匿名内部类，根据权限修改属性的读取控制限制 162 AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; 163 public Object run() &#123; 164 readMethod.setAccessible(true); 165 return null; 166 &#125; 167 &#125;); 168 &#125; 169 else &#123; 170 readMethod.setAccessible(true); 171 &#125; 172 &#125; 173 try &#123; 174 //属性没有提供getter方法时，调用潜在的读取属性值//的方法，获取属性值 175 if (System.getSecurityManager() != null) &#123; 176 oldValue = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Object&gt;() &#123; 177 public Object run() throws Exception &#123; 178 return readMethod.invoke(object); 179 &#125; 180 &#125;, acc); 181 &#125; 182 else &#123; 183 oldValue = readMethod.invoke(object); 184 &#125; 185 &#125; 186 catch (Exception ex) &#123; 187 if (ex instanceof PrivilegedActionException) &#123; 188 ex = ((PrivilegedActionException) ex).getException(); 189 &#125; 190 if (logger.isDebugEnabled()) &#123; 191 logger.debug(&quot;Could not read previous value of property &apos;&quot; + 192 this.nestedPath + propertyName + &quot;&apos;&quot;, ex); 193 &#125; 194 &#125; 195 &#125; 196 //设置属性的注入值 197 valueToApply = convertForProperty(propertyName, oldValue, originalValue, pd); 198 &#125; 199 pv.getOriginalPropertyValue().conversionNecessary = (valueToApply != originalValue); 200 &#125; 201 //根据JDK的内省机制，获取属性的setter(写方法)方法 202 final Method writeMethod = (pd instanceof GenericTypeAwarePropertyDescriptor ? 203 ((GenericTypeAwarePropertyDescriptor) pd).getWriteMethodForActualAccess() : 204 pd.getWriteMethod()); 205 //如果属性的setter方法是非public，即访问控制权限比较严格，则使用JDK的反射机制， 206 //强行设置setter方法可访问(暴力为属性赋值) 207 if (!Modifier.isPublic(writeMethod.getDeclaringClass().getModifiers()) &amp;&amp; !writeMethod.isAccessible()) &#123; 208 //如果使用了JDK的安全机制，则需要权限验证 209 if (System.getSecurityManager()!= null) &#123; 210 AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; 211 public Object run() &#123; 212 writeMethod.setAccessible(true); 213 return null; 214 &#125; 215 &#125;); 216 &#125; 217 else &#123; 218 writeMethod.setAccessible(true); 219 &#125; 220 &#125; 221 final Object value = valueToApply; 222 if (System.getSecurityManager() != null) &#123; 223 try &#123; 224 //将属性值设置到属性上去 225 AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Object&gt;() &#123; 226 public Object run() throws Exception &#123; 227 writeMethod.invoke(object, value); 228 return null; 229 &#125; 230 &#125;, acc); 231 &#125; 232 catch (PrivilegedActionException ex) &#123; 233 throw ex.getException(); 234 &#125; 235 &#125; 236 else &#123; 237 writeMethod.invoke(this.object, value); 238 &#125; 239 &#125; 240 catch (TypeMismatchException ex) &#123; 241 throw ex; 242 &#125; 243 catch (InvocationTargetException ex) &#123; 244 PropertyChangeEvent propertyChangeEvent = 245 new PropertyChangeEvent(this.rootObject, this.nestedPath + propertyName, oldValue, pv.getValue()); 246 if (ex.getTargetException() instanceof ClassCastException) &#123; 247 throw new TypeMismatchException(propertyChangeEvent, pd.getPropertyType(), ex.getTargetException()); 248 &#125; 249 else &#123; 250 throw new MethodInvocationException(propertyChangeEvent, ex.getTargetException()); 251 &#125; 252 &#125; 253 catch (Exception ex) &#123; 254 PropertyChangeEvent pce = 255 new PropertyChangeEvent(this.rootObject, this.nestedPath + propertyName, oldValue, pv.getValue()); 256 throw new MethodInvocationException(pce, ex); 257 &#125; 258 &#125; &#125; ;) 通过对上面注入依赖代码的分析，我们已经明白了Spring IoC容器是如何将属性的值注入到Bean实例对象中去的： (1).对于集合类型的属性，将其属性值解析为目标类型的集合后直接赋值给属性。 (2).对于非集合类型的属性，大量使用了JDK的反射和内省机制，通过属性的getter方法(reader method)获取指定属性注入以前的值，同时调用属性的setter方法(writer method)为属性设置注入后的值。看到这里相信很多人都明白了Spring的setter注入原理。 至此Spring IoC容器对Bean定义资源文件的定位，载入、解析和依赖注入已经全部分析完毕，现在Spring IoC容器中管理了一系列靠依赖关系联系起来的Bean，程序不需要应用自己手动创建所需的对象，Spring IoC容器会在我们使用的时候自动为我们创建，并且为我们注入好相关的依赖，这就是Spring核心功能的控制反转和依赖注入的相关功能。 五、IoC容器的高级特性1、介绍 ​ 通过前面4篇文章对Spring IoC容器的源码分析，我们已经基本上了解了Spring IoC容器对Bean定义资源的定位、读入和解析过程，同时也清楚了当用户通过getBean方法向IoC容器获取被管理的Bean时，IoC容器对Bean进行的初始化和依赖注入过程，这些是Spring IoC容器的基本功能特性。Spring IoC容器还有一些高级特性，如使用lazy-init属性对Bean预初始化、FactoryBean产生或者修饰Bean对象的生成、IoC容器初始化Bean过程中使用BeanPostProcessor后置处理器对Bean声明周期事件管理和IoC容器的autowiring自动装配功能等。 2、Spring IoC容器的lazy-init属性实现预实例化： ​ 通过前面我们对IoC容器的实现和工作原理分析，我们知道IoC容器的初始化过程就是对Bean定义资源的定位、载入和注册，此时容器对Bean的依赖注入并没有发生，依赖注入主要是在应用程序第一次向容器索取Bean时，通过getBean方法的调用完成。 当Bean定义资源的元素中配置了lazy-init属性时，容器将会在初始化的时候对所配置的Bean进行预实例化，Bean的依赖注入在容器初始化的时候就已经完成。这样，当应用程序第一次向容器索取被管理的Bean时，就不用再初始化和对Bean进行依赖注入了，直接从容器中获取已经完成依赖注入的现成Bean，可以提高应用第一次向容器获取Bean的性能。 下面我们通过代码分析容器预实例化的实现过程： (1).refresh() 先从IoC容器的初始会过程开始，通过前面文章分析，我们知道IoC容器读入已经定位的Bean定义资源是从refresh方法开始的，我们首先从AbstractApplicationContext类的refresh方法入手分析，源码如下： ;) 123456789101112131415161718192021222324252627282930313233343536373839401 //容器初始化的过程，读入Bean定义资源，并解析注册 2 public void refresh() throws BeansException, IllegalStateException &#123; 3 synchronized (this.startupShutdownMonitor) &#123; 4 //调用容器准备刷新的方法，获取容器的当时时间，同时给容器设置同步标识 5 prepareRefresh(); 6 //告诉子类启动refreshBeanFactory()方法，Bean定义资源文件的载入从 7 //子类的refreshBeanFactory()方法启动 8 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); 9 //为BeanFactory配置容器特性，例如类加载器、事件处理器等 10 prepareBeanFactory(beanFactory); 11 try &#123; 12 //为容器的某些子类指定特殊的BeanPost事件处理器 13 postProcessBeanFactory(beanFactory); 14 //调用所有注册的BeanFactoryPostProcessor的Bean 15 invokeBeanFactoryPostProcessors(beanFactory); 16 //为BeanFactory注册BeanPost事件处理器. 17 //BeanPostProcessor是Bean后置处理器，用于监听容器触发的事件 18 registerBeanPostProcessors(beanFactory); 19 //初始化信息源，和国际化相关. 20 initMessageSource(); 21 //初始化容器事件传播器. 22 initApplicationEventMulticaster(); 23 //调用子类的某些特殊Bean初始化方法 24 onRefresh(); 25 //为事件传播器注册事件监听器. 26 registerListeners(); 27 //这里是对容器lazy-init属性进行处理的入口方法 28 finishBeanFactoryInitialization(beanFactory); 29 //初始化容器的生命周期事件处理器，并发布容器的生命周期事件 30 finishRefresh(); 31 &#125; 32 catch (BeansException ex) &#123; 33 //销毁以创建的单态Bean 34 destroyBeans(); 35 //取消refresh操作，重置容器的同步标识. 36 cancelRefresh(ex); 37 throw ex; 38 &#125; 39 &#125; &#125; ;) 在refresh方法中ConfigurableListableBeanFactorybeanFactory = obtainFreshBeanFactory();启动了Bean定义资源的载入、注册过程，而finishBeanFactoryInitialization方法是对注册后的Bean定义中的预实例化(lazy-init=false，Spring默认就是预实例化，即为true)的Bean进行处理的地方。 (2).finishBeanFactoryInitialization处理预实例化Bean： 当Bean定义资源被载入IoC容器之后，容器将Bean定义资源解析为容器内部的数据结构BeanDefinition注册到容器中，AbstractApplicationContext类中的finishBeanFactoryInitialization方法对配置了预实例化属性的Bean进行预初始化过程，源码如下： ;) 123456789101112131415161 //对配置了lazy-init属性的Bean进行预实例化处理 2 protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; 3 //这是Spring3以后新加的代码，为容器指定一个转换服务(ConversionService) 4 //在对某些Bean属性进行转换时使用 5 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; 6 beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; 7 beanFactory.setConversionService( 8 beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); 9 &#125; 10 //为了类型匹配，停止使用临时的类加载器 11 beanFactory.setTempClassLoader(null); 12 //缓存容器中所有注册的BeanDefinition元数据，以防被修改 13 beanFactory.freezeConfiguration(); 14 //对配置了lazy-init属性的单态模式Bean进行预实例化处理 15 beanFactory.preInstantiateSingletons(); &#125; ;) ConfigurableListableBeanFactory是一个接口，其preInstantiateSingletons方法由其子类DefaultListableBeanFactory提供。 (3)、DefaultListableBeanFactory对配置lazy-init属性单态Bean的预实例化： ;) 12345678910111213141516171819202122232425262728293031323334353637383940414243441//对配置lazy-init属性单态Bean的预实例化 2public void preInstantiateSingletons() throws BeansException &#123; 3 if (this.logger.isInfoEnabled()) &#123; 4 this.logger.info(&quot;Pre-instantiating singletons in &quot; + this); 5 &#125; 6 //在对配置lazy-init属性单态Bean的预实例化过程中，必须多线程同步，以确保数据一致性 7 synchronized (this.beanDefinitionMap) &#123; 8 for (String beanName : this.beanDefinitionNames) &#123; 9 //获取指定名称的Bean定义 10 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); 11 //Bean不是抽象的，是单态模式的，且lazy-init属性配置为false 12 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; 13 //如果指定名称的bean是创建容器的Bean 14 if (isFactoryBean(beanName)) &#123; 15 //FACTORY_BEAN_PREFIX=”&amp;”，当Bean名称前面加”&amp;”符号 16 //时，获取的是产生容器对象本身，而不是容器产生的Bean. 17 //调用getBean方法，触发容器对Bean实例化和依赖注入过程 18 final FactoryBean factory = (FactoryBean) getBean(FACTORY_BEAN_PREFIX + beanName); 19 //标识是否需要预实例化 20 boolean isEagerInit; 21 if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; 22 //一个匿名内部类 23 isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; 24 public Boolean run() &#123; 25 return ((SmartFactoryBean) factory).isEagerInit(); 26 &#125; 27 &#125;, getAccessControlContext()); 28 &#125; 29 else &#123; 30 isEagerInit = factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean) factory).isEagerInit(); 31 &#125; 32 if (isEagerInit) &#123; 33 //调用getBean方法，触发容器对Bean实例化和依赖注入过程 34 getBean(beanName); 35 &#125; 36 &#125; 37 else &#123; 38 //调用getBean方法，触发容器对Bean实例化和依赖注入过程 39 getBean(beanName); 40 &#125; 41 &#125; 42 &#125; 43 &#125; &#125; ;) 通过对lazy-init处理源码的分析，我们可以看出，如果设置了lazy-init属性，则容器在完成Bean定义的注册之后，会通过getBean方法，触发对指定Bean的初始化和依赖注入过程，这样当应用第一次向容器索取所需的Bean时，容器不再需要对Bean进行初始化和依赖注入，直接从已经完成实例化和依赖注入的Bean中取一个线程的Bean，这样就提高了第一次获取Bean的性能。 3、FactoryBean的实现： ​ 在Spring中，有两个很容易混淆的类：BeanFactory和FactoryBean。BeanFactory：Bean工厂，是一个工厂(Factory)，我们Spring IoC容器的最顶层接口就是这个BeanFactory，它的作用是管理Bean，即实例化、定位、配置应用程序中的对象及建立这些对象间的依赖。 FactoryBean：工厂Bean，是一个Bean，作用是产生其他bean实例。通常情况下，这种bean没有什么特别的要求，仅需要提供一个工厂方法，该方法用来返回其他bean实例。通常情况下，bean无须自己实现工厂模式，Spring容器担任工厂角色；但少数情况下，容器中的bean本身就是工厂，其作用是产生其它bean实例。 当用户使用容器本身时，可以使用转义字符”&amp;”来得到FactoryBean本身，以区别通过FactoryBean产生的实例对象和FactoryBean对象本身。在BeanFactory中通过如下代码定义了该转义字符： StringFACTORY_BEAN_PREFIX = “&amp;”; 如果myJndiObject是一个FactoryBean，则使用&amp;myJndiObject得到的是myJndiObject对象，而不是myJndiObject产生出来的对象。 (1).FactoryBean的源码如下： ;) 12345678910//工厂Bean，用于产生其他对象 public interface FactoryBean&lt;T&gt; &#123; //获取容器管理的对象实例 T getObject() throws Exception; //获取Bean工厂创建的对象的类型 Class&lt;?&gt; getObjectType(); //Bean工厂创建的对象是否是单态模式，如果是单态模式，则整个容器中只有一个实例 //对象，每次请求都返回同一个实例对象 boolean isSingleton(); &#125; ;) (2). AbstractBeanFactory的getBean方法调用FactoryBean： 在前面我们分析Spring Ioc容器实例化Bean并进行依赖注入过程的源码时，提到在getBean方法触发容器实例化Bean的时候会调用AbstractBeanFactory的doGetBean方法来进行实例化的过程，源码如下： ;) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869701 //真正实现向IoC容器获取Bean的功能，也是触发依赖注入功能的地方 2 @SuppressWarnings(&quot;unchecked&quot;) 3 protected &lt;T&gt; T doGetBean( 4 final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) 5 throws BeansException &#123; 6 //根据指定的名称获取被管理Bean的名称，剥离指定名称中对容器的相关依赖 7 //如果指定的是别名，将别名转换为规范的Bean名称 8 final String beanName = transformedBeanName(name); 9 Object bean; 10 //先从缓存中取是否已经有被创建过的单态类型的Bean，对于单态模式的Bean整 11 //个IoC容器中只创建一次，不需要重复创建 12 Object sharedInstance = getSingleton(beanName); 13 //IoC容器创建单态模式Bean实例对象 14 if (sharedInstance != null &amp;&amp; args == null) &#123; 15 if (logger.isDebugEnabled()) &#123; 16 //如果指定名称的Bean在容器中已有单态模式的Bean被创建，直接返回 17 //已经创建的Bean 18 if (isSingletonCurrentlyInCreation(beanName)) &#123; 19 logger.debug(&quot;Returning eagerly cached instance of singleton bean &apos;&quot; + beanName + 20 &quot;&apos; that is not fully initialized yet - a consequence of a circular reference&quot;); 21 &#125; 22 else &#123; 23 logger.debug(&quot;Returning cached instance of singleton bean &apos;&quot; + beanName + &quot;&apos;&quot;); 24 &#125; 25 &#125; 26 //获取给定Bean的实例对象，主要是完成FactoryBean的相关处理 27 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); 28 &#125; 29 …… 30 &#125; 31 //获取给定Bean的实例对象，主要是完成FactoryBean的相关处理 32 protected Object getObjectForBeanInstance( 33 Object beanInstance, String name, String beanName, RootBeanDefinition mbd) &#123; 34 //容器已经得到了Bean实例对象，这个实例对象可能是一个普通的Bean，也可能是 35 //一个工厂Bean，如果是一个工厂Bean，则使用它创建一个Bean实例对象，如果 36 //调用本身就想获得一个容器的引用，则指定返回这个工厂Bean实例对象 37 //如果指定的名称是容器的解引用(dereference，即是对象本身而非内存地址)， 38 //且Bean实例也不是创建Bean实例对象的工厂Bean 39 if (BeanFactoryUtils.isFactoryDereference(name) &amp;&amp; !(beanInstance instanceof FactoryBean)) &#123; 40 throw new BeanIsNotAFactoryException(transformedBeanName(name), beanInstance.getClass()); 41 &#125; 42 //如果Bean实例不是工厂Bean，或者指定名称是容器的解引用，调用者向获取对 43 //容器的引用，则直接返回当前的Bean实例 44 if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) &#123; 45 return beanInstance; 46 &#125; 47 //处理指定名称不是容器的解引用，或者根据名称获取的Bean实例对象是一个工厂Bean 48 //使用工厂Bean创建一个Bean的实例对象 49 Object object = null; 50 if (mbd == null) &#123; 51 //从Bean工厂缓存中获取给定名称的Bean实例对象 52 object = getCachedObjectForFactoryBean(beanName); 53 &#125; 54 //让Bean工厂生产给定名称的Bean对象实例 55 if (object == null) &#123; 56 FactoryBean factory = (FactoryBean) beanInstance; 57 //如果从Bean工厂生产的Bean是单态模式的，则缓存 58 if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; 59 //从容器中获取指定名称的Bean定义，如果继承基类，则合并基类相关属性 60 mbd = getMergedLocalBeanDefinition(beanName); 61 &#125; 62 //如果从容器得到Bean定义信息，并且Bean定义信息不是虚构的，则让工厂 63 //Bean生产Bean实例对象 64 boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); 65 //调用FactoryBeanRegistrySupport类的getObjectFromFactoryBean 66 //方法，实现工厂Bean生产Bean对象实例的过程 67 object = getObjectFromFactoryBean(factory, beanName, !synthetic); 68 &#125; 69 return object; &#125; ;) 在上面获取给定Bean的实例对象的getObjectForBeanInstance方法中，会调用FactoryBeanRegistrySupport类的getObjectFromFactoryBean方法，该方法实现了Bean工厂生产Bean实例对象。 Dereference(解引用)：一个在C/C++中应用比较多的术语，在C++中，”*”是解引用符号，而”&amp;”是引用符号，解引用是指变量指向的是所引用对象的本身数据，而不是引用对象的内存地址。 (3)、AbstractBeanFactory生产Bean实例对象： AbstractBeanFactory类中生产Bean实例对象的主要源码如下： ;) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717271 //Bean工厂生产Bean实例对象 72 protected Object getObjectFromFactoryBean(FactoryBean factory, String beanName, boolean shouldPostProcess) &#123; 73 //Bean工厂是单态模式，并且Bean工厂缓存中存在指定名称的Bean实例对象 74 if (factory.isSingleton() &amp;&amp; containsSingleton(beanName)) &#123; 75 //多线程同步，以防止数据不一致 76 synchronized (getSingletonMutex()) &#123; 77 //直接从Bean工厂缓存中获取指定名称的Bean实例对象 78 Object object = this.factoryBeanObjectCache.get(beanName); 79 //Bean工厂缓存中没有指定名称的实例对象，则生产该实例对象 80 if (object == null) &#123; 81 //调用Bean工厂的getObject方法生产指定Bean的实例对象 82 object = doGetObjectFromFactoryBean(factory, beanName, shouldPostProcess); 83 //将生产的实例对象添加到Bean工厂缓存中 84 this.factoryBeanObjectCache.put(beanName, (object != null ? object : NULL_OBJECT)); 85 &#125; 86 return (object != NULL_OBJECT ? object : null); 87 &#125; 88 &#125; 89 //调用Bean工厂的getObject方法生产指定Bean的实例对象 90 else &#123; 91 return doGetObjectFromFactoryBean(factory, beanName, shouldPostProcess); 92 &#125; 93 &#125; 94 //调用Bean工厂的getObject方法生产指定Bean的实例对象 95 private Object doGetObjectFromFactoryBean( 96 final FactoryBean factory, final String beanName, final boolean shouldPostProcess) 97 throws BeanCreationException &#123; 98 Object object; 99 try &#123; 100 if (System.getSecurityManager() != null) &#123; 101 AccessControlContext acc = getAccessControlContext(); 102 try &#123; 103 //实现PrivilegedExceptionAction接口的匿名内置类 104 //根据JVM检查权限，然后决定BeanFactory创建实例对象 105 object = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Object&gt;() &#123; 106 public Object run() throws Exception &#123; 107 //调用BeanFactory接口实现类的创建对象方法 108 return factory.getObject(); 109 &#125; 110 &#125;, acc); 111 &#125; 112 catch (PrivilegedActionException pae) &#123; 113 throw pae.getException(); 114 &#125; 115 &#125; 116 else &#123; 117 //调用BeanFactory接口实现类的创建对象方法 118 object = factory.getObject(); 119 &#125; 120 &#125; 121 catch (FactoryBeanNotInitializedException ex) &#123; 122 throw new BeanCurrentlyInCreationException(beanName, ex.toString()); 123 &#125; 124 catch (Throwable ex) &#123; 125 throw new BeanCreationException(beanName, &quot;FactoryBean threw exception on object creation&quot;, ex); 126 &#125; 127 //创建出来的实例对象为null，或者因为单态对象正在创建而返回null 128 if (object == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; 129 throw new BeanCurrentlyInCreationException( 130 beanName, &quot;FactoryBean which is currently in creation returned null from getObject&quot;); 131 &#125; 132 //为创建出来的Bean实例对象添加BeanPostProcessor后置处理器 133 if (object != null &amp;&amp; shouldPostProcess) &#123; 134 try &#123; 135 object = postProcessObjectFromFactoryBean(object, beanName); 136 &#125; 137 catch (Throwable ex) &#123; 138 throw new BeanCreationException(beanName, &quot;Post-processing of the FactoryBean&apos;s object failed&quot;, ex); 139 &#125; 140 &#125; 141 return object; &#125; ;) 从上面的源码分析中，我们可以看出，BeanFactory接口调用其实现类的getObject方法来实现创建Bean实例对象的功能。 (4).工厂Bean的实现类getObject方法创建Bean实例对象： FactoryBean的实现类有非常多，比如：Proxy、RMI、JNDI、ServletContextFactoryBean等等，FactoryBean接口为Spring容器提供了一个很好的封装机制，具体的getObject有不同的实现类根据不同的实现策略来具体提供，我们分析一个最简单的AnnotationTestFactoryBean的实现源码： ;) 12345678910111213141516143 public class AnnotationTestBeanFactory implements FactoryBean&lt;IJmxTestBean&gt; &#123; 144 private final FactoryCreatedAnnotationTestBean instance = new FactoryCreatedAnnotationTestBean(); 145 public AnnotationTestBeanFactory() &#123; 146 this.instance.setName(&quot;FACTORY&quot;); 147 &#125; 148 //AnnotationTestBeanFactory产生Bean实例对象的实现 149 public IJmxTestBean getObject() throws Exception &#123; 150 return this.instance; 151 &#125; 152 public Class&lt;? extends IJmxTestBean&gt; getObjectType() &#123; 153 return FactoryCreatedAnnotationTestBean.class; 154 &#125; 155 public boolean isSingleton() &#123; 156 return true; 157 &#125; &#125; ;) 其他的Proxy，RMI，JNDI等等，都是根据相应的策略提供getObject的实现。这里不做一一分析，这已经不是Spring的核心功能，有需要的时候再去深入研究。 4.BeanPostProcessor后置处理器的实现： BeanPostProcessor后置处理器是Spring IoC容器经常使用到的一个特性，这个Bean后置处理器是一个监听器，可以监听容器触发的Bean声明周期事件。后置处理器向容器注册以后，容器中管理的Bean就具备了接收IoC容器事件回调的能力。 BeanPostProcessor的使用非常简单，只需要提供一个实现接口BeanPostProcessor的实现类，然后在Bean的配置文件中设置即可。 (1).BeanPostProcessor的源码如下： ;) 123456781 package org.springframework.beans.factory.config; 2 import org.springframework.beans.BeansException; 3 public interface BeanPostProcessor &#123; 4 //为在Bean的初始化前提供回调入口 5 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; 6 //为在Bean的初始化之后提供回调入口 7 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; &#125; ;) 这两个回调的入口都是和容器管理的Bean的生命周期事件紧密相关，可以为用户提供在Spring IoC容器初始化Bean过程中自定义的处理操作。 (2).AbstractAutowireCapableBeanFactory类对容器生成的Bean添加后置处理器： BeanPostProcessor后置处理器的调用发生在Spring IoC容器完成对Bean实例对象的创建和属性的依赖注入完成之后，在对Spring依赖注入的源码分析过程中我们知道，当应用程序第一次调用getBean方法(lazy-init预实例化除外)向Spring IoC容器索取指定Bean时触发Spring IoC容器创建Bean实例对象并进行依赖注入的过程，其中真正实现创建Bean对象并进行依赖注入的方法是AbstractAutowireCapableBeanFactory类的doCreateBean方法，主要源码如下： ;) 1234567891011121314151617181920211 //真正创建Bean的方法 2 protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) &#123; 3 //创建Bean实例对象 4 …… 5 try &#123; 6 //对Bean属性进行依赖注入 7 populateBean(beanName, mbd, instanceWrapper); 8 if (exposedObject != null) &#123; 9 //在对Bean实例对象生成和依赖注入完成以后，开始对Bean实例对象 10 //进行初始化 ，为Bean实例对象应用BeanPostProcessor后置处理器 11 exposedObject = initializeBean(beanName, exposedObject, mbd); 12 &#125; 13 &#125; 14 catch (Throwable ex) &#123; 15 if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; 16 throw (BeanCreationException) ex; 17 &#125; 18 …… 19 //为应用返回所需要的实例对象 20 return exposedObject; &#125; ;) 从上面的代码中我们知道，为Bean实例对象添加BeanPostProcessor后置处理器的入口的是initializeBean方法。 (3).initializeBean方法为容器产生的Bean实例对象添加BeanPostProcessor后置处理器： 同样在AbstractAutowireCapableBeanFactory类中，initializeBean方法实现为容器创建的Bean实例对象添加BeanPostProcessor后置处理器，源码如下： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768691 //初始容器创建的Bean实例对象，为其添加BeanPostProcessor后置处理器 2 protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; 3 //JDK的安全机制验证权限 4 if (System.getSecurityManager() != null) &#123; 5 //实现PrivilegedAction接口的匿名内部类 6 AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; 7 public Object run() &#123; 8 invokeAwareMethods(beanName, bean); 9 return null; 10 &#125; 11 &#125;, getAccessControlContext()); 12 &#125; 13 else &#123; 14 //为Bean实例对象包装相关属性，如名称，类加载器，所属容器等信息 15 invokeAwareMethods(beanName, bean); 16 &#125; 17 Object wrappedBean = bean; 18 //对BeanPostProcessor后置处理器的postProcessBeforeInitialization 19 //回调方法的调用，为Bean实例初始化前做一些处理 20 if (mbd == null || !mbd.isSynthetic()) &#123; 21 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); 22 &#125; 23 //调用Bean实例对象初始化的方法，这个初始化方法是在Spring Bean定义配置 24 //文件中通过init-method属性指定的 25 try &#123; 26 invokeInitMethods(beanName, wrappedBean, mbd); 27 &#125; 28 catch (Throwable ex) &#123; 29 throw new BeanCreationException( 30 (mbd != null ? mbd.getResourceDescription() : null), 31 beanName, &quot;Invocation of init method failed&quot;, ex); 32 &#125; 33 //对BeanPostProcessor后置处理器的postProcessAfterInitialization 34 //回调方法的调用，为Bean实例初始化之后做一些处理 35 if (mbd == null || !mbd.isSynthetic()) &#123; 36 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); 37 &#125; 38 return wrappedBean; 39 &#125; 40 //调用BeanPostProcessor后置处理器实例对象初始化之前的处理方法 41 public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) 42 throws BeansException &#123; 43 Object result = existingBean; 44 //遍历容器为所创建的Bean添加的所有BeanPostProcessor后置处理器 45 for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) &#123; 46 //调用Bean实例所有的后置处理中的初始化前处理方法，为Bean实例对象在 47 //初始化之前做一些自定义的处理操作 48 result = beanProcessor.postProcessBeforeInitialization(result, beanName); 49 if (result == null) &#123; 50 return result; 51 &#125; 52 &#125; 53 return result; 54 &#125; 55 //调用BeanPostProcessor后置处理器实例对象初始化之后的处理方法 56 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) 57 throws BeansException &#123; 58 Object result = existingBean; 59 //遍历容器为所创建的Bean添加的所有BeanPostProcessor后置处理器 60 for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) &#123; 61 //调用Bean实例所有的后置处理中的初始化后处理方法，为Bean实例对象在 62 //初始化之后做一些自定义的处理操作 63 result = beanProcessor.postProcessAfterInitialization(result, beanName); 64 if (result == null) &#123; 65 return result; 66 &#125; 67 &#125; 68 return result; &#125; ;) BeanPostProcessor是一个接口，其初始化前的操作方法和初始化后的操作方法均委托其实现子类来实现，在Spring中，BeanPostProcessor的实现子类非常的多，分别完成不同的操作，如：AOP面向切面编程的注册通知适配器、Bean对象的数据校验、Bean继承属性/方法的合并等等，我们以最简单的AOP切面织入来简单了解其主要的功能。 (4).AdvisorAdapterRegistrationManager在Bean对象初始化后注册通知适配器： AdvisorAdapterRegistrationManager是BeanPostProcessor的一个实现类，其主要的作用为容器中管理的Bean注册一个面向切面编程的通知适配器，以便在Spring容器为所管理的Bean进行面向切面编程时提供方便，其源码如下： ;) 1234567891011121314151617181920211 //为容器中管理的Bean注册一个面向切面编程的通知适配器 2 public class AdvisorAdapterRegistrationManager implements BeanPostProcessor &#123; 3 //容器中负责管理切面通知适配器注册的对象 4 private AdvisorAdapterRegistry advisorAdapterRegistry = GlobalAdvisorAdapterRegistry.getInstance(); 5 public void setAdvisorAdapterRegistry(AdvisorAdapterRegistry advisorAdapterRegistry) &#123; 6 this.advisorAdapterRegistry = advisorAdapterRegistry; 7 &#125; 8 //BeanPostProcessor在Bean对象初始化前的操作 9 public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; 10 //没有做任何操作，直接返回容器创建的Bean对象 11 return bean; 12 &#125; 13 //BeanPostProcessor在Bean对象初始化后的操作 14 public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; 15 if (bean instanceof AdvisorAdapter)&#123; 16 //如果容器创建的Bean实例对象是一个切面通知适配器，则向容器的注册 this.advisorAdapterRegistry.registerAdvisorAdapter((AdvisorAdapter) bean); 17 &#125; 18 return bean; 19 &#125; &#125; ;) 其他的BeanPostProcessor接口实现类的也类似，都是对Bean对象使用到的一些特性进行处理，或者向IoC容器中注册，为创建的Bean实例对象做一些自定义的功能增加，这些操作是容器初始化Bean时自动触发的，不需要认为的干预。 5.Spring IoC容器autowiring实现原理： Spring IoC容器提供了两种管理Bean依赖关系的方式： a. 显式管理：通过BeanDefinition的属性值和构造方法实现Bean依赖关系管理。 b． autowiring：Spring IoC容器的依赖自动装配功能，不需要对Bean属性的依赖关系做显式的声明，只需要在配置好autowiring属性，IoC容器会自动使用反射查找属性的类型和名称，然后基于属性的类型或者名称来自动匹配容器中管理的Bean，从而自动地完成依赖注入。 通过对autowiring自动装配特性的理解，我们知道容器对Bean的自动装配发生在容器对Bean依赖注入的过程中。在前面对Spring IoC容器的依赖注入过程源码分析中，我们已经知道了容器对Bean实例对象的属性注入的处理发生在AbstractAutoWireCapableBeanFactory类中的populateBean方法中，我们通过程序流程分析autowiring的实现原理： (1). AbstractAutoWireCapableBeanFactory对Bean实例进行属性依赖注入： 应用第一次通过getBean方法(配置了lazy-init预实例化属性的除外)向IoC容器索取Bean时，容器创建Bean实例对象，并且对Bean实例对象进行属性依赖注入，AbstractAutoWireCapableBeanFactory的populateBean方法就是实现Bean属性依赖注入的功能，其主要源码如下： ;) 12345678910111213141516171819201 protected void populateBean(String beanName, AbstractBeanDefinition mbd, BeanWrapper bw) &#123; 2 //获取Bean定义的属性值，并对属性值进行处理 3 PropertyValues pvs = mbd.getPropertyValues(); 4 …… 5 //对依赖注入处理，首先处理autowiring自动装配的依赖注入 6 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || 7 mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; 8 MutablePropertyValues newPvs = new MutablePropertyValues(pvs); 9 //根据Bean名称进行autowiring自动装配处理 10 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; 11 autowireByName(beanName, mbd, bw, newPvs); 12 &#125; 13 //根据Bean类型进行autowiring自动装配处理 14 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; 15 autowireByType(beanName, mbd, bw, newPvs); 16 &#125; 17 &#125; 18 //对非autowiring的属性进行依赖注入处理 19 …… &#125; ;) (2).Spring IoC容器根据Bean名称或者类型进行autowiring自动依赖注入： ;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374751 //根据名称对属性进行自动依赖注入 2 protected void autowireByName( 3 String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; 4 //对Bean对象中非简单属性(不是简单继承的对象，如8中原始类型，字符串，URL等//都是简单属性)进行处理 5 String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); 6 for (String propertyName : propertyNames) &#123; 7 //如果Spring IoC容器中包含指定名称的Bean 8 if (containsBean(propertyName)) &#123; 9 //调用getBean方法向IoC容器索取指定名称的Bean实例，迭代触发属性的//初始化和依赖注入 10 Object bean = getBean(propertyName); 11 //为指定名称的属性赋予属性值 12 pvs.add(propertyName, bean); 13 //指定名称属性注册依赖Bean名称，进行属性依赖注入 14 registerDependentBean(propertyName, beanName); 15 if (logger.isDebugEnabled()) &#123; 16 logger.debug(&quot;Added autowiring by name from bean name &apos;&quot; + beanName + 17 &quot;&apos; via property &apos;&quot; + propertyName + &quot;&apos; to bean named &apos;&quot; + propertyName + &quot;&apos;&quot;); 18 &#125; 19 &#125; 20 else &#123; 21 if (logger.isTraceEnabled()) &#123; 22 logger.trace(&quot;Not autowiring property &apos;&quot; + propertyName + &quot;&apos; of bean &apos;&quot; + beanName + 23 &quot;&apos; by name: no matching bean found&quot;); 24 &#125; 25 &#125; 26 &#125; 27 &#125; 28 //根据类型对属性进行自动依赖注入 29 protected void autowireByType( 30 String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; 31 //获取用户定义的类型转换器 32 TypeConverter converter = getCustomTypeConverter(); 33 if (converter == null) &#123; 34 converter = bw; 35 &#125; 36 //存放解析的要注入的属性 37 Set&lt;String&gt; autowiredBeanNames = new LinkedHashSet&lt;String&gt;(4); 38 //对Bean对象中非简单属性(不是简单继承的对象，如8中原始类型，字符 39 //URL等都是简单属性)进行处理 40 String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); 41 for (String propertyName : propertyNames) &#123; 42 try &#123; 43 //获取指定属性名称的属性描述器 44 PropertyDescriptor pd = bw.getPropertyDescriptor(propertyName); 45 //不对Object类型的属性进行autowiring自动依赖注入 46 if (!Object.class.equals(pd.getPropertyType())) &#123; 47 //获取属性的setter方法 48 MethodParameter methodParam = BeanUtils.getWriteMethodParameter(pd); 49 //检查指定类型是否可以被转换为目标对象的类型 50 boolean eager = !PriorityOrdered.class.isAssignableFrom(bw.getWrappedClass()); 51 //创建一个要被注入的依赖描述 52 DependencyDescriptor desc = new AutowireByTypeDependencyDescriptor(methodParam, eager); 53 //根据容器的Bean定义解析依赖关系，返回所有要被注入的Bean对象 54 Object autowiredArgument = resolveDependency(desc, beanName, autowiredBeanNames, converter); 55 if (autowiredArgument != null) &#123; 56 //为属性赋值所引用的对象 57 pvs.add(propertyName, autowiredArgument); 58 &#125; 59 for (String autowiredBeanName : autowiredBeanNames) &#123; 60 //指定名称属性注册依赖Bean名称，进行属性依赖注入 61 registerDependentBean(autowiredBeanName, beanName); 62 if (logger.isDebugEnabled()) &#123; 63 logger.debug(&quot;Autowiring by type from bean name &apos;&quot; + beanName + &quot;&apos; via property &apos;&quot; + 64 propertyName + &quot;&apos; to bean named &apos;&quot; + autowiredBeanName + &quot;&apos;&quot;); 65 &#125; 66 &#125; 67 //释放已自动注入的属性 68 autowiredBeanNames.clear(); 69 &#125; 70 &#125; 71 catch (BeansException ex) &#123; 72 throw new UnsatisfiedDependencyException(mbd.getResourceDescription(), beanName, propertyName, ex); 73 &#125; 74 &#125; &#125; ;) 通过上面的源码分析，我们可以看出来通过属性名进行自动依赖注入的相对比通过属性类型进行自动依赖注入要稍微简单一些，但是真正实现属性注入的是DefaultSingletonBeanRegistry类的registerDependentBean方法。 (3).DefaultSingletonBeanRegistry的registerDependentBean方法对属性注入： ;) 123456789101112131415161718192021222324252627282930311 //为指定的Bean注入依赖的Bean 2 public void registerDependentBean(String beanName, String dependentBeanName) &#123; 3 //处理Bean名称，将别名转换为规范的Bean名称 4 String canonicalName = canonicalName(beanName); 5 //多线程同步，保证容器内数据的一致性 6 //先从容器中：bean名称--&gt;全部依赖Bean名称集合找查找给定名称Bean的依赖Bean 7 synchronized (this.dependentBeanMap) &#123; 8 //获取给定名称Bean的所有依赖Bean名称 9 Set&lt;String&gt; dependentBeans = this.dependentBeanMap.get(canonicalName); 10 if (dependentBeans == null) &#123; 11 //为Bean设置依赖Bean信息 12 dependentBeans = new LinkedHashSet&lt;String&gt;(8); 13 this.dependentBeanMap.put(canonicalName, dependentBeans); 14 &#125; 15 //向容器中：bean名称--&gt;全部依赖Bean名称集合添加Bean的依赖信息 16 //即，将Bean所依赖的Bean添加到容器的集合中 17 dependentBeans.add(dependentBeanName); 18 &#125; 19 //从容器中：bean名称--&gt;指定名称Bean的依赖Bean集合找查找给定名称 20 //Bean的依赖Bean 21 synchronized (this.dependenciesForBeanMap) &#123; 22 Set&lt;String&gt; dependenciesForBean = this.dependenciesForBeanMap.get(dependentBeanName); 23 if (dependenciesForBean == null) &#123; 24 dependenciesForBean = new LinkedHashSet&lt;String&gt;(8); 25 this.dependenciesForBeanMap.put(dependentBeanName, dependenciesForBean); 26 &#125; 27 //向容器中：bean名称--&gt;指定Bean的依赖Bean名称集合添加Bean的依赖信息 28 //即，将Bean所依赖的Bean添加到容器的集合中 29 dependenciesForBean.add(canonicalName); 30 &#125; &#125; ;) 通过对autowiring的源码分析，我们可以看出，autowiring的实现过程： a. 对Bean的属性迭代调用getBean方法，完成依赖Bean的初始化和依赖注入。 b. 将依赖Bean的属性引用设置到被依赖的Bean属性上。 c. 将依赖Bean的名称和被依赖Bean的名称存储在IoC容器的集合中。 Spring IoC容器的autowiring属性自动依赖注入是一个很方便的特性，可以简化开发时的配置，但是凡是都有两面性，自动属性依赖注入也有不足，首先，Bean的依赖关系在配置文件中无法很清楚地看出来，对于维护造成一定困难。其次，由于自动依赖注入是Spring容器自动执行的，容器是不会智能判断的，如果配置不当，将会带来无法预料的后果，所以自动依赖注入特性在使用时还是综合考虑。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FSpring%20AOP%2F</url>
    <content type="text"><![CDATA[AOP：※探秘Spring AOP（慕课网视频，很不错） 慕课网视频，讲解的很不错，详细且深入 spring源码剖析（六）AOP实现原理剖析 通过源码分析Spring AOP的原理 说说 Spring AOP https://www.cnblogs.com/hongwz/p/5764917.html Spring AOP 实现原理 https://blog.csdn.net/moreevan/article/details/11977115/ Aspect-OrientedProgramming（面向切面编程）是对OOP的补充，目的是将那些与业务无关，却为业务模块所共同调用的逻辑封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可操作性和可维护性。权限控制，错误处理，记录跟踪，事务等功能都会用到。 名词解释切面（Aspect）：其实就是共有功能的实现。如日志切面、权限切面、事务切面等。在实际应用中通常是一个存放共有功能实现的普通Java类，之所以能被AOP容器识别成切面，是在配置中指定的。可以用Spring的 Advisor或拦截器实现。 通知（Advice）：是切面的具体实现。以目标方法为参照点，根据放置的地方不同，可分为前置通知（Before）、后置通知（AfterReturning）、异常通知（AfterThrowing）、最终通知（After）与环绕通知（Around）5种。在实际应用中通常是切面类中的一个方法，具体属于哪类通知，同样是在配置中指定的。 连接点（Joinpoint）：就是程序在运行过程中能够插入切面的地点。例如，方法调用、异常抛出或字段修改等，但Spring只支持方法级的连接点。 切入点（Pointcut）：用于定义通知应该切入到哪些连接点上。不同的通知通常需要切入到不同的连接点上，这种精准的匹配是由切入点的正则表达式来定义的。AOP框架必须允许开发者指定切入点：例如，使用正则表达式。 Spring定义了Pointcut接口，用来组合MethodMatcher和ClassFilter，可以通过名字很清楚的理解， MethodMatcher是用来检查目标类的方法是否可以被应用此通知，而ClassFilter是用来检查Pointcut是否应该应用到目标类上 目标对象（Target）：包含连接点的对象。也被称作被通知或被代理对象。就是那些即将切入切面的对象，也就是那些被通知的对象。这些对象中已经只剩下干干净净的核心业务逻辑代码了，所有的共有功能代码等待AOP容器的切入。 代理对象（Proxy）：将通知应用到目标对象之后被动态创建的对象。可以简单地理解为，代理对象的功能等于目标对象的核心业务逻辑功能加上共有功能。代理对象对于使用者而言是透明的，是程序运行过程中的产物。AOP框架创建的对象，包含通知。 在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）：将切面应用到目标对象从而创建一个新的代理对象的过程。这个过程可以发生在编译期、类装载期及运行期，当然不同的发生点有着不同的前提条件。譬如发生在编译期的话，就要求有一个支持这种AOP实现的特殊编译器（例如使用AspectJ编译器）；发生在类装载期，就要求有一个支持AOP实现的特殊类装载器；只有发生在运行期，则可直接通过Java语言的反射机制与动态代理机制来动态实现。 AOP实现实现AOP的技术，主要分为两大类： 静态AOP实现：AspectJ是一个基于Java语言的AOP框架，提供了强大的AOP功能，在编译阶段对程序进行修改，即实现对目标类的增强，生成静态AOP代理类 动态AOP实现：采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行。SpringAOP中如果目标对象实现了接口，Spring默认情况下会采用JDK的动态代理实现AOP，如果目标对象没有实现接口，必须采用CGLIB库。 AspectJ12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697@Component@Aspectpublic class MyAspect &#123; private static final Logger LOGGER= LoggerFactory.getLogger(MyAspect.class); private static ThreadLocal&lt;Date&gt; threadLocal=new ThreadLocal&lt;&gt;(); @Autowired(required=false) private HttpServletRequest request; @Pointcut("execution(* *.print())") public void exec()&#123;&#125; @Before(value = "exec()") public void beforeLog()&#123; threadLocal.set(new Date()); String requestURI = request.getRequestURI(); String ipAddr = getIpAddr(request); LOGGER.info("type:&#123;&#125;,uri:&#123;&#125;,ipaddr:&#123;&#125;","before",requestURI,ipAddr); &#125; @After(value = "exec()") public void afterLog(JoinPoint joinPoint) throws Exception &#123; String controllerMethodDescription = getControllerMethodDescription(joinPoint); long beginTime = threadLocal.get().getTime(); long endTime=System.currentTimeMillis(); LOGGER.info("type:&#123;&#125;,method:&#123;&#125;,time:&#123;&#125;","after",controllerMethodDescription,endTime-beginTime); &#125; @Around(value = "exec()") public Object aroundTest(ProceedingJoinPoint point)&#123; LOGGER.info("type:&#123;&#125;","around begin"); Object o=null; try &#123; o=point.proceed(); LOGGER.info("type:&#123;&#125;,name:&#123;&#125;","around",(String)o); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; String modified=o+"modified"; LOGGER.info("type:&#123;&#125;,name:&#123;&#125;","around end",modified); return modified; &#125; public String getControllerMethodDescription(JoinPoint joinPoint) throws Exception&#123; //获取目标类名 String targetName = joinPoint.getTarget().getClass().getName(); //获取方法名 String methodName = joinPoint.getSignature().getName(); return targetName+"#"+methodName; &#125; public static String getIpAddr(HttpServletRequest request) &#123; String ip = request.getHeader("x-forwarded-for"); if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) &#123; ip = request.getHeader("Proxy-Client-IP"); &#125; if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) &#123; ip = request.getHeader("WL-Proxy-Client-IP"); &#125; if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) &#123; ip = request.getRemoteAddr(); if (ip.equals("127.0.0.1")) &#123; //根据网卡取本机配置的IP InetAddress inet = null; try &#123; inet = InetAddress.getLocalHost(); &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125; ip = inet.getHostAddress(); &#125; &#125; // 对于通过多个代理的情况，第一个IP为客户端真实IP,多个IP按照','分割 if (ip != null &amp;&amp; ip.length() &gt; 15) &#123; if (ip.indexOf(",") &gt; 0) &#123; ip = ip.substring(0, ip.indexOf(",")); &#125; &#125; return ip; &#125;&#125;@RestControllerpublic class MyController &#123; @GetMapping("/name") public String print()&#123; System.out.println("getName..."); return "baozi"; &#125;&#125; 通知的类型 通知 描述 @Before 在一个方法执行之前，执行通知。 @After 在一个方法执行之后，不考虑其结果，执行通知。 @AfterReturning 在切入点return内容之后切入内容（可以用来对处理返回值做一些加工处理） @AfterThrowing 只有在方法退出抛出异常时，才能执行通知。 @Around 在切入点前后切入内容，并自己控制何时执行切入点自身的内容 Spring AOP可以通过配置文件或者编程的方式来使用Spring AOP。 配置可以通过xml文件来进行，大概有四种方式： 配置ProxyFactoryBean，显式地设置advisors, advice, target等 配置AutoProxyCreator，这种方式下，还是如以前一样使用定义的bean，但是从容器中获得的其实已经是代理对象 通过aop:config来配置 通过&lt;aop: aspectj-autoproxy&gt;来配置，使用AspectJ的注解来标识通知及切入点 https://blog.csdn.net/RobertoHuang/article/details/70148474 也可以直接使用ProxyFactory来以编程的方式使用Spring AOP，通过ProxyFactory提供的方法可以设置target对象, advisor等相关配置，最终通过 getProxy()方法来获取代理对象 Spring AOP代理对象的生成Spring提供了两种方式来生成代理对象: JDKProxy和Cglib，具体使用哪种方式生成由AopProxyFactory根据AdvisedSupport对象的配置来决定。默认的策略是如果目标类是接口，则使用JDK动态代理技术，否则使用Cglib来生成代理。下面我们来研究一下Spring如何使用JDK来生成代理对象，具体的生成代码放在JdkDynamicAopProxy这个类中，直接上相关代码： 123456789public Object getProxy(@Nullable ClassLoader classLoader) &#123; if (logger.isTraceEnabled()) &#123; logger.trace("Creating JDK dynamic proxy: " + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); this.findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 我们知道InvocationHandler是JDK动态代理的核心，生成的代理对象的方法调用都会委托到InvocationHandler.invoke()方法。而通过JdkDynamicAopProxy的签名我们可以看到这个类其实也实现了InvocationHandler，下面我们就通过分析这个类中实现的invoke()方法来具体看下Spring AOP是如何织入切面的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Nullablepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Object target = null; Integer var9; try &#123; if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; Boolean var19 = this.equals(args[0]); return var19; &#125; if (this.hashCodeDefined || !AopUtils.isHashCodeMethod(method)) &#123; if (method.getDeclaringClass() == DecoratingProxy.class) &#123; Class var18 = AopProxyUtils.ultimateTargetClass(this.advised); return var18; &#125; Object retVal; if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; retVal = AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); return retVal; &#125; if (this.advised.exposeProxy) &#123; oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; target = targetSource.getTarget(); Class&lt;?&gt; targetClass = target != null ? target.getClass() : null; List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); retVal = invocation.proceed(); &#125; Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType != Object.class &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; retVal = proxy; &#125; else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException(&quot;Null return value from advice does not match primitive return type for: &quot; + method); &#125; Object var13 = retVal; return var13; &#125; var9 = this.hashCode(); &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; AopContext.setCurrentProxy(oldProxy); &#125; &#125; return var9;&#125; 主流程可以简述为：获取可以应用到此方法上的通知链（Interceptor Chain）,如果有,则应用通知,并执行joinpoint; 如果没有,则直接反射执行joinpoint。而这里的关键是通知链是如何获取的以及它又是如何执行的，下面逐一分析下。 首先，从上面的代码可以看到，通知链是通过Advised.getInterceptorsAndDynamicInterceptionAdvice()这个方法来获取的,我们来看下这个方法的实现: 12345678910public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, @Nullable Class&lt;?&gt; targetClass) &#123; AdvisedSupport.MethodCacheKey cacheKey = new AdvisedSupport.MethodCacheKey(method); List&lt;Object&gt; cached = (List)this.methodCache.get(cacheKey); if (cached == null) &#123; cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice(this, method, targetClass); this.methodCache.put(cacheKey, cached); &#125; return cached;&#125; 可以看到实际的获取工作其实是由AdvisorChainFactory. getInterceptorsAndDynamicInterceptionAdvice()这个方法来完成的，获取到的结果会被缓存。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Advised config, Method method, @Nullable Class&lt;?&gt; targetClass) &#123; AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); Advisor[] advisors = config.getAdvisors(); List&lt;Object&gt; interceptorList = new ArrayList(advisors.length); Class&lt;?&gt; actualClass = targetClass != null ? targetClass : method.getDeclaringClass(); Boolean hasIntroductions = null; Advisor[] var9 = advisors; int var10 = advisors.length; for(int var11 = 0; var11 &lt; var10; ++var11) &#123; Advisor advisor = var9[var11]; if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pointcutAdvisor = (PointcutAdvisor)advisor; if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) &#123; MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); boolean match; if (mm instanceof IntroductionAwareMethodMatcher) &#123; if (hasIntroductions == null) &#123; hasIntroductions = hasMatchingIntroductions(advisors, actualClass); &#125; match = ((IntroductionAwareMethodMatcher)mm).matches(method, actualClass, hasIntroductions); &#125; else &#123; match = mm.matches(method, actualClass); &#125; if (match) &#123; MethodInterceptor[] interceptors = registry.getInterceptors(advisor); if (mm.isRuntime()) &#123; MethodInterceptor[] var17 = interceptors; int var18 = interceptors.length; for(int var19 = 0; var19 &lt; var18; ++var19) &#123; MethodInterceptor interceptor = var17[var19]; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; else if (advisor instanceof IntroductionAdvisor) &#123; IntroductionAdvisor ia = (IntroductionAdvisor)advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; else &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; return interceptorList;&#125; 这个方法执行完成后，Advised中配置能够应用到连接点或者目标类的Advisor全部被转化成了MethodInterceptor. 接下来我们再看下得到的拦截器链是怎么起作用的。 1234567if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); retVal = invocation.proceed(); &#125; ​ ​ 从这段代码可以看出，如果得到的拦截器链为空，则直接反射调用目标方法，否则创建MethodInvocation，调用其proceed方法，触发拦截器链的执行，来看下具体代码 123456789101112131415@Nullable public Object proceed() throws Throwable &#123; if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; return this.invokeJoinpoint(); &#125; else &#123; Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher)interceptorOrInterceptionAdvice; Class&lt;?&gt; targetClass = this.targetClass != null ? this.targetClass : this.method.getDeclaringClass(); return dm.methodMatcher.matches(this.method, targetClass, this.arguments) ? dm.interceptor.invoke(this) : this.proceed(); &#125; else &#123; return ((MethodInterceptor)interceptorOrInterceptionAdvice).invoke(this); &#125; &#125; &#125; 补充静态代理123456789101112131415161718192021222324252627282930313233public interface MyInterface &#123; void print();&#125;public class MyImplement implements MyInterface &#123; @Override public void print() &#123; System.out.println("hello world"); &#125;&#125;public class MyProxy implements MyInterface&#123; private MyImplement myImplement; public MyProxy() &#123; this.myImplement = new MyImplement(); &#125; @Override public void print() &#123; System.out.println("before"); myImplement.print(); System.out.println("after"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyInterface myInterface=new MyProxy(); myInterface.print(); &#125;&#125; 这样的代理方式调用者不知道被代理对象的存在。 动态代理从静态代理中可以看出，静态代理只能代理一个具体的类，如果要代理一个接口的多个实现的话需要定义不同的代理类，所以需要使用动态代理。 JDK实现JDK动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。 Proxy 类是用于创建代理对象，而 InvocationHandler 接口来处理执行逻辑。 Proxy 的newProxyInstance 方法动态创建代理类。第一个参数为类加载器，第二个参数为代理类需要实现的接口列表，最后一个则是处理器。 1234567891011121314151617181920212223242526272829303132333435public class MyProxy implements InvocationHandler &#123; private Object target; public MyProxy(Class clazz) &#123; try &#123; this.target=clazz.newInstance(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("before"); Object result=method.invoke(target,args); System.out.println("after"); return result; &#125; public Object getProxy()&#123; return Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), target.getClass().getInterfaces(),this); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyProxy myProxy = new MyProxy(MyImplement.class); MyInterface myInterface= (MyInterface) myProxy.getProxy(); myInterface.print(); &#125;&#125; CGLIB实现CGLIB是对一个小而快的字节码处理框架 ASM 的封装。把代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。 12345678910111213141516171819202122232425262728293031323334353637public class MyProxy implements MethodInterceptor &#123; private Object target; public MyProxy(Class clazz) &#123; try &#123; this.target = clazz.newInstance(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; public Object getProxy()&#123; Enhancer enhancer=new Enhancer(); enhancer.setSuperclass(target.getClass()); enhancer.setCallback(this); return enhancer.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("before"); Object result=method.invoke(target,objects); System.out.println("after"); return result; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyInterface myInterface= (MyInterface) new MyProxy(MyImplement.class).getProxy(); myInterface.print(); &#125;&#125; 如何强制使用CGLIB实现AOP？ 添加CGLIB库，SPRING_HOME/cglib/*.jar 在spring配置文件中加入&lt;aop:aspectj-autoproxy proxy-target-class=“true”/&gt; JDK动态代理和CGLIB字节码生成的区别？ JDK动态代理只能对实现了接口的类生成代理，而不能针对类 CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，这就要求被该类或方法不要声明成final]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FBeanFactory%E5%92%8CApplicationContext%2F</url>
    <content type="text"><![CDATA[BeanFactoryBeanFactory提供了最简单的容器的功能，只提供了实例化对象和拿对象的功能，但通常在Spring应用程序中仅使用BeanFactory是不够的。 BeanFactory在启动的时候不会去实例化Bean，只有从容器中拿Bean的时候才会去实例化 应用启动的时候占用资源很少；对资源要求较高的应用，比较有优势 ApplicationContextApplicationContext（应用上下文）继承众多接口（包括BeanFactory接口），它是Spring的一各更高级的容器，提供了更多的有用的功能 1public interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver HierarchicalBeanFactory BeanFactory的子接口HierarchicalBeanFactory是一个具有层级关系的Bean 工厂，拥有属性parentBeanFactory。当获取 Bean对象时，如果当前BeanFactory中不存在对应的bean，则会访问其直接 parentBeanFactory 以尝试获取bean 对象。此外，还可以在当前的 BeanFactory 中 override 父级BeanFactory的同名bean。 ListableBeanFactory ListableBeanFactory 继承了BeanFactory，实现了枚举方法可以列举出当前BeanFactory中所有的bean对象而不必根据name一个一个的获取。 如果 ListableBeanFactory 对象还是一个HierarchicalBeanFactory则getBeanDefinitionNames()方法只会返回当前BeanFactory中的Bean对象而不会去父级BeanFactory中查询。 首先，它是个BeanFactory，可以管理、装配bean，可以有父级BeanFactory实现Bean的层级管理（具体到这里来说它可以有父级的ApplicationContext，因为ApplicationContext本身就是一个BeanFactory。这在web项目中很有用，可以使每个Servlet具有其独立的context, 所有Servlet共享一个父级的context），它还是Listable的，可以枚举出所管理的bean对象。载入多个（有继承关系）上下文 ，使得每一个上下文专注于一个特定的层次，比如应用的web层 EnvironmentCapable：这是一个Environment Holder，只有一个方法Environment getEnvironment() 用来获取Environment对象，提供当前Application运行的所需环境，Environment继承了PropertyResolver（配置文件解析器的最顶级接口，解析配置文件获取属性值等作用）。 MessageSource：用于支撑国际化等功能 ApplicationEventPublisher：可以发布事件给注册的Listener，实现监听机制。 ResourcePatternResolver：其中ResourceLoader用于从一个源（如InputStream等）加载资源文件，ResourcePatternResolver 是ResourceLoader的子类，根据 path-pattern 加载资源文件。 ApplicationContext在启动的时候就把所有的Bean全部实例化了。但是它可以为Bean配置lazy-init=true来让Bean延迟实例化 所有的Bean在启动的时候都加载，系统运行的速度快； 在启动的时候所有的Bean都加载了，我们就能在系统启动的时候，尽早的发现系统中的配置问题 建议web应用，在启动的时候就把所有的Bean都加载了，把费时的操作放到系统启动中完成 ApplicationContext优于BeanFactory，但是它的内存消耗比bean工厂大，如果程序对于内存的使用对于非常挑剔(如applet或移动环境)，可以使用BeanFactory，否则一般情况下应使用ApplicationContext。]]></content>
  </entry>
  <entry>
    <title><![CDATA[ApplicationContext]]></title>
    <url>%2F2019%2F05%2F14%2FSpring%2FApplicationContext%2F</url>
    <content type="text"><![CDATA[applicationcontext和beanfactory都是加载bean的 applicationcontext包含beanfactory的所有功能，是对beanfactory的扩展。 添加了@Qualifier @Autowired等功能 beanfactory适合内存小的 实例化bean比较复杂，FactoryBean是一个工厂类接口，可以通过改接口实例化bean的逻辑 spring中的循环依赖分三种 构造器循环依赖 无法解决 只能跑出beancurrentlyincreateionexception setter注入 通过提前暴露单例工厂方法addSingletionFactory protootype,spring无法完成依赖注入]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>ApplicationContext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zero-Copy]]></title>
    <url>%2F2019%2F05%2F14%2FNetty%2FZero-Copy%2F</url>
    <content type="text"><![CDATA[多个物理分离的buffer，通过逻辑上合并成为一个，从而避免了数据在内存之间的拷贝。 所谓的 Zero-copy, 就是在操作数据时, 不需要将数据 buffer 从一个内存区域拷贝到另一个内存区域. 因为少了一次内存的拷贝, 因此 CPU 的效率就得到的提升. 在 OS 层面上的 Zero-copy 通常指避免在 用户态(User-space) 与 内核态(Kernel-space) 之间来回拷贝数据. 例如 Linux 提供的 mmap 系统调用, 它可以将一段用户空间内存映射到内核空间, 当映射成功后, 用户对这段内存区域的修改可以直接反映到内核空间; 同样地, 内核空间对这段区域的修改也直接反映用户空间. 正因为有这样的映射关系, 我们就不需要在 用户态(User-space) 与 内核态(Kernel-space) 之间拷贝数据, 提高了数据传输的效率. 而需要注意的是, Netty 中的 Zero-copy 与上面我们所提到到 OS 层面上的 Zero-copy 不太一样, Netty的 Zero-coyp完全是在用户态(Java 层面)的, 它的 Zero-copy 的更多的是偏向于 优化数据操作 这样的概念. Netty 的 Zero-copy 体现在如下几个个方面: Netty 提供了 CompositeByteBuf 类, 它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了各个 ByteBuf 之间的拷贝. 通过 wrap 操作, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作. ByteBuf 支持 slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf, 避免了内存的拷贝. 通过 FileRegion 包装的FileChannel.tranferTo 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel, 避免了传统通过循环 write 方式导致的内存拷贝问题. 下面我们就来简单了解一下这几种常见的零拷贝操作. 通过 CompositeByteBuf 实现零拷贝假设我们有一份协议数据, 它由头部和消息体组成, 而头部和消息体是分别存放在两个 ByteBuf 中的, 即: 12ByteBuf header = ...ByteBuf body = ... 我们在代码处理中, 通常希望将 header 和 body 合并为一个 ByteBuf, 方便处理, 那么通常的做法是: 123ByteBuf allBuf = Unpooled.buffer(header.readableBytes() + body.readableBytes());allBuf.writeBytes(header);allBuf.writeBytes(body); 可以看到, 我们将 header 和 body 都拷贝到了新的 allBuf 中了, 这无形中增加了两次额外的数据拷贝操作了. 那么有没有更加高效优雅的方式实现相同的目的呢? 我们来看一下 CompositeByteBuf 是如何实现这样的需求的吧. 12345ByteBuf header = ...ByteBuf body = ...CompositeByteBuf compositeByteBuf = Unpooled.compositeBuffer();compositeByteBuf.addComponents(true, header, body); 上面代码中, 我们定义了一个 CompositeByteBuf 对象, 然后调用 123public CompositeByteBuf addComponents(boolean increaseWriterIndex, ByteBuf... buffers) &#123;...&#125; 方法将 header 与 body 合并为一个逻辑上的 ByteBuf, 即: 1不过需要注意的是, 虽然看起来 CompositeByteBuf 是由两个 ByteBuf 组合而成的, 不过在 CompositeByteBuf 内部, 这两个 ByteBuf 都是单独存在的, CompositeByteBuf 只是逻辑上是一个整体. 上面 CompositeByteBuf 代码还以一个地方值得注意的是, 我们调用 addComponents(boolean increaseWriterIndex, ByteBuf... buffers) 来添加两个 ByteBuf, 其中第一个参数是 true, 表示当添加新的 ByteBuf 时, 自动递增 CompositeByteBuf 的 writeIndex.如果我们调用的是 1compositeByteBuf.addComponents(header, body); 那么其实 compositeByteBuf 的 writeIndex 仍然是0, 因此此时我们就不可能从 compositeByteBuf 中读取到数据, 这一点希望大家要特别注意. 除了上面直接使用 CompositeByteBuf 类外, 我们还可以使用 Unpooled.wrappedBuffer 方法, 它底层封装了 CompositeByteBuf 操作, 因此使用起来更加方便: 1234ByteBuf header = ...ByteBuf body = ...ByteBuf allByteBuf = Unpooled.wrappedBuffer(header, body); 通过 wrap 操作实现零拷贝例如我们有一个 byte 数组, 我们希望将它转换为一个 ByteBuf 对象, 以便于后续的操作, 那么传统的做法是将此 byte 数组拷贝到 ByteBuf 中, 即: 123byte[] bytes = ...ByteBuf byteBuf = Unpooled.buffer();byteBuf.writeBytes(bytes); 显然这样的方式也是有一个额外的拷贝操作的, 我们可以使用 Unpooled 的相关方法, 包装这个 byte 数组, 生成一个新的 ByteBuf 实例, 而不需要进行拷贝操作. 上面的代码可以改为: 12byte[] bytes = ...ByteBuf byteBuf = Unpooled.wrappedBuffer(bytes); 可以看到, 我们通过 Unpooled.wrappedBuffer 方法来将 bytes 包装成为一个 UnpooledHeapByteBuf 对象, 而在包装的过程中, 是不会有拷贝操作的. 即最后我们生成的生成的 ByteBuf 对象是和 bytes 数组共用了同一个存储空间, 对 bytes 的修改也会反映到 ByteBuf 对象中. Unpooled 工具类还提供了很多重载的 wrappedBuffer 方法: 12345678910111213public static ByteBuf wrappedBuffer(byte[] array)public static ByteBuf wrappedBuffer(byte[] array, int offset, int length)public static ByteBuf wrappedBuffer(ByteBuffer buffer)public static ByteBuf wrappedBuffer(ByteBuf buffer)public static ByteBuf wrappedBuffer(byte[]... arrays)public static ByteBuf wrappedBuffer(ByteBuf... buffers)public static ByteBuf wrappedBuffer(ByteBuffer... buffers)public static ByteBuf wrappedBuffer(int maxNumComponents, byte[]... arrays)public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuf... buffers)public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuffer... buffers) 这些方法可以将一个或多个 buffer 包装为一个 ByteBuf 对象, 从而避免了拷贝操作. 通过 slice 操作实现零拷贝slice 操作和 wrap 操作刚好相反, Unpooled.wrappedBuffer 可以将多个 ByteBuf 合并为一个, 而 slice 操作可以将一个 ByteBuf 切片 为多个共享一个存储区域的 ByteBuf 对象.ByteBuf 提供了两个 slice 操作方法: 12public ByteBuf slice();public ByteBuf slice(int index, int length); 不带参数的 slice 方法等同于 buf.slice(buf.readerIndex(), buf.readableBytes()) 调用, 即返回 buf 中可读部分的切片. 而 slice(int index, int length) 方法相对就比较灵活了, 我们可以设置不同的参数来获取到 buf 的不同区域的切片. 下面的例子展示了 ByteBuf.slice 方法的简单用法: 123ByteBuf byteBuf = ...ByteBuf header = byteBuf.slice(0, 5);ByteBuf body = byteBuf.slice(5, 10); 用 slice 方法产生 header 和 body 的过程是没有拷贝操作的, header 和 body 对象在内部其实是共享了 byteBuf 存储空间的不同部分而已. 即: 通过 FileRegion 实现零拷贝Netty 中使用 FileRegion 实现文件传输的零拷贝, 不过在底层 FileRegion 是依赖于 Java NIO FileChannel.transfer 的零拷贝功能. 首先我们从最基础的 Java IO 开始吧. 假设我们希望实现一个文件拷贝的功能, 那么使用传统的方式, 我们有如下实现: 123456789101112public static void copyFile(String srcFile, String destFile) throws Exception &#123; byte[] temp = new byte[1024]; FileInputStream in = new FileInputStream(srcFile); FileOutputStream out = new FileOutputStream(destFile); int length; while ((length = in.read(temp)) != -1) &#123; out.write(temp, 0, length); &#125; in.close(); out.close();&#125; 上面是一个典型的读写二进制文件的代码实现了. 不用我说, 大家肯定都知道, 上面的代码中不断中源文件中读取定长数据到 temp 数组中, 然后再将 temp 中的内容写入目的文件, 这样的拷贝操作对于小文件倒是没有太大的影响, 但是如果我们需要拷贝大文件时, 频繁的内存拷贝操作就消耗大量的系统资源了.下面我们来看一下使用 Java NIO 的 FileChannel 是如何实现零拷贝的: 123456789101112public static void copyFileWithFileChannel(String srcFileName, String destFileName) throws Exception &#123; RandomAccessFile srcFile = new RandomAccessFile(srcFileName, &quot;r&quot;); FileChannel srcFileChannel = srcFile.getChannel(); RandomAccessFile destFile = new RandomAccessFile(destFileName, &quot;rw&quot;); FileChannel destFileChannel = destFile.getChannel(); long position = 0; long count = srcFileChannel.size(); srcFileChannel.transferTo(position, count, destFileChannel);&#125; 可以看到, 使用了 FileChannel 后, 我们就可以直接将源文件的内容直接拷贝(transferTo) 到目的文件中, 而不需要额外借助一个临时 buffer, 避免了不必要的内存操作. 有了上面的一些理论知识, 我们来看一下在 Netty 中是怎么使用 FileRegion 来实现零拷贝传输一个文件的: 1234567891011121314151617181920212223242526272829@Overridepublic void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; RandomAccessFile raf = null; long length = -1; try &#123; // 1. 通过 RandomAccessFile 打开一个文件. raf = new RandomAccessFile(msg, &quot;r&quot;); length = raf.length(); &#125; catch (Exception e) &#123; ctx.writeAndFlush(&quot;ERR: &quot; + e.getClass().getSimpleName() + &quot;: &quot; + e.getMessage() + &apos;\n&apos;); return; &#125; finally &#123; if (length &lt; 0 &amp;&amp; raf != null) &#123; raf.close(); &#125; &#125; ctx.write(&quot;OK: &quot; + raf.length() + &apos;\n&apos;); if (ctx.pipeline().get(SslHandler.class) == null) &#123; // SSL not enabled - can use zero-copy file transfer. // 2. 调用 raf.getChannel() 获取一个 FileChannel. // 3. 将 FileChannel 封装成一个 DefaultFileRegion ctx.write(new DefaultFileRegion(raf.getChannel(), 0, length)); &#125; else &#123; // SSL enabled - cannot use zero-copy file transfer. ctx.write(new ChunkedFile(raf)); &#125; ctx.writeAndFlush(&quot;\n&quot;);&#125; 上面的代码是 Netty 的一个例子, 其源码在 netty/example/src/main/java/io/netty/example/file/FileServerHandler.java可以看到, 第一步是通过 RandomAccessFile 打开一个文件, 然后 Netty 使用了 DefaultFileRegion 来封装一个 FileChannel 即: 1new DefaultFileRegion(raf.getChannel(), 0, length) 当有了 FileRegion 后, 我们就可以直接通过它将文件的内容直接写入 Channel 中, 而不需要像传统的做法: 拷贝文件内容到临时 buffer, 然后再将 buffer 写入 Channel. 通过这样的零拷贝操作, 无疑对传输大文件很有帮助.]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty入门]]></title>
    <url>%2F2019%2F05%2F14%2FNetty%2FNetty%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Netty《Netty原理剖析》 Reactor 模式介绍。 Netty 是 Reactor 模式的一种实现。 为什么选择 Netty 说说业务中，Netty 的使用场景 原生的 NIO 在 JDK 1.7 版本存在 epoll bug 什么是TCP 粘包/拆包 TCP粘包/拆包的解决办法 Netty 线程模型 说说 Netty 的零拷贝 Netty 内部执行流程 Netty 重连实现 深入一些的话，就会问NIO的原理、NIO属于哪种IO模型、NIO的三大组成等等，这有些难，当时我也是研究了很久才搞懂NIO。提一句，NIO并不是严格意义上的非阻塞IO而应该属于多路复用IO，面试回答的时候要注意这个细节，讲到NIO会阻塞在Selector的select方法上会增加面试官对你的好感。 如果用过Netty，可能会问一些Netty的东西，毕竟这个框架基本属于当前最好的NIO框架了（Mina其实也不错，不过总体来说还是比不上Netty的），大多数互联网公司也都在用Netty。 nio netty 监听端口 NioEventLoop 新连接socket Channel 接收数据io byte ByteBuf 业务逻辑 ChannelHandler nioeventloop run()相当于while(true) 单独从性能角度，Netty在基础的NIO等类库之上进行了很多改进，例如：更加优雅的Reactor模式实现、灵活的线程模型、利用EventLoop等创新性的机制，可以非常高效地管理成百上千的Channel。充分利用了Java的Zero-Copy机制，并且从多种角度，“斤斤计较”般的降低内存分配和回收的开销。例如，使用池化的Direct Bufer等技术，在提高IO性能的同时，减少了对象的创建和销毁；利用反射等技术直接操纵SelectionKey，使用数组而不是Java容器等。使用更多本地代码。例如，直接利用JNI调用Open SSL等方式，获得比Java内建SSL引擎更好的性能。在通信协议、序列化等其他角度的优化。总的来说，Netty并没有Java核心类库那些强烈的通用性、跨平台等各种负担，针对性能等特定目标以及Linux等特定环境，采取了一些极致的优化手段。考点分析这是一个比较开放的问题，我给出的回答是个概要性的举例说明。面试官很可能利用这种开放问题作为引子，针对你回答的一个或者多个点，深入探讨你在不同层次上的理解程度。在面试准备中，兼顾整体性的同时，不要忘记选定个别重点进行深入理解掌握，最好是进行源码层面的深入阅读和实验。如果你希望了解更多从性能角度Netty在编码层面的手段，可以参考Norman在Devoxx上的分享，其中的很多技巧对于实现极致性能的API有一定借鉴意义，但在一般的业务开发中要谨慎采用。虽然提到Netty，人们会自然地想到高性能，但是Netty本身的优势不仅仅只有这一个方面，下面我会侧重两个方面：对Netty进行整体介绍，帮你了解其基本组成。从一个简单的例子开始，对比在第11讲中基于IO、NIO等标准API的实例，分析它的技术要点，给你提供一个进一步深入学习的思路。知识扩展首先，我们从整体了解一下Netty。按照官方定义，它是一个异步的、基于事件Client/Server的网络框架，目标是提供一种简单、快速构建网络应用的方式，同时保证高吞吐量、低延时、高可靠性。从设计思路和目的上，Netty与Java自身的NIO框架相比有哪些不同呢？我们知道Java的标准类库，由于其基础性、通用性的定位，往往过于关注技术模型上的抽象，而不是从一线应用开发者的角度去思考。我曾提到过，引入并发包的一个重要原因就是，应用开发者使用Thread API比较痛苦，需要操心的不仅仅是业务逻辑，而且还要自己负责将其映射到Thread模型上。Java NIO的设计也有类似的特点，开发者需要深入掌握线程、IO、网络等相关概念，学习路径很长，很容易导致代码复杂、晦涩，即使是有经验的工程师，也难以快速地写出高可靠性的实现。Netty的设计强调了 “Separation Of Concerns”，通过精巧设计的事件机制，将业务逻辑和无关技术逻辑进行隔离，并通过各种方便的抽象，一定程度上填补了了基础平台和业务开发之间的鸿沟，更有利于在应用开发中普及业界的最佳实践。另外，Netty &gt; java.nio + java. net！从API能力范围来看，Netty完全是Java NIO框架的一个大大的超集，你可以参考Netty官方的模块划分。第38讲 | 对比Java标准NIO类库，你知道Netty是如何实现更高性能的吗？杨晓峰 00:18 / 09:27极客时间除了核心的事件机制等，Netty还额外提供了很多功能，例如：从网络协议的角度，Netty除了支持传输层的UDP、TCP、SCTP协议，也支持HTTP(s)、WebSocket等多种应用层协议，它并不是单一协议的API。在应用中，需要将数据从Java对象转换成为各种应用协议的数据格式，或者进行反向的转换，Netty为此提供了一系列扩展的编解码框架，与应用开发场景无缝衔接，并且性能良好。它扩展了Java NIO Bufer，提供了自己的ByteBuf实现，并且深度支持Direct Bufer等技术，甚至hack了Java内部对Direct Bufer的分配和销毁等。同时，Netty也提供了更加完善的Scatter/Gather机制实现。可以看到，Netty的能力范围大大超过了Java核心类库中的NIO等API，可以说它是一个从应用视角出发的产物。当然，对于基础API设计，Netty也有自己独到的见解，未来Java NIO API也可能据此进行一定的改进，如果你有兴趣可以参考JDK-8187540。接下来，我们一起来看一个入门的代码实例，看看Netty应用到底是什么样子。与第11讲类似，同样是以简化的Echo Server为例，下图是Netty官方提供的Server部分，完整用例请点击链接。上面的例子，虽然代码很短，但已经足够体现出Netty的几个核心概念，请注意我用红框标记出的部分：ServerBootstrap，服务器端程序的入口，这是Netty为简化网络程序配置和关闭等生命周期管理，所引入的Bootstrapping机制。我们通常要做的创建Channel、绑定端口、注册Handler等，都可以通过这个统一的入口，以Fluent API等形式完成，相对简化了API使用。与之相对应， Bootstrap则是Client端的通常入口。Channel，作为一个基于NIO的扩展框架，Channel和Selector等概念仍然是Netty的基础组件，但是针对应用开发具体需求，提供了相对易用的抽象。EventLoop，这是Netty处理事件的核心机制。例子中使用了EventLoopGroup。我们在NIO中通常要做的几件事情，如注册感兴趣的事件、调度相应的Handler等，都是EventLoop负责。ChannelFuture，这是Netty实现异步IO的基础之一，保证了同一个Channel操作的调用顺序。Netty扩展了Java标准的Future，提供了针对自己场景的特有Future定义。ChannelHandler，这是应用开发者放置业务逻辑的主要地方，也是我上面提到的“Separation Of Concerns”原则的体现。ChannelPipeline，它是ChannelHandler链条的容器，每个Channel在创建后，自动被分配一个ChannelPipeline。在上面的示例中，我们通过ServerBootstrap注册了ChannelInitializer，并且实现了initChannel方法，而在该方法中则承担了向ChannelPipleline安装其他Handler的任务。你可以参考下面的简化示意图，忽略Inbound/OutBound Handler的细节，理解这几个基本单元之间的操作流程和对应关系。极客时间对比Java标准NIO的代码，Netty提供的相对高层次的封装，减少了对Selector等细节的操纵，而EventLoop、Pipeline等机制则简化了编程模型，开发者不用担心并发等问题，在一定程度上简化了应用代码的开发。最难能可贵的是，这一切并没有以可靠性、可扩展性为代价，反而将其大幅度提高。我在专栏周末福利中已经推荐了Norman Maurer等编写的《Netty实战》（Netty In Action），如果你想系统学习Netty，它会是个很好的入门参考。针对Netty的一些实现原理，很可能成为面试中的考点，例如：Reactor模式和Netty线程模型。Pipelining、EventLoop等部分的设计实现细节。Netty的内存管理机制、引用计数等特别手段。有的时候面试官也喜欢对比Java标准NIO API，例如，你是否知道Java NIO早期版本中的Epoll空转问题，以及Netty的解决方式等。对于这些知识点，公开的深入解读已经有很多了，在学习时希望你不要一开始就被复杂的细节弄晕，可以结合实例，逐步、有针对性的进行学习。我的一个建议是，可以试着画出相应的示意图，非常有助于理解并能清晰阐述自己的看法。今天，从Netty性能的问题开始，我概要地介绍了Netty框架，并且以Echo Server为例，对比了Netty和Java NIO在设计上的不同。但这些都仅仅是冰山的一角，全面掌握还需要下非常多的功夫。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nio]]></title>
    <url>%2F2019%2F05%2F14%2FNetty%2FNIO%2F</url>
    <content type="text"><![CDATA[初识NIO： ​ 在 JDK 1. 4 中 新 加入 了 NIO( New Input/ Output) 类, 引入了一种基于通道和缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆的 DirectByteBuffer 对象作为这块内存的引用进行操作，避免了在 Java 堆和 Native 堆中来回复制数据。 ​ NIO 是一种同步非阻塞的 IO 模型。同步是指线程不断轮询 IO 事件是否就绪，非阻塞是指线程在等待 IO 的时候，可以同时做其他任务。同步的核心就是 Selector，Selector 代替了线程本身轮询 IO 事件，避免了阻塞同时减少了不必要的线程消耗；非阻塞的核心就是通道和缓冲区，当 IO 事件就绪时，可以通过写道缓冲区，保证 IO 的成功，而无需线程阻塞式地等待。 Buffer： ​ 为什么说NIO是基于缓冲区的IO方式呢？因为，当一个链接建立完成后，IO的数据未必会马上到达，为了当数据到达时能够正确完成IO操作，在BIO（阻塞IO）中，等待IO的线程必须被阻塞，以全天候地执行IO操作。为了解决这种IO方式低效的问题，引入了缓冲区的概念，当数据到达时，可以预先被写入缓冲区，再由缓冲区交给线程，因此线程无需阻塞地等待IO。 通道： ​ 当执行：SocketChannel.write(Buffer)，便将一个 buffer 写到了一个通道中。如果说缓冲区还好理解，通道相对来说就更加抽象。网上博客难免有写不严谨的地方，容易使初学者感到难以理解。 ​ 引用 Java NIO 中权威的说法：通道是 I/O 传输发生时通过的入口，而缓冲区是这些数 据传输的来源或目标。对于离开缓冲区的传输，您想传递出去的数据被置于一个缓冲区，被传送到通道。对于传回缓冲区的传输，一个通道将数据放置在您所提供的缓冲区中。 ​ 例如 有一个服务器通道 ServerSocketChannel serverChannel，一个客户端通道 SocketChannel clientChannel；服务器缓冲区：serverBuffer，客户端缓冲区：clientBuffer。 ​ 当服务器想向客户端发送数据时，需要调用：clientChannel.write(serverBuffer)。当客户端要读时，调用 clientChannel.read(clientBuffer) ​ 当客户端想向服务器发送数据时，需要调用：serverChannel.write(clientBuffer)。当服务器要读时，调用 serverChannel.read(serverBuffer) ​ 这样，通道和缓冲区的关系似乎更好理解了。在实践中，未必会出现这种双向连接的蠢事（然而这确实存在的，后面的内容还会涉及），但是可以理解为在NIO中：如果想将Data发到目标端，则需要将存储该Data的Buffer，写入到目标端的Channel中，然后再从Channel中读取数据到目标端的Buffer中。 Selector： ​ 通道和缓冲区的机制，使得线程无需阻塞地等待IO事件的就绪，但是总是要有人来监管这些IO事件。这个工作就交给了selector来完成，这就是所谓的同步。 ​ Selector允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便。 ​ 要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪，这就是所说的轮询。一旦这个方法返回，线程就可以处理这些事件。 ​ Selector中注册的感兴趣事件有： OP_ACCEPT OP_CONNECT OP_READ OP_WRITE 优化： ​ 一种优化方式是：将Selector进一步分解为Reactor，将不同的感兴趣事件分开，每一个Reactor只负责一种感兴趣的事件。这样做的好处是：1、分离阻塞级别，减少了轮询的时间；2、线程无需遍历set以找到自己感兴趣的事件，因为得到的set中仅包含自己感兴趣的事件。 NIO和epoll： ​ epoll是Linux内核的IO模型。我想一定有人想问，AIO听起来比NIO更加高大上，为什么不使用AIO？AIO其实也有应用，但是有一个问题就是，Linux是不支持AIO的，因此基于AIO的程序运行在Linux上的效率相比NIO反而更低。而Linux是最主要的服务器OS，因此相比AIO，目前NIO的应用更加广泛。 ​ 说到这里，可能你已经明白了，epoll一定和NIO有着很深的因缘。没错，如果仔细研究epoll的技术内幕，你会发现它确实和NIO非常相似，都是基于“通道”和缓冲区的，也有selector，只是在epoll中，通道实际上是操作系统的“管道”。和NIO不同的是，NIO中，解放了线程，但是需要由selector阻塞式地轮询IO事件的就绪；而epoll中，IO事件就绪后，会自动发送消息，通知selector：“我已经就绪了。”可以认为，Linux的epoll是一种效率更高的NIO。 NIO轶事： ​ 一篇有意思的博客，讲的 Java selector.open() 的时候，会创建一个自己和自己的链接（windows上是tcp，linux上是通道） ​ 这么做的原因：可以从 Apache Mina 中窥探。在 Mina 中，有如下机制： Mina框架会创建一个Work对象的线程。 Work对象的线程的run()方法会从一个队列中拿出一堆Channel，然后使用Selector.select()方法来侦听是否有数据可以读/写。 最关键的是，在select的时候，如果队列有新的Channel加入，那么，Selector.select()会被唤醒，然后重新select最新的Channel集合。 要唤醒select方法，只需要调用Selector的wakeup()方法。 ​ 而一个阻塞在select上的线程有以下三种方式可以被唤醒： 有数据可读/写，或出现异常。 阻塞时间到，即time out。 收到一个non-block的信号。可由kill或pthread_kill发出。 ​ 首先 2 可以排除，而第三种方式，只在linux中存在。因此，Java NIO为什么要创建一个自己和自己的链接：就是如果想要唤醒select，只需要朝着自己的这个loopback连接发点数据过去，于是，就可以唤醒阻塞在select上的线程了。 《Java NIO编写Socket服务器的一个例子》 1.Java NIO概览首先，熟悉一下NIO的主要组成部分：Bufer，高效的数据容器，除了布尔类型，所有原始数据类型都有相应的Bufer实现。Channel，类似在Linux之类操作系统上看到的文件描述符，是NIO中被用来支持批量式IO操作的一种抽象。File或者Socket，通常被认为是比较高层次的抽象，而Channel则是更加操作系统底层的一种抽象，这也使得NIO得以充分利用现代操作系统底层机制，获得特定场景的性能优化，例如，DMA（Direct Memory Access）等。不同层次的抽象是相互关联的，我们可以通过Socket获取Channel，反之亦然。Selector，是NIO实现多路复用的基础，它提供了一种高效的机制，可以检测到注册在Selector上的多个Channel中，是否有Channel处于就绪状态，进而实现了单线程对多Channel的高效管理。Selector同样是基于底层操作系统机制，不同模式、不同版本都存在区别，例如，在最新的代码库里，相关实现如下：Linux上依赖于epoll（http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/linux/classes/sun/nio/ch/EPollSelectorImpl.java）。Windows上NIO2（AIO）模式则是依赖于iocp（http://hg.openjdk.java.net/jdk/jdk/fle/d8327f838b88/src/java.base/windows/classes/sun/nio/ch/Iocp.java）。Chartset，提供Unicode字符串定义，NIO也提供了相应的编解码器等，例如，通过下面的方式进行字符串到ByteBufer的转换：Charset.defaultCharset().encode(“Hello world!”));2.NIO能解决什么问题？下面我通过一个典型场景，来分析为什么需要NIO，为什么需要多路复用。设想，我们需要实现一个服务器应用，只简单要求能够同时服务多个客户端请求即可。使用java.io和java.net中的同步、阻塞式API，可以简单实现。public class DemoServer extends Thread {private ServerSocket serverSocket;public int getPort() {return serverSocket.getLocalPort();}public void run() {try {极客时间serverSocket = new ServerSocket(0);while (true) {Socket socket = serverSocket.accept();RequesHandler requesHandler = new RequesHandler(socket);requesHandler.sart();}} catch (IOException e) {e.printStackTrace();} fnally {if (serverSocket != null) {try {serverSocket.close();} catch (IOException e) {e.printStackTrace();};}}}public satic void main(String[] args) throws IOException {DemoServer server = new DemoServer();server.sart();try (Socket client = new Socket(InetAddress.getLocalHos(), server.getPort())) {BuferedReader buferedReader = new BuferedReader(new InputStreamReader(client.getInputStream()));buferedReader.lines().forEach(s -&gt; Sysem.out.println(s));}}}// 简化实现，不做读取，直接发送字符串class RequesHandler extends Thread {private Socket socket;RequesHandler(Socket socket) {this.socket = socket;}@Overridepublic void run() {try (PrintWriter out = new PrintWriter(socket.getOutputStream());) {out.println(“Hello world!”);out.fush();} catch (Exception e) {e.printStackTrace();}}}其实现要点是：服务器端启动ServerSocket，端口0表示自动绑定一个空闲端口。调用accept方法，阻塞等待客户端连接。利用Socket模拟了一个简单的客户端，只进行连接、读取、打印。当连接建立后，启动一个单独线程负责回复客户端请求。这样，一个简单的Socket服务器就被实现出来了。思考一下，这个解决方案在扩展性方面，可能存在什么潜在问题呢？大家知道Java语言目前的线程实现是比较重量级的，启动或者销毁一个线程是有明显开销的，每个线程都有单独的线程栈等结构，需要占用非常明显的内存，所以，每一个Client启动一个线程似乎都有些浪费。那么，稍微修正一下这个问题，我们引入线程池机制来避免浪费。serverSocket = new ServerSocket(0);executor = Executors.newFixedThreadPool(8);while (true) {Socket socket = serverSocket.accept();RequesHandler requesHandler = new RequesHandler(socket);executor.execute(requesHandler);}这样做似乎好了很多，通过一个固定大小的线程池，来负责管理工作线程，避免频繁创建、销毁线程的开销，这是我们构建并发服务的典型方式。这种工作方式，可以参考下图来理解。极客时间如果连接数并不是非常多，只有最多几百个连接的普通应用，这种模式往往可以工作的很好。但是，如果连接数量急剧上升，这种实现方式就无法很好地工作了，因为线程上下文切换开销会在高并发时变得很明显，这是同步阻塞方式的低扩展性劣势。NIO引入的多路复用机制，提供了另外一种思路，请参考我下面提供的新的版本。public class NIOServer extends Thread {public void run() {try (Selector selector = Selector.open();ServerSocketChannel serverSocket = ServerSocketChannel.open();) {// 创建Selector和ChannelserverSocket.bind(new InetSocketAddress(InetAddress.getLocalHos(), 8888));serverSocket.confgureBlocking(false);// 注册到Selector，并说明关注点serverSocket.regiser(selector, SelectionKey.OP_ACCEPT);while (true) {selector.select();// 阻塞等待就绪的Channel，这是关键点之一Set selectedKeys = selector.selectedKeys();Iterator iter = selectedKeys.iterator();while (iter.hasNext()) {SelectionKey key = iter.next();// 生产系统中一般会额外进行就绪状态检查sayHelloWorld((ServerSocketChannel) key.channel());iter.remove();}}} catch (IOException e) {e.printStackTrace();}}private void sayHelloWorld(ServerSocketChannel server) throws IOException {try (SocketChannel client = server.accept();) { client.write(Charset.defaultCharset().encode(“Hello world!”));}}// 省略了与前面类似的main}这个非常精简的样例掀开了NIO多路复用的面纱，我们可以分析下主要步骤和元素：首先，通过Selector.open()创建一个Selector，作为类似调度员的角色。然后，创建一个ServerSocketChannel，并且向Selector注册，通过指定SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求。注意，为什么我们要明确配置非阻塞模式呢？这是因为阻塞模式下，注册操作是不允许的，会抛出IllegalBlockingModeException异常。Selector阻塞在select操作，当有Channel发生接入请求，就会被唤醒。在sayHelloWorld方法中，通过SocketChannel和Bufer进行数据操作，在本例中是发送了一段字符串。可以看到，在前面两个样例中，IO都是同步阻塞模式，所以需要多线程以实现多任务处理。而NIO则是利用了单线程轮询事件的机制，通过高效地定位就绪的Channel，来决定做什么，仅仅select阶段是阻塞的，可以有效避免大量客户端连接时，频繁线程切换带来的问题，应用的扩展能力有了非常大的提高。下面这张图对这种实现思路进行了形象地说明。极客时间在Java 7引入的NIO 2中，又增添了一种额外的异步IO模式，利用事件和回调，处理Accept、Read等操作。 AIO实现看起来是类似这样子：AsynchronousServerSocketChannel serverSock = AsynchronousServerSocketChannel.open().bind(sockAddr);serverSock.accept(serverSock, new CompletionHandler&lt;&gt;() { //为异步操作指定CompletionHandler回调函数@Overridepublic void completed(AsynchronousSocketChannel sockChannel, AsynchronousServerSocketChannel serverSock) {serverSock.accept(serverSock, this);// 另外一个 write（sock，CompletionHandler{}）sayHelloWorld(sockChannel, Charset.defaultCharset().encode(“Hello World!”));}// 省略其他路径处理方法…});鉴于其编程要素（如Future、CompletionHandler等），我们还没有进行准备工作，为避免理解困难，我会在专栏后面相关概念补充后的再进行介绍，尤其是Reactor、Proactor模式等方面将在Netty主题一起分析，这里我先进行概念性的对比：基本抽象很相似，AsynchronousServerSocketChannel对应于上面例子中的ServerSocketChannel；AsynchronousSocketChannel则对应SocketChannel。业务逻辑的关键在于，通过指定CompletionHandler回调接口，在accept/read/write等关键节点，通过事件机制调用，这是非常不同的一种编程思路。今天我初步对Java提供的IO机制进行了介绍，概要地分析了传统同步IO和NIO的主要组成，并根据典型场景，通过不同的IO模式进行了实现与拆解。专栏下一讲，我还将继续分析Java IO的主题。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis中的设计模式]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2FMybatis%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、装饰模式 最明显的就是cache包下面的实现 Cahe、LoggingCache、LruCache、TransactionalCahe…等 以LoggingCache为例，UML图 ​ Cache cache = new LoggingCache(new PerpetualCache(“cacheid”));一层层包装就使得默认cache实现PerpetualCache具有附加的功能，比如上面的log功能。二、建造者模式 BaseBuilder、XMLMapperBuilder 三、工厂方法 SqlSessionFactory 四、适配器模式 Log、LogFactory 五、模板方法 BaseExecutor、SimpleExecutor 六、动态代理 Plugin 见7图 7、责任链模式 Interceptor、InterceptorChain]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis缓存]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2FMyBatis%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[前言MyBatis是常见的Java数据库访问层框架。在日常工作中，开发人员多数情况下是使用MyBatis的默认缓存配置，但是MyBatis缓存机制有一些不足之处，在使用中容易引起脏数据，形成一些潜在的隐患。个人在业务开发中也处理过一些由于MyBatis缓存引发的开发问题，带着个人的兴趣，希望从应用及源码的角度为读者梳理MyBatis缓存机制。本次分析中涉及到的代码和数据库表均放在GitHub上，地址： mybatis-cache-demo 。 目录本文按照以下顺序展开。 一级缓存介绍及相关配置。 一级缓存工作流程及源码分析。 一级缓存总结。 二级缓存介绍及相关配置。 二级缓存源码分析。 二级缓存总结。 全文总结。 一级缓存一级缓存介绍在应用运行过程中，我们有可能在一次数据库会话中，执行多次查询条件完全相同的SQL，MyBatis提供了一级缓存的方案优化这部分场景，如果是相同的SQL语句，会优先命中一级缓存，避免直接对数据库进行查询，提高性能。具体执行过程如下图所示。每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户。具体实现类的类关系图如下图所示。 一级缓存配置我们来看看如何使用MyBatis一级缓存。开发者只需在MyBatis的配置文件中，添加如下语句，就可以使用一级缓存。共有两个选项，SESSION或者STATEMENT，默认是SESSION级别，即在一个MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是STATEMENT级别，可以理解为缓存只对当前执行的这一个Statement有效。 1&lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt; 一级缓存实验接下来通过实验，了解MyBatis一级缓存的效果，每个单元测试后都请恢复被修改的数据。首先是创建示例表student，创建对应的POJO类和增改的方法，具体可以在entity包和mapper包中查看。 123456CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(200) COLLATE utf8_bin DEFAULT NULL, `age` tinyint(3) unsigned DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 在以下实验中，id为1的学生名称是凯伦。 实验1开启一级缓存，范围为会话级别，调用三次getStudentById，代码如下所示： 1234567public void getStudentById() throws Exception &#123; SqlSession sqlSession = factory.openSession(true); // 自动提交事务 StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); &#125; 执行结果：我们可以看到，只有第一次真正查询了数据库，后续的查询使用了一级缓存。 实验2增加了对数据库的修改操作，验证在一次数据库会话中，如果对数据库发生了修改操作，一级缓存是否会失效。 123456789@Testpublic void addStudent() throws Exception &#123; SqlSession sqlSession = factory.openSession(true); // 自动提交事务 StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(&quot;增加了&quot; + studentMapper.addStudent(buildStudent()) + &quot;个学生&quot;); System.out.println(studentMapper.getStudentById(1)); sqlSession.close();&#125; 执行结果：我们可以看到，在修改操作后执行的相同查询，查询了数据库，一级缓存失效。 实验3开启两个SqlSession，在sqlSession1中查询数据，使一级缓存生效，在sqlSession2中更新数据库，验证一级缓存只在数据库会话内部共享。 1234567891011121314@Testpublic void testLocalCacheScope() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper2更新了&quot; + studentMapper2.updateStudentName(&quot;小岑&quot;,1) + &quot;个学生的数据&quot;); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; sqlSession2更新了id为1的学生的姓名，从凯伦改为了小岑，但session1之后的查询中，id为1的学生的名字还是凯伦，出现了脏数据，也证明了之前的设想，一级缓存只在数据库会话内部共享。 一级缓存工作流程&amp;源码分析那么，一级缓存的工作流程是怎样的呢？我们从源码层面来学习一下。 工作流程一级缓存执行的时序图，如下图所示。 源码分析接下来将对MyBatis查询相关的核心类和一级缓存的源码进行走读。这对后面学习二级缓存也有帮助。SqlSession： 对外提供了用户和数据库之间交互需要的所有方法，隐藏了底层的细节。默认实现类是DefaultSqlSession。 Executor： SqlSession向用户提供操作数据库的方法，但和数据库操作有关的职责都会委托给Executor。 如下图所示，Executor有若干个实现类，为Executor赋予了不同的能力，大家可以根据类名，自行学习每个类的基本作用。 在一级缓存的源码分析中，主要学习BaseExecutor的内部实现。BaseExecutor： BaseExecutor是一个实现了Executor接口的抽象类，定义若干抽象方法，在执行的时候，把具体的操作委托给子类进行执行。 1234protected abstract int doUpdate(MappedStatement ms, Object parameter) throws SQLException;protected abstract List&lt;BatchResult&gt; doFlushStatements(boolean isRollback) throws SQLException;protected abstract &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException;protected abstract &lt;E&gt; Cursor&lt;E&gt; doQueryCursor(MappedStatement ms, Object parameter, RowBounds rowBounds, BoundSql boundSql) throws SQLException; 在一级缓存的介绍中提到对Local Cache的查询和写入是在Executor内部完成的。在阅读BaseExecutor的代码后发现Local Cache是BaseExecutor内部的一个成员变量，如下代码所示。 123public abstract class BaseExecutor implements Executor &#123;protected ConcurrentLinkedQueue&lt;DeferredLoad&gt; deferredLoads;protected PerpetualCache localCache; Cache： MyBatis中的Cache接口，提供了和缓存相关的最基本的操作，如下图所示。有若干个实现类，使用装饰器模式互相组装，提供丰富的操控缓存的能力，部分实现类如下图所示。BaseExecutor成员变量之一的PerpetualCache，是对Cache接口最基本的实现，其实现非常简单，内部持有HashMap，对一级缓存的操作实则是对HashMap的操作。如下代码所示。 123public class PerpetualCache implements Cache &#123; private String id; private Map&lt;Object, Object&gt; cache = new HashMap&lt;Object, Object&gt;(); 在阅读相关核心类代码后，从源代码层面对一级缓存工作中涉及到的相关代码，出于篇幅的考虑，对源码做适当删减，读者朋友可以结合本文，后续进行更详细的学习。为执行和数据库的交互，首先需要初始化SqlSession，通过DefaultSqlSessionFactory开启SqlSession： 12345private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; ............ final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit);&#125; 在初始化SqlSesion时，会使用Configuration类创建一个全新的Executor，作为DefaultSqlSession构造函数的参数，创建Executor代码如下所示： 123456789101112131415161718public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; // 尤其可以注意这里，如果二级缓存开关开启的话，是使用CahingExecutor装饰BaseExecutor的子类 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; executor = (Executor) interceptorChain.pluginAll(executor); return executor;&#125; SqlSession创建完毕后，根据Statment的不同类型，会进入SqlSession的不同方法中，如果是Select语句的话，最后会执行到SqlSession的selectList，代码如下所示： 12345@Overridepublic &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);&#125; SqlSession把具体的查询职责委托给了Executor。如果只开启了一级缓存的话，首先会进入BaseExecutor的query方法。代码如下所示： 123456@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 在上述代码中，会先根据传入的参数生成CacheKey，进入该方法查看CacheKey是如何生成的，代码如下所示： 1234567CacheKey cacheKey = new CacheKey();cacheKey.update(ms.getId());cacheKey.update(rowBounds.getOffset());cacheKey.update(rowBounds.getLimit());cacheKey.update(boundSql.getSql());//后面是update了sql中带的参数cacheKey.update(value); 在上述的代码中，将MappedStatement的Id、sql的offset、Sql的limit、Sql本身以及Sql中的参数传入了CacheKey这个类，最终构成CacheKey。以下是这个类的内部结构： 123456789101112131415private static final int DEFAULT_MULTIPLYER = 37;private static final int DEFAULT_HASHCODE = 17;private int multiplier;private int hashcode;private long checksum;private int count;private List&lt;Object&gt; updateList;public CacheKey() &#123; this.hashcode = DEFAULT_HASHCODE; this.multiplier = DEFAULT_MULTIPLYER; this.count = 0; this.updateList = new ArrayList&lt;Object&gt;();&#125; 首先是成员变量和构造函数，有一个初始的hachcode和乘数，同时维护了一个内部的updatelist。在CacheKey的update方法中，会进行一个hashcode和checksum的计算，同时把传入的参数添加进updatelist中。如下代码所示。 123456789public void update(Object object) &#123; int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object); count++; checksum += baseHashCode; baseHashCode *= count; hashcode = multiplier * hashcode + baseHashCode; updateList.add(object);&#125; 同时重写了CacheKey的equals方法，代码如下所示： 123456789101112@Overridepublic boolean equals(Object object) &#123; ............. for (int i = 0; i &lt; updateList.size(); i++) &#123; Object thisObject = updateList.get(i); Object thatObject = cacheKey.updateList.get(i); if (!ArrayUtil.equals(thisObject, thatObject)) &#123; return false; &#125; &#125; return true;&#125; 除去hashcode，checksum和count的比较外，只要updatelist中的元素一一对应相等，那么就可以认为是CacheKey相等。只要两条SQL的下列五个值相同，即可以认为是相同的SQL。 Statement Id + Offset + Limmit + Sql + Params BaseExecutor的query方法继续往下走，代码如下所示： 1234567list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;if (list != null) &#123; // 这个主要是处理存储过程用的。 handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);&#125; 如果查不到的话，就从数据库查，在queryFromDatabase中，会对localcache进行写入。在query方法执行的最后，会判断一级缓存级别是否是STATEMENT级别，如果是的话，就清空缓存，这也就是STATEMENT级别的一级缓存无法共享localCache的原因。代码如下所示： 123if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; clearLocalCache();&#125; 在源码分析的最后，我们确认一下，如果是insert/delete/update方法，缓存就会刷新的原因。SqlSession的insert方法和delete方法，都会统一走update的流程，代码如下所示： 12345678@Overridepublic int insert(String statement, Object parameter) &#123; return update(statement, parameter); &#125; @Override public int delete(String statement) &#123; return update(statement, null);&#125; update方法也是委托给了Executor执行。BaseExecutor的执行方法如下所示。 123456789@Overridepublic int update(MappedStatement ms, Object parameter) throws SQLException &#123; ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing an update&quot;).object(ms.getId()); if (closed) &#123; throw new ExecutorException(&quot;Executor was closed.&quot;); &#125; clearLocalCache(); return doUpdate(ms, parameter);&#125; 每次执行update前都会清空localCache。 至此，一级缓存的工作流程讲解以及源码分析完毕。 总结 MyBatis一级缓存的生命周期和SqlSession一致。 MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。 二级缓存二级缓存介绍在上文中提到的一级缓存中，其最大的共享范围就是一个SqlSession内部，如果多个SqlSession之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用CachingExecutor装饰Executor，进入一级缓存的查询流程前，先在CachingExecutor进行二级缓存的查询，具体的工作流程如下所示。 二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。当开启缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。 二级缓存配置要正确的使用二级缓存，需完成如下配置的。 在MyBatis的配置文件中开启二级缓存。 1&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; 在MyBatis的映射XML中配置cache或者 cache-ref 。 cache标签用于声明这个namespace使用二级缓存，并且可以自定义配置。 1&lt;cache/&gt; type：cache使用的类型，默认是PerpetualCache，这在一级缓存中提到过。 eviction： 定义回收的策略，常见的有FIFO，LRU。 flushInterval： 配置一定时间自动刷新缓存，单位是毫秒。 size： 最多缓存对象的个数。 readOnly： 是否只读，若配置可读写，则需要对应的实体类能够序列化。 blocking： 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。 cache-ref代表引用别的命名空间的Cache配置，两个命名空间的操作使用的是同一个Cache。 1&lt;cache-ref namespace=&quot;mapper.StudentMapper&quot;/&gt; 二级缓存实验接下来我们通过实验，了解MyBatis二级缓存在使用上的一些特点。在本实验中，id为1的学生名称初始化为点点。 实验1测试二级缓存效果，不提交事务，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。 1234567891011@Testpublic void testCacheWithoutCommitOrClose() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; 执行结果：我们可以看到，当sqlsession没有调用commit()方法时，二级缓存并没有起到作用。 实验2测试二级缓存效果，当提交事务时，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。 123456789101112@Testpublic void testCacheWithCommitOrClose() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; 从图上可知，sqlsession2的查询，使用了缓存，缓存的命中率是0.5。 实验3测试update操作是否会刷新该namespace下的二级缓存。 123456789101112131415161718@Testpublic void testCacheWithUpdate() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); StudentMapper studentMapper3 = sqlSession3.getMapper(StudentMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1)); studentMapper3.updateStudentName(&quot;方方&quot;,1); sqlSession3.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentById(1));&#125; 我们可以看到，在sqlSession3更新数据库，并提交事务后，sqlsession2的StudentMapper namespace下的查询走了数据库，没有走Cache。 实验4验证MyBatis的二级缓存不适应用于映射文件中存在多表查询的情况。通常我们会为每个单表创建单独的映射文件，由于MyBatis的二级缓存是基于namespace的，多表查询语句所在的namspace无法感应到其他namespace中的语句对多表查询中涉及的表进行的修改，引发脏数据问题。 123456789101112131415161718@Testpublic void testCacheWithDiffererntNamespace() throws Exception &#123; SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); ClassMapper classMapper = sqlSession3.getMapper(ClassMapper.class); System.out.println(&quot;studentMapper读取数据: &quot; + studentMapper.getStudentByIdWithClassInfo(1)); sqlSession1.close(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentByIdWithClassInfo(1)); classMapper.updateClassName(&quot;特色一班&quot;,1); sqlSession3.commit(); System.out.println(&quot;studentMapper2读取数据: &quot; + studentMapper2.getStudentByIdWithClassInfo(1));&#125; 执行结果：在这个实验中，我们引入了两张新的表，一张class，一张classroom。class中保存了班级的id和班级名，classroom中保存了班级id和学生id。我们在StudentMapper中增加了一个查询方法getStudentByIdWithClassInfo，用于查询学生所在的班级，涉及到多表查询。在ClassMapper中添加了updateClassName，根据班级id更新班级名的操作。当sqlsession1的studentmapper查询数据后，二级缓存生效。保存在StudentMapper的namespace下的cache中。当sqlSession3的classMapper的updateClassName方法对class表进行更新时，updateClassName不属于StudentMapper的namespace，所以StudentMapper下的cache没有感应到变化，没有刷新缓存。当StudentMapper中同样的查询再次发起时，从缓存中读取了脏数据。 实验5为了解决实验4的问题呢，可以使用Cache ref，让ClassMapper引用StudenMapper命名空间，这样两个映射文件对应的Sql操作都使用的是同一块缓存了。执行结果：不过这样做的后果是，缓存的粒度变粗了，多个Mapper namespace下的所有操作都会对缓存使用造成影响。 二级缓存源码分析MyBatis二级缓存的工作流程和前文提到的一级缓存类似，只是在一级缓存处理前，用CachingExecutor装饰了BaseExecutor的子类，在委托具体职责给delegate之前，实现了二级缓存的查询和写入功能，具体类关系图如下图所示。 源码分析源码分析从CachingExecutor的query方法展开，源代码走读过程中涉及到的知识点较多，不能一一详细讲解，读者朋友可以自行查询相关资料来学习。CachingExecutor的query方法，首先会从MappedStatement中获得在配置初始化时赋予的Cache。 1Cache cache = ms.getCache(); 本质上是装饰器模式的使用，具体的装饰链是 SynchronizedCache -&gt; LoggingCache -&gt; SerializedCache -&gt; LruCache -&gt; PerpetualCache。 以下是具体这些Cache实现类的介绍，他们的组合为Cache赋予了不同的能力。 SynchronizedCache： 同步Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache： 日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache： 序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache： 采用了Lru算法的Cache实现，移除最近最少使用的key/value。 PerpetualCache： 作为为最基础的缓存类，底层实现比较简单，直接使用了HashMap。 然后是判断是否需要刷新缓存，代码如下所示： 1flushCacheIfRequired(ms); 在默认的设置中SELECT语句不会刷新缓存，insert/update/delte会刷新缓存。进入该方法。代码如下所示： 123456private void flushCacheIfRequired(MappedStatement ms) &#123; Cache cache = ms.getCache(); if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123; tcm.clear(cache); &#125;&#125; MyBatis的CachingExecutor持有了TransactionalCacheManager，即上述代码中的tcm。TransactionalCacheManager中持有了一个Map，代码如下所示： 1private Map&lt;Cache, TransactionalCache&gt; transactionalCaches = new HashMap&lt;Cache, TransactionalCache&gt;(); 这个Map保存了Cache和用TransactionalCache包装后的Cache的映射关系。TransactionalCache实现了Cache接口，CachingExecutor会默认使用他包装初始生成的Cache，作用是如果事务提交，对缓存的操作才会生效，如果事务回滚或者不提交事务，则不对缓存产生影响。在TransactionalCache的clear，有以下两句。清空了需要在提交时加入缓存的列表，同时设定提交时清空缓存，代码如下所示： 12345@Overridepublic void clear() &#123; clearOnCommit = true; entriesToAddOnCommit.clear();&#125; CachingExecutor继续往下走，ensureNoOutParams主要是用来处理存储过程的，暂时不用考虑。 12if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, parameterObject, boundSql); 之后会尝试从tcm中获取缓存的列表。 1List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); 在getObject方法中，会把获取值的职责一路传递，最终到PerpetualCache。如果没有查到，会把key加入Miss集合，这个主要是为了统计命中率。 1234Object object = delegate.getObject(key);if (object == null) &#123; entriesMissedInCache.add(key);&#125; CachingExecutor继续往下走，如果查询到数据，则调用tcm.putObject方法，往缓存中放入值。 1234if (list == null) &#123; list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116&#125; tcm的put方法也不是直接操作缓存，只是在把这次的数据和key放入待提交的Map中。 1234@Overridepublic void putObject(Object key, Object object) &#123; entriesToAddOnCommit.put(key, object);&#125; 从以上的代码分析中，我们可以明白，如果不调用commit方法的话，由于TranscationalCache的作用，并不会对二级缓存造成直接的影响。因此我们看看Sqlsession的commit方法中做了什么。代码如下所示： 1234@Overridepublic void commit(boolean force) &#123; try &#123; executor.commit(isCommitOrRollbackRequired(force)); 因为我们使用了CachingExecutor，首先会进入CachingExecutor实现的commit方法。 12345@Overridepublic void commit(boolean required) throws SQLException &#123; delegate.commit(required); tcm.commit();&#125; 会把具体commit的职责委托给包装的Executor。主要是看下tcm.commit()，tcm最终又会调用到TrancationalCache。 1234567public void commit() &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; flushPendingEntries(); reset();&#125; 看到这里的clearOnCommit就想起刚才TrancationalCache的clear方法设置的标志位，真正的清理Cache是放到这里来进行的。具体清理的职责委托给了包装的Cache类。之后进入flushPendingEntries方法。代码如下所示： 123456private void flushPendingEntries() &#123; for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123; delegate.putObject(entry.getKey(), entry.getValue()); &#125; ................&#125; 在flushPendingEntries中，将待提交的Map进行循环处理，委托给包装的Cache类，进行putObject的操作。后续的查询操作会重复执行这套流程。如果是insert|update|delete的话，会统一进入CachingExecutor的update方法，其中调用了这个函数，代码如下所示： 1private void flushCacheIfRequired(MappedStatement ms) 在二级缓存执行流程后就会进入一级缓存的执行流程，因此不再赘述。 总结 MyBatis的二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。 MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本，直接使用Redis,Memcached等分布式缓存可能成本更低，安全性也更高。 全文总结本文对介绍了MyBatis一二级缓存的基本概念，并从应用及源码的角度对MyBatis的缓存机制进行了分析。最后对MyBatis缓存机制做了一定的总结，个人建议MyBatis缓存特性在生产环境中进行关闭，单纯作为一个ORM框架使用可能更为合适。]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos常用软件安装]]></title>
    <url>%2F2019%2F05%2F14%2FBasic%2Flinux%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[配置IP1vi /etc/sysconfig/network-scripts/ifcfg-ens33 12345BOOTPROTO=&quot;static&quot;IPADDR=&quot;192.168.10.110&quot;NETMASK=&quot;255.255.255.0&quot;GATEWAY=&quot;192.168.10.1&quot;DNS1=&quot;8.8.8.8&quot; 1systemctl restart network 12345678910vi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=hadoop1vi /etc/hosts192.168.10.110 hadoop1192.168.10.111 hadoop2192.168.10.112 hadoop3192.168.10.113 hadoop4192.168.10.114 hadoop5192.168.10.115 hadoop6 关闭防火墙12systemctl stop firewalld.servicesystemctl disable firewalld.service 安装JDK1vi /etc/profile 123export JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tool.jar 1source /etc/profile yum install java-1.8.0-openjdk -y 安装MySQL123456789101112wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum install mysql-serverchkconfig mysqld onsystemctl enable mysqldsystemctl start mysqldmysql -u rootset password for root@localhost=password(&apos;root&apos;);mysql -u root -prootgrant all PRIVILEGES on *.* to masonnpe@&apos;%&apos; identified by &apos;cx245852&apos;;flush privileges; zookeeper123cd confcp zoo_sample.cfg zoo.cfgvi zoo.cfg 1dataDir=/usr/local/zookeeper/data 1bin/zkServer.sh start 集群： vi zoo.cfg新增：server.0=node01:2888:3888server.1=node02:2888:3888server.2=node03:2888:3888 cd /usr/local/zookeeper/datavi myid0其他节点修改myid kafka1vi config/server.properties 12host.name=node #修改主机名zookeeper.connect=node:2181 #修改Zookeeper服务器地址 12345bin/kafka-server-start.sh -daemon config/server.properties# 创建topicbin/kafka-topics.sh --create --zookeeper node:2181 --replication-factor 1 --partitions 1 --topic userinfobin/kafka-console-producer.sh --broker-list node:9092 --topic userinfobin/kafka-console-consumer.sh --zookeeper node:2181 —topic --whitelist userinfo --from-beginning 集群： vi /usr/local/kafka/config/server.propertiesbroker.id=0zookeeper.connect=node01:2181,node02:2181,node03:2181其他节点修改server.properties中的broker.id 设置为1和2 Redis12345678cd redismake &amp;&amp; make installcp utils/redis_init_script /etc/init.dmv redis_init_script redis_6379mkdir /etc/rediscp redis.conf /etc/redismv redis.conf 6379.confvi 6379.conf 123daemonize yesbind 0.0.0.0dir /var/redis/6379 1234chmod 777 redis_6379ps -ef|grep redisvi redis_6379 #开机启动 chkconfig redis_6379 on./redis_6379 start docker123456789101112131415yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engineyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install -y docker-cesystemctl start dockerdocker version RabbitMQ123docker search rabbitmq:managementdocker pull rabbitmq:managementdocker run -d --name rabbitmq --publish 5671:5671 --publish 5672:5672 --publish 4369:4369 --publish 25672:25672 --publish 15671:15671 --publish 15672:15672 rabbitmq:management 12http://192.168.10.104:15672/guest guest Redis12docker pull redisdocker run -di --name=redis -p 6379:6379 redis zipkin12docker pull openzipkin/zipkindocker run -d -p 9411:9411 openzipkin/zipkin zookeeper12docker pull zookeeperdocker run --privileged=true -d --name zookeeper --publish 2181:2181 -d zookeeper:latest 安装Perl12345yum install -y gcctar -zxvf perl-5.16.1.tar.gzmv perl-5.16.1 perl./Configure -des -Dprefix=/usr/local/perlmake &amp;&amp; make install 配置ssh免密码通信各自主机 12ssh-keygen -t rsacp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys IP:192.168.10.51的主机 1ssh-copy-id -i 192.168.10.52 IP:192.168.10.52的主机 1ssh-copy-id -i 192.168.10.51 安装Nginx12345678910111213141516171819202122mkdir -p /usr/servers/distribution_nginxcd /usr/servers/distribution_nginxyum install -y readline-devel pcre-devel openssl-devel gcctar -xzvf ngx_openresty-1.7.7.2.tar.gzcd ngx_openresty-1.7.7.2cd bundle/LuaJIT-2.1-20150120make clean &amp;&amp; make &amp;&amp; make installln -sf luajit-2.1.0-alpha /usr/local/bin/luajitcd ..wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gztar -xvf 2.3.tar.gzwget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gztar -xvf v0.3.0.tar.gzcd .../configure --prefix=/usr/servers/distribution_nginx --with-http_realip_module --with-pcre --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2make &amp;&amp; make installcd ..cd nginx/sbin/./nginxcd ..cd confvi nginx.conf http{}内添加 123lua_package_path &quot;/usr/servers/distribution_nginx/lualib/?.lua;;&quot;;lua_package_cpath &quot;/usr/servers/distribution_nginx/lualib/?.so;;&quot;;include lua.conf; /usr/servers/distribution_nginx/nginx/conf 路径下 vi lua.conf 12345678server &#123; listen 80; server_name _; location /lua &#123; default_type &apos;text/html&apos;; content_by_lua &apos;ngx.say(&quot;hello world&quot;)&apos;; &#125; &#125; /usr/servers/distribution_nginx/nginx/sbin目录下 1./nginx -s reload iptables -I INPUT -p tcp –dport 80 -j ACCEPT 开放端口 访问 http://192.168.10.51/lua 安装tcl1234tar -xzvf tcl8.6.1-src.tar.gzcd unix./configuremake &amp;&amp; make install nginx安装12345678yum install -y gccyum -y install pcre-develyum install -y zlib-develtar -zxvf nginx-1.14.0.tar.gz cd nginx-1.14.0./configuremakemake install 平滑重启 kill -HUP pid tail -f file 用于监视File文件增长 vim12shitf+g到最后一行 o修改u 撤销 window本地 taskkill F IM java.exe 杀进程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载机制]]></title>
    <url>%2F2019%2F05%2F14%2FJVM%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类的加载机制就是将class文件加载到内存，对数据进行校验，转换，解析，初始化，变成可以使用的java类型 加载：根据全限定名获取二进制流，静态存储结构转化为运行时数据结构，生成class对象作各种数据的访问入口 验证：文件格式验证，元数据验证，字节码验证，符号引用验证 。确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全 准备：分配内存，设置初始值 解析：符号引号替换为直接引用，其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定 初始化：通过程序制定的主观计划去初始化类变量和其它资源 必须对类进行初始化的情况 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 被动引用不会初始化 通过子类引用父类的静态变量，不会导致子类初始化 父类会初始化 通过数组定义来引用类，不会触发此类的初始化。该过程对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化 static final 不会初始化 static会初始化 任何一个类 ，都需要有加载它的类加载器和这个类本身确立在jvm中的唯一性，来源于同一个class，但加载它们的类加载器不同，那么这两个类必定不相等 ‘类加载器分类- 启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 &lt;JRE_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。- 扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。- 应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。双亲委派模型（Pattern Delegation Model）,要求除了顶层的启动类加载器外，其余的类加载器都应该有自己的父类加载器。子类通过组合关系来复用父加载器的代码1private final ClassLoader parent; 12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // 判断class是否已经加载过 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; // 没有加载过 long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; // 存在父类加载器，看父类加载器是否在家加载 c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 工作过程： 如果一个类加载器收到了类加载的请求，先把这个请求委派给父类加载器去完成（所以每个类加载都会经过最顶层的启动类加载器，比如 java.lang.Object这样的类在各个类加载器下都是同一个类），只有当父类加载器无法完成类加载请求时才尝试加载，使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一。如果没有双亲委派模型，由各个类加载器自行加载的话。当用户自己编写了一个 java.lang.Object类，那样系统中就会出现多个 Object，这样 Java 程序中最基本的行为都无法保证，程序会变的非常混乱。 使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是java类随着它的类加载器一起具备了一种带有优先级的层次关系 可以自定义一个ClassLoader，重写loadClass,findClass方法来破坏双亲委派机制 对象创建 ①类加载检查： 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 ②分配内存： 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在Eden区分配一块儿内存，JVM在给线程中的对象分配内存时，首先在TLAB分配，当对象大于TLAB中的剩余内存或TLAB的内存已用尽时，再采用上述的CAS进行内存分配 ③初始化零值： 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 ④设置对象头： 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 ⑤执行 init 方法： 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>ClassLoader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收]]></title>
    <url>%2F2019%2F05%2F14%2FJVM%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[判定对象是否可被回收都与引用有关，在Java语言中，除了基本数据类型外，其他的都是指向各类对象的对象引用；Java中根据其生命周期的长短，将引用分为4类。 1．强引用 Object obj = new Object(); 如果一个对象具有强引用，当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2．软引用（SoftReference） 如果一个对象只具有软引用，如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。 3．弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可物的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 Tomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能 4．虚引用（PhantomReference） “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃 圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是 否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 判断是否可回收引用计数算法 给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收，JVM不使用 可达性算法 通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。 在 Java 中 GC Roots 一般包含以下内容： 虚拟机栈中局部变量表中引用的对象 本级方法栈(Native方法)引用的对象 方法区中类静态变量引用的对象 方法区中的常量引用的对象 方法区回收 主要是对常量池的回收和对类的卸载。在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载功能，以保证不会出现内存溢出。类的卸载条件很多，需要满足以下三个条件，并且满足了也不一定会被卸载： 该类所有的实例都已经被回收，也就是堆中不存在该类的任何实例 加载该类的 ClassLoader 已经被回收 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法 垃圾收集算法标记 - 清除标记存活的对象，将未被标记的对象清除 缺点： 标记和清除过程效率都不高 会产生大量不连续的内存碎片，导致无法给大对象分配内存 复制算法将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理 JVM新生代中是这么做的：新生代分为一块较大的 Eden区和两块较小的Survivor区，每次使用Eden区和其中一块Survivor区。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor 标记 - 整理将存活的对象集中起来，使其内存连续，将边界以外的内存清除 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 垃圾收集器 SerialClient 模式下的默认新生代收集器，单线程的收集器，优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。缺点是回收时会将正在执行的线程暂停。适用于单CPU、新生代空间较小及对暂停时间要求不是非常高的应用上 Parallel Scavenge吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间）-XX:MaxGCPauseMillis=n 控制最大垃圾收集停顿时间-XX:GCTimeRatio=n 设置吞吐量大小的垃圾收集时间占总时间的比率，设置为19 最大gc时间就占总的1/20-XX:UseAdaptiveSizePolicy GC Ergonomics 动态调整java堆中各个区域的大小和年龄 多线程收集器。其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。 缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 ParNew-XX:ParallelGCThreads Server 模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作 默认开启的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数 在整个扫描和复制过程采用多线程的方式来进行，适用于多CPU、对暂停时间要求较短的应用上，用-XX:+UseParallelGC来强制指定，用-XX:ParallelGCThreads=4来指定线程数 Serial OldCMS备用预案 Concurrent Mode Failusre时使用标记-整理算法 Serial收集器的老年代版本，它同样使用一个单线程执行收集，基于标记整理法，作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用 Parallel Old标记-整理算法 Parallel Scavenge收集器的老年代版本，使用多线程和标记整理法，在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器，可以用在注重吞吐量和CPU资源敏感的场合,UsePallelOldGC打开 CMS标记-清除算法减少回收停顿时间碎片 -XX:CMSInitiatingOccupancyFraction 被使用多少后触发垃圾收集，提高cms触发百分比Concurrent Mode Failure 启用Serial Old 123-XX:+UseCMSCompactAtFullCollection fullgc时开启内存碎片合并整理-XX:CMSFullGCsBeforeCompaction 执行多少次不压缩FullGC后 来一次带压缩的 0 表示每次都压-XX:+UseConcMarkSweep Concurrent Mark Sweep，基于标记清除法。目标是解决Serial GC 的停顿问题，以达到最短回收时间，有高并发、高响应的特点 初始标记仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿 并发标记(CMS concurrenr mark) 进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿 重新标记(CMS remark) 为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿 并发清除(CMS concurrent sweep)不需要停顿 缺点： 产生大量碎片，大内存找不到连续的空间 会full gc 通过-XX:CMSFullGCBeforeCompaction参数设置执行多少次不压缩的Full GC之后，跟着来一次碎片整理，默认为0，即每次Full GC都对老生代进行碎片整理压缩。Full GC 不同于 老生代75%时触发的CMS GC，只在老生代达到100%，堆外内存满，老生代碎片过大无法分配空间给新晋升的大对象这些特殊情况里发生，所以设为每次都进行碎片整理是合适的 无法清除浮动垃圾 在默认设置下，CMS收集器在老年代使用了68%的空间时就会被激活，也可以通过参数-XX:CMSInitiatingOccupancyFraction的值来提供触发百分比。可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 占用cpu资源 导致程序变慢，吞吐量下降 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高 G1-XX:+UseG1GC 使用G1垃圾收集器 面向Server的垃圾收集器，相比CMS有不少改进，在多 CPU 和大内存的场景下有很好的性能。G1 可以直接对新生代和老年代一起回收 。优点： 整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片 能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒 通过引入 Region （区域）的概念，将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。通过记录每个 Region 垃圾回收时间以及回收所获得的空间，并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 收集器 串行/并行/并发 新生代/老年代 算法 优先目标 适用场景 Serial 串行 新生代 复制算法 响应速度 单CPU的Client模式 Serial Old 串行 老年代 标记-整理 响应速度 单CPU的Client模式、CMS的后备预案 ParNew 并行 新生代 复制算法 响应速度 多CPU时在Server模式下与CMS配合 Parallel Scavenge 并行 新生代 复制算法 吞吐量 在后台运算而不需要太多交互的任务 Parallel Old 并行 老年代 标记-整理 吞吐量 在后台运算而不需要太多交互的任务 CMS 并发 老年代 标记-清除 响应速度 集中在互联网站或B/S系统服务端上的Java应用 G1 并发 新生代和老年代 标记-整理+复制算法 响应速度 面向服务端应用，将来替换CMS 串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序 并行指的是垃圾收集器和用户程序同时执行 4 垃圾收集器如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非了挑选出一个最好的收集器。因为知道现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的HotSpot虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial收集器Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。 4.2 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。 4.3 Parallel Scavenge收集器Parallel Scavenge 收集器类似于ParNew 收集器。 那么它有什么特别之处呢？ 1234567-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行-XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行 Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 4.4.Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 4.5 Parallel Old收集器 Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 4.6 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 ； 并发标记： 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对CPU资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 4.7 G1收集器G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征. 被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。 G1收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 内存分配策略 对象优先在eden区分配内存，eden区空间不够时出发minor gc 大对象直接进入老年代 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制 长期存活的进入老年代在新生代多次gc存活下来的进入老年代 -XX:MaxTenuringThreshold 用来定义经过多少次minor gc还存活后进入老年代 动态对象年龄判定当 Survivor 中相同年龄所有对象大小的总和&gt; Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需经过 MaxTenuringThreshold 次gc 空间分配担保在 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那么就要进行一次 Full GC TLAB thread local allcationnnotion buffer 在eden区 每隔线程都有自己的 TLAB thread local allaction buffer 触发条件minor gc当 eden 空间满时，就将触发一次 minor gc，清理eden区 major gc是清理老年代 full gc清理整个堆空间—包括年轻代和老年代 空间分配担保失败触发full gc minor gc之前检查 老年代最大可用连续空间是否&gt;新生代所有对象总空间 调用System.gc时，系统建议执行Full GC，但是不必然执行 老年代空间不足 大对象直接进入老年代，长期存活的对象进入老年代，老年代没有足够大小的连续内存空间，触发full gc 方法区空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 如何查看当前的垃圾回收器​ -XX:+PrintFlagsFinal​ -XX:+PrintCommandLineFlags​ server client​ MBean GC日志​ 1.输出日志​ -XX:+PrintGCTimeStamps​ -XX:+PrintGCDetails​ -Xloggc:/home/administrator/james/gc.log​ -XX:+PrintHeapAtGC​ 2.日志文件控制​ -XX:-UseGCLogFileRotation​ -XX:GCLogFileSize=8K​ 3.怎么看 JDK自带的 监控工具https://docs.oracle.com/javase/8/docs/technotes/tools/windows/toc.html​ jmap -heap pid 堆使用情况​ jstat -gcutil pid 1000​ jstack 线程dump​ jvisualvm​ jconsole MAT​ http://help.eclipse.org/oxygen/index.jsp?topic=/org.eclipse.mat.ui.help/welcome.html​ -XX:+HeapDumpOnOutOfMemoryError​ -XX:HeapDumpPath=/home/administrator/james/error.hprof 怀疑：​ 1.看GC日志 126719K-&gt;126719K(126720K)​ 2.dump​ 3.MAT​ 1.占用Retained Heap​ 2.看有没有GC Root指向 什么条件触发STW的Full GC呢？Perm空间不足；CMS GC时出现promotion failed和concurrent mode failure（concurrent mode failure发生的原因一般是CMS正在进行，但是由于老年代空间不足，需要尽快回收老年代里面的不再被使用的对象，这时停止所有的线程，同时终止CMS，直接进行Serial Old GC）；（promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc ） 统计得到的Young GC晋升到老年代的平均大小大于老年代的剩余空间； 主动触发Full GC（执行jmap -histo:live [pid]）来避免碎片问题。​​java -Xms8m -Xmx64m -verbose:gc -Xloggc:/home/administrator/james/gc.log -XX:+PrintHeapAtGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9004 -Djava.rmi.server.hostname=177.1.1.122 -jar jvm-demo1-0.0.1-SNAPSHOT.jar &gt; catalina.out 2&gt;&amp;1 &amp; java -Xms128m -Xmx128m -verbose:gc -Xloggc:/home/administrator/james/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/administrator/james/error.hprof -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9004 -Djava.rmi.server.hostname=177.1.1.122 -jar jvm-demo1-0.0.1-SNAPSHOT.jar &gt; catalina.out 2&gt;&amp;1 &amp; java -Xms128m -Xmx128m -verbose:gc -Xloggc:/home/administrator/james/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+PrintHeapAtGC -XX:HeapDumpPath=/home/administrator/james/error.hprof -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+PrintCommandLineFlags -XX:+PrintFlagsFinal -XX:+PrintGCDetails -XX:+UseCMSCompactAtFullCollection -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9004 -Djava.rmi.server.hostname=177.1.1.122 -jar jvm-demo1-0.0.1-SNAPSHOT.jar &gt; catalina.out 2&gt;&amp;1 &amp; -XX:+CMSScavengeBeforeRemark]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM组成]]></title>
    <url>%2F2019%2F05%2F14%2FJVM%2FJVM%E7%BB%84%E6%88%90%2F</url>
    <content type="text"><![CDATA[方法区和堆内存是线程共享的。程序计数器、虚拟机栈、本地方法栈是线程私有的 方法区 存放已经被JVM加载的类的信息，如常量，静态变量、即时编译器编译后的代码等。 从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中，1.8 metaspace 放类加载信息 堆内存 所有对象的创建都在这里进行分配，采取分代管理，分为新生代和老年代，执行不同的垃圾回收策略，所有实例域，静态域和数组元素都是放在堆内存中。常量池在堆中 程序计数器 指向当前线程执行的字节码行号，多线程切换时可以知道上一次运行的状态和位置 虚拟机栈 由一个个栈帧组成，每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、动态链接和方法返回地址等信息，每一个方法从调用到执行完成，都对应着栈帧在虚拟机里面从入栈到出栈的过程 虚拟机编译器在重载时是通过参数的静态类型而不是实际类型作为判断依据，静态类型时编译器克制的 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常 本地方法栈 调用Native Method 直接内存 在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存（Native 堆），然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据 metaspace 为了jvm的融合 永久带大小不确定 小了容易oom metespace使用的是本地内存]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM性能监控和故障处理工具]]></title>
    <url>%2F2019%2F05%2F14%2FJVM%2FJVM%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%92%8C%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[jps查看虚拟机进程状态 -l 主类的全名 -v 数据jvm参数 jstat看虚拟机状态 jstat -gc 6424 jstat -gcutil 6424 jinfo 看jvm的参数 jinfo -flag CMSInitiatingOccupancyFraction 5192 jmap -dump:format=b,file=D:\DUMP.bin 5192 生成堆转储快照 jmap -heap 5192 堆详情信息 jhat D:\DUMP.bin 分析dump jstack 5192 打印堆栈信息 jconsole、visualVM可视化工具]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM参数]]></title>
    <url>%2F2019%2F05%2F14%2FJVM%2FJVM%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Java HotSpot VM Options文档官网的这份文档有点老了不知道新的在哪儿，可以做参考 内存和GC-XX:MaxDirectMemorySize 堆外内存的最大值 -XX:MetaspaceSize=128m Metaspace初始大小 第一次扩张会造成JVM停顿，spring aop后类比较多 -XX:MaxMetaspaceSize=512m Metaspace最大大小 设一个更大的Max值以求保险，防止将内存用光 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly 内存达到75%主动CMS GC -XX:+ExplicitGCInvokesConcurrent 让full gc时使用CMS算法，不是全程停顿 性能 -XX:AutoBoxCacheMax=20000 加大Integer Cache 自动装箱时缓冲中有的直接从缓存拿 运维-Xloggc:/dev/shm/gc-myapp.log -XX:+PrintGCDateStamps -XX:+PrintGCDetails 打印gc日志 -XX:+PrintCommandLineFlags 将每次启动的参数输出到stdout -XX:-OmitStackTraceInFastThrow 输出完整栈的日志 -XX:ErrorFile =${LOGDIR}/hs_err_%p.log JVM crash时，hotspot 会生成一个error文件，提供JVM状态信息的细节。如前所述，将其输出到固定目录，避免到时会到处找这文件。文件名中的%p会被自动替换为应用的PID -XX:+HeapDumpOnOutOfMemoryError 在OOM时，输出一个dump.core文件，记录当时的堆内存快照 对内存快照目录设置-XX:HeapDumpPath=${LOGDIR}/ -XX:+PrintGCApplicationStoppedTime 打印清晰的完整的GC停顿时间外，还可以打印其他的JVM停顿时间，比如取消偏向锁，class 被agent redefine，code deoptimization等等 -XX:+PrintPromotionFailure 多大的新生代对象晋升到老生代失败从而引发Full GC的 -XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1 -XX:+UnlockDiagnosticVMOptions -XX:-DisplayVMOutput -XX:+LogVMOutput -XX:LogFile=/dev/shm/vm-myapp.log 开启安全点日志 Option and Default Value Description -Xms 初始堆大小 -Xmx 最大堆大小 -Xss 线程占用栈空间大小，默认1m（以前是256k），可以适当调小，节约空间开启更多的线程 -Xmn 新生代的大小 JDK默认新生代占堆大小的1/3，调大可以让对象尽量在新生代被回收掉，不进入老年代 Behavioral Options Option and Default Value Description -XX:-AllowUserSignalHandlers Do not complain if the application installs signal handlers. (Relevant to Solaris and Linux only.) -XX:AltStackSize=16384 Alternate signal stack size (in Kbytes). (Relevant to Solaris only, removed from 5.0.) -XX:-DisableExplicitGC By default calls to System.gc() are enabled (-XX:-DisableExplicitGC). Use -XX:+DisableExplicitGC to disable calls to System.gc(). Note that the JVM still performs garbage collection when necessary. -XX:+FailOverToOldVerifier Fail over to old verifier when the new type checker fails. (Introduced in 6.) -XX:+HandlePromotionFailure The youngest generation collection does not require a guarantee of full promotion of all live objects. (Introduced in 1.4.2 update 11) [5.0 and earlier: false.] -XX:+MaxFDLimit Bump the number of file descriptors to max. (Relevant to Solaris only.) -XX:PreBlockSpin=10 Spin count variable for use with -XX:+UseSpinning. Controls the maximum spin iterations allowed before entering operating system thread synchronization code. (Introduced in 1.4.2.) -XX:-RelaxAccessControlCheck Relax the access control checks in the verifier. (Introduced in 6.) -XX:+ScavengeBeforeFullGC Do young generation GC prior to a full GC. (Introduced in 1.4.1.) -XX:+UseAltSigs Use alternate signals instead of SIGUSR1 and SIGUSR2 for VM internal signals. (Introduced in 1.3.1 update 9, 1.4.1. Relevant to Solaris only.) -XX:+UseBoundThreads Bind user level threads to kernel threads. (Relevant to Solaris only.) -XX:-UseConcMarkSweepGC Use concurrent mark-sweep collection for the old generation. (Introduced in 1.4.1) -XX:+UseGCOverheadLimit Use a policy that limits the proportion of the VM’s time that is spent in GC before an OutOfMemory error is thrown. (Introduced in 6.) -XX:+UseLWPSynchronization Use LWP-based instead of thread based synchronization. (Introduced in 1.4.0. Relevant to Solaris only.) -XX:-UseParallelGC Use parallel garbage collection for scavenges. (Introduced in 1.4.1) -XX:-UseParallelOldGC Use parallel garbage collection for the full collections. Enabling this option automatically sets -XX:+UseParallelGC. (Introduced in 5.0 update 6.) -XX:-UseSerialGC Use serial garbage collection. (Introduced in 5.0.) -XX:-UseSpinning Enable naive spinning on Java monitor before entering operating system thread synchronizaton code. (Relevant to 1.4.2 and 5.0 only.) [1.4.2, multi-processor Windows platforms: true] -XX:+UseTLAB Use thread-local object allocation (Introduced in 1.4.0, known as UseTLE prior to that.) [1.4.2 and earlier, x86 or with -client: false] -XX:+UseSplitVerifier Use the new type checker with StackMapTable attributes. (Introduced in 5.0.)[5.0: false] -XX:+UseThreadPriorities Use native thread priorities. -XX:+UseVMInterruptibleIO Thread interrupt before or with EINTR for I/O operations results in OS_INTRPT. (Introduced in 6. Relevant to Solaris only.) Garbage First (G1) Garbage Collection Options Option and Default Value Description -XX:+UseG1GC Use the Garbage First (G1) Collector -XX:MaxGCPauseMillis=n Sets a target for the maximum GC pause time. This is a soft goal, and the JVM will make its best effort to achieve it. -XX:InitiatingHeapOccupancyPercent=n Percentage of the (entire) heap occupancy to start a concurrent GC cycle. It is used by GCs that trigger a concurrent GC cycle based on the occupancy of the entire heap, not just one of the generations (e.g., G1). A value of 0 denotes ‘do constant GC cycles’. The default value is 45. -XX:NewRatio=n 老年代/新生代. 默认2. -XX:SurvivorRatio=n eden/survivor值. 默认8. -XX:MaxTenuringThreshold=n 对象在Survivor区最多熬过多少次Young GC后晋升到年老代，调大让对象在新生代多存活几次，默认15 -XX:ParallelGCThreads=n Sets the number of threads used during parallel phases of the garbage collectors. The default value varies with the platform on which the JVM is running. -XX:ConcGCThreads=n Number of threads concurrent garbage collectors will use. The default value varies with the platform on which the JVM is running. -XX:G1ReservePercent=n Sets the amount of heap that is reserved as a false ceiling to reduce the possibility of promotion failure. The default value is 10. -XX:G1HeapRegionSize=n With G1 the Java heap is subdivided into uniformly sized regions. This sets the size of the individual sub-divisions. The default value of this parameter is determined ergonomically based upon heap size. The minimum value is 1Mb and the maximum value is 32Mb. Performance Options Option and Default Value Description -XX:+AggressiveOpts Turn on point performance compiler optimizations that are expected to be default in upcoming releases. (Introduced in 5.0 update 6.) -XX:CompileThreshold=10000 Number of method invocations/branches before compiling [-client: 1,500] -XX:LargePageSizeInBytes=4m Sets the large page size used for the Java heap. (Introduced in 1.4.0 update 1.) [amd64: 2m.] -XX:MaxHeapFreeRatio=70 GC后，如果发现空闲堆内存大于70%时，则收缩堆内存的最大值 -XX:MaxNewSize=size Maximum size of new generation (in bytes). Since 1.4, MaxNewSize is computed as a function of NewRatio. [1.3.1 Sparc: 32m; 1.3.1 x86: 2.5m.] -XX:MinHeapFreeRatio=40 GC后，如果发现空闲堆内存小于40%时，则放大堆内存的最大值，但不超过固定最大值 -XX:NewRatio=2 Ratio of old/new generation sizes. [Sparc -client: 8; x86 -server: 8; x86 -client: 12.]-client: 4 (1.3) 8 (1.3.1+), x86: 12] -XX:NewSize=2m Default size of new generation (in bytes) [5.0 and newer: 64 bit VMs are scaled 30% larger; x86: 1m; x86, 5.0 and older: 640k] -XX:ReservedCodeCacheSize=32m Reserved code cache size (in bytes) - maximum code cache size. [Solaris 64-bit, amd64, and -server x86: 2048m; in 1.5.0_06 and earlier, Solaris 64-bit and amd64: 1024m.] -XX:SurvivorRatio=8 Ratio of eden/survivor space size [Solaris amd64: 6; Sparc in 1.3.1: 25; other Solaris platforms in 5.0 and earlier: 32] -XX:TargetSurvivorRatio=50 Desired percentage of survivor space used after scavenge. -XX:ThreadStackSize=512 Thread Stack Size (in Kbytes). (0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.] -XX:-UseBiasedLocking 取消偏向锁，大量多线程并发，锁会从偏向所升级，取消反而有性能提升 -XX:+UseFastAccessorMethods Use optimized versions of GetField. -XX:-UseISM Use Intimate Shared Memory. [Not accepted for non-Solaris platforms.] For details, see Intimate Shared Memory. -XX:+UseLargePages Use large page memory. (Introduced in 5.0 update 5.) For details, see Java Support for Large Memory Pages. -XX:+UseMPSS Use Multiple Page Size Support w/4mb pages for the heap. Do not use with ISM as this replaces the need for ISM. (Introduced in 1.4.0 update 1, Relevant to Solaris 9 and newer.) [1.4.1 and earlier: false] -XX:+UseStringCache Enables caching of commonly allocated strings. -XX:AllocatePrefetchLines=1 Number of cache lines to load after the last object allocation using prefetch instructions generated in JIT compiled code. Default values are 1 if the last allocated object was an instance and 3 if it was an array. -XX:AllocatePrefetchStyle=1 Generated code style for prefetch instructions. 0 - no prefetch instructions are generated, 1 - execute prefetch instructions after each allocation, 2 - use TLAB allocation watermark pointer to gate when prefetch instructions are executed. -XX:+UseCompressedStrings Use a byte[] for Strings which can be represented as pure ASCII. (Introduced in Java 6 Update 21 Performance Release) -XX:+OptimizeStringConcat Optimize String concatenation operations where possible. (Introduced in Java 6 Update 20) Debugging Options Option and Default Value Description -XX:-CITime Prints time spent in JIT Compiler. (Introduced in 1.4.0.) -XX:ErrorFile=./hs_err_pid.log If an error occurs, save the error data to this file. (Introduced in 6.) -XX:-ExtendedDTraceProbes Enable performance-impacting dtrace probes. (Introduced in 6. Relevant to Solaris only.) -XX:HeapDumpPath=./java_pid.hprof 设置内存快照目录 -XX:+HeapDumpOnOutOfMemoryError 在OOM时，输出一个dump.core文件，记录当时堆内存快照 -XX:OnError=”;“ Run user-defined commands on fatal error. (Introduced in 1.4.2 update 9.) -XX:OnOutOfMemoryError=”; “ Run user-defined commands when an OutOfMemoryError is first thrown. (Introduced in 1.4.2 update 12, 6) -XX:-PrintClassHistogram Print a histogram of class instances on Ctrl-Break. Manageable. (Introduced in 1.4.2.) The jmap -histocommand provides equivalent functionality. -XX:-PrintConcurrentLocks Print java.util.concurrent locks in Ctrl-Break thread dump. Manageable. (Introduced in 6.) The jstack -lcommand provides equivalent functionality. -XX:-PrintCommandLineFlags Print flags that appeared on the command line. (Introduced in 5.0.) -XX:-PrintCompilation Print message when a method is compiled. -XX:-PrintGC Print messages at garbage collection. Manageable. -XX:-PrintGCDetails Print more details at garbage collection. Manageable. (Introduced in 1.4.0.) -XX:-PrintGCTimeStamps Print timestamps at garbage collection. Manageable(Introduced in 1.4.0.) -XX:-PrintTenuringDistribution 查看survivor区对象大部分多少次进老年代 -XX:-PrintAdaptiveSizePolicy Enables printing of information about adaptive generation sizing. -XX:-TraceClassLoading Trace loading of classes. -XX:-TraceClassLoadingPreorder Trace all classes loaded in order referenced (not loaded). (Introduced in 1.4.2.) -XX:-TraceClassResolution Trace constant pool resolutions. (Introduced in 1.4.2.) -XX:-TraceClassUnloading Trace unloading of classes. -XX:-TraceLoaderConstraints Trace recording of loader constraints. (Introduced in 6.) -XX:+PerfDataSaveToFile Saves jvmstat binary data on exit. -XX:ParallelGCThreads=n Sets the number of garbage collection threads in the young and old parallel garbage collectors. The default value varies with the platform on which the JVM is running. -XX:+UseCompressedOops Enables the use of compressed pointers (object references represented as 32 bit offsets instead of 64-bit pointers) for optimized 64-bit performance with Java heap sizes less than 32gb. -XX:+AlwaysPreTouch 为了避免多次内存分配的开销，让HotSpot VM在commit内存时跑个循环来强制保证申请的内存真的commit了 -XX:AllocatePrefetchDistance=n Sets the prefetch distance for object allocation. Memory about to be written with the value of new objects is prefetched into cache at this distance (in bytes) beyond the address of the last allocated object. Each Java thread has its own allocation point. The default value varies with the platform on which the JVM is running. -XX:InlineSmallCode=n Inline a previously compiled method only if its generated native code size is less than this. The default value varies with the platform on which the JVM is running. -XX:MaxInlineSize=35 Maximum bytecode size of a method to be inlined. -XX:FreqInlineSize=n Maximum bytecode size of a frequently executed method to be inlined. The default value varies with the platform on which the JVM is running. -XX:LoopUnrollLimit=n Unroll loop bodies with server compiler intermediate representation node count less than this value. The limit used by the server compiler is a function of this value, not the actual value. The default value varies with the platform on which the JVM is running. -XX:InitialTenuringThreshold=7 Sets the initial tenuring threshold for use in adaptive GC sizing in the parallel young collector. The tenuring threshold is the number of times an object survives a young collection before being promoted to the old, or tenured, generation. -XX:MaxTenuringThreshold=n Sets the maximum tenuring threshold for use in adaptive GC sizing. The current largest value is 15. The default value is 15 for the parallel collector and is 4 for CMS. -Xloggc: Log GC verbose output to specified file. The verbose output is controlled by the normal verbose GC flags. -XX:-UseGCLogFileRotation Enabled GC log rotation, requires -Xloggc. -XX:NumberOfGClogFiles=1 Set the number of files to use when rotating logs, must be &gt;= 1. The rotated log files will use the following naming scheme, .0, .1, …, .n-1. -XX:GCLogFileSize=8K The size of the log file at which point the log will be rotated, must be &gt;= 8K.]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC日志]]></title>
    <url>%2F2019%2F05%2F14%2FJVM%2FGC%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[垃圾收集器长时间停顿，表现在 Web 页面上可能是页面响应码 500 之类的服务器错误问题，如果是个支付过程可能会导致支付失败，将造成公司的直接经济损失。 MetaSpace内存溢出JDK8 使用 MetaSpace 来保存类加载之后的类信息，字符串常量池也被移动到 Java 堆 JDK 8 中将类信息移到到了本地堆内存(Native Heap)中，将原有的永久代移动到了本地堆中成为 MetaSpace ,如果不指定该区域的大小，JVM 将会动态的调整。 可以使用 -XX:MaxMetaspaceSize=10M 来限制最大元数据。这样当不停的创建类时将会占满该区域并出现 OOM 动态代理对象太多也会 oom动态代理生成的对象在Jvm中指向的不是同一个地址，它只是与源对象有相同的hashcode值而已 CMS (concurrent mode failure) 老年代碎片化严重，无法容纳新生代提升上来的大对象 新生代来不及回收，老年代被用完 发送这种情况，应用线程将会全部停止（相当于网站这段时间无法响应用户请求），进行压缩式垃圾收集（回退到 Serial Old 算法） 解决办法： 新生代提升过快问题：（1）如果频率太快的话，说明空间不足，首先可以尝试调大新生代空间和晋升阈值。（2）如果内存有限，可以设置 CMS 垃圾收集在老年代占比达到多少时启动来减少问题发生频率（越早启动问题发生频率越低，但是会降低吞吐量，具体得多调整几次找到平衡点），参数如下：如果没有第二个参数，会随着 JVM 动态调节 CMS 启动时间 -XX:CMSInitiatingOccupancyFraction=68 （默认是 68） -XX:+UseCMSInitiatingOccupancyOnly 老年代碎片严重问题：（1）如果频率太快或者 Full GC 后空间释放不多的话，说明空间不足，首先可以尝试调大老年代空间（2）如果内存不足，可以设置进行 n 次 CMS 后进行一次压缩式 Full GC，参数如下： -XX:+UseCMSCompactAtFullCollection：允许在 Full GC 时，启用压缩式 GC -XX:CMSFullGCBeforeCompaction=n 在进行 n 次，CMS 后，进行一次压缩的 Full GC，用以减少 CMS 产生的碎片 CMS (promotion failed)在 Minor GC 过程中，Survivor Unused 可能不足以容纳 Eden 和另一个 Survivor 中的存活对象， 那么多余的将被移到老年代， 称为过早提升（Premature Promotion）。 这会导致老年代中短期存活对象的增长， 可能会引发严重的性能问题。 再进一步， 如果老年代满了， Minor GC 后会进行 Full GC， 这将导致遍历整个堆， 称为提升失败（Promotion Failure）。 提升失败日志： 提升失败原因：Minor GC 时发现 Survivor 空间放不下，而老年代的空闲也不够 新生代提升太快 老年代碎片太多，放不下大对象提升（表现为老年代还有很多空间但是，出现了 promotion failed） 解决方法：是调整年轻代和年老代的比例，还有CMSGC的时机 ​ 两条和上面 concurrent mode failure 一样 ​ 另一条，是因为 Survivor Unused 不足，那么可以尝试调大 Survivor 来尝试下 三. 在 GC 的时候其他系统活动影响 有些时候系统活动诸如内存换入换出（vmstat）、网络活动（netstat）、I/O （iostat）在 GC 过程中发生会使 GC 时间变长。 前提是你的服务器上是有 SWAP 区域（用 top、 vmstat 等命令可以看出）用于内存的换入换出，那么操作系统可能会将 JVM 中不活跃的内存页换到 SWAP 区域用以释放内存给线程使用（这也透露出内存开始不够用了）。内存换入换出是一个开销巨大的磁盘操作，比内存访问慢好几个数量级。 看一段 GC 日志：耗时 29.47 秒 再看看此时的 vmstat 命令中 si、so 列的数值，如果数值大说明换入换出严重，这是内存不足的表现。 解决方法：减少线程，这样可以降低内存换入换出；增加内存；如果是 JVM 内存设置过大导致线程所用内存不足，则适当调低 -Xmx 和 -Xms。 五. 总结 ​ 长时间停顿问题的排查及解决首先需要一定的信息和方法论： 详细的 GC 日志 借助 Linux 平台下的 iostat、vmstat、netstat、mpstat 等命令监控系统情况 查看 GC 日志中是否出现了上述的典型内存异常问题（promotion failed, concurrent mode failure），整体来说把上述两个典型内存异常情况控制在可接受的发生频率即可，对 CMS 碎片问题来说杜绝以上问题似乎不太可能，只能靠 G1 来解决了 是不是 JVM 本身的 bug 导致的 如果程序没问题，参数调了几次还是不能解决，可能说明流量太大，需要加机器把压力分散到更多 JVM 上 gc常见错误java.lang.OutOfMemoryError: Java heap space 原因：Heap内存溢出，意味着Young和Old generation的内存不够。 解决：调整java启动参数-Xms -Xmx 来增加Heap内存。 java.lang.OutOfMemoryError: unable to create new native thread 原因：Stack空间不足以创建额外的线程，要么是创建的线程过多，要么是Stack空间确实小了。 解决：由于JVM没有提供参数设置总的stack空间大小，但可以设置单个线程栈的大小；而系统的用户空间一共是3G，除了Text/Data/BSS /MemoryMapping几个段之外，Heap和Stack空间的总量有限，是此消彼长的。因此遇到这个错误，可以通过两个途径解决：1.通过 -Xss启动参数减少单个线程栈大小，这样便能开更多线程（当然不能太小，太小会出现StackOverflowError）；2.通过-Xms -Xmx 两参数减少Heap大小，将内存让给Stack（前提是保证Heap空间够用）。 java.lang.OutOfMemoryError: Requested array size exceeds VM limit 原因：这个错误比较少见（试着new一个长度1亿的数组看看），同样是由于Heap空间不足。如果需要new一个如此之大的数组，程序逻辑多半是不合理的。 解决：修改程序逻辑吧。或者也可以通过-Xmx来增大堆内存。 在GC花费了大量时间，却仅回收了少量内存时，也会报出OutOfMemoryError ，我只遇到过一两次。当使用-XX:+UseParallelGC或-XX:+UseConcMarkSweepGC收集器时，在上述情况下会报错，在 HotSpot GC Turning文档 上有说明： The parallel(concurrent) collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. 对这个问题，一是需要进行GC turning，二是需要优化程序逻辑。 java.lang.StackOverflowError 原因：这也内存溢出错误的一种，即线程栈的溢出，要么是方法调用层次过多（比如存在无限递归调用），要么是线程栈太小。 解决：优化程序设计，减少方法调用层次；调整-Xss参数增加线程栈大小。 IOException: Too many open files 原因： 这个是由于TCP connections 的buffer 大小不够用了。 java.lang.OutOfMemoryError:Direct buffer memory 解决：调整-XX:MaxDirectMemorySize=]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F09%2F%E5%88%86%E5%B8%83%E5%BC%8F%2Fhystrix%2F</url>
    <content type="text"><![CDATA[一个大型服务不可避免的需要依赖其他服务，并且有可能需要通过网络请求依赖第三方客户端。这样就有可能因为单个依赖服务延迟而导致整个服务器上的资源被阻塞。更糟糕的是，倘若两个服务相互依赖，有一个服务对另一个服务响应延时就有可能造成雪崩效应，导致两个服务一起崩溃。 现如今微服务架构十分流行，其解决依赖隔离方案hystrix也被大家所认知。但目前还有很多服务还是停留在Spring mvc框架，无法直接使用Spring Cloud集成的hystrix方案。本文先简单介绍hystrix的基本知识，然后介绍hystrix在Spring mvc的使用，最后简单介绍下如何实现项目的hystrix信息监控。 一、简介1、为什么要使用Hystrix在复杂的分布式结构中，每个应用都可能会依赖很多其他的服务，并且这些服务都不可避免地有失效的可能。倘若没有对依赖失败进行隔离，那整个服务可能就会有被拖垮的风险。 例如，一个应用依赖了 30 个服务，并且每个服务能保证 99.99% 的可用率，下面是一些计算结果： 可用率：99.99%^30=99.7%1亿次请求*0.3%=300,000次失效换算成时间大约每个月2个小时服务不稳定。 然而，现实更加残酷，如果你没有针对整个系统做快速恢复，即使所有依赖只有 0.01% 的不可用率，累积起来每个月给系统带来的不可用时间也有数小时之多。 http://tech.lede.com/2017/06/15/rd/server/hystrix/hystrix-1.png) 当一个依赖服务有延迟，它将会阻塞整个用户请求： 在高QPS的环境下，一个依赖服务的延迟会导致整个服务器上资源都被阻塞。 应用中每一个网络请求或者间接通过客户端库发出的网络请求都是潜在的导致应用失效的原因。更严重的是，这些应用可能被其他服务依赖，由于每个服务都有诸如请求队列，线程池，或者其他系统资源等，一旦某个服务失效或者延迟增高，会导致更严重的级联失效。 hystrix被设计用来： 在通过第三方客户端访问（通常是通过网络）依赖服务出现高延迟或者失败时，为系统提供保护和控制 在分布式系统中防止级联失败 可以进行快速失败（不需要等待）和快速恢复（当依赖服务失效后又恢复正常，其对应的线程池会被清理干净，即剩下的都是未使用的线程，相对于整个 Tomcat 容器的线程池被占满需要耗费更长时间以恢复可用来说，此时系统可以快速恢复） 提供失败回退（Fallback）和优雅的服务降级机制 提供近实时的监控、报警和运维控制手段 2、Hystrix如何解决依赖隔离 将所有请求外部系统（或者叫依赖服务）的逻辑封装到 HystrixCommand 或者 HystrixObservableCommand 对象中，这些逻辑将会在独立的线程中被执行（利用了设计模式中的 Command模式） 对那些耗时超过设置的阈值的请求，Hystrix 采取自动超时的策略。该策略默认对所有 Command 都有效，当然，你也可以通过设置 Command 的配置以自定义超时时间，以使你的依赖服务在引入 Hystrix 之后能达到 99.5% 的性能 为每一个依赖服务维护一个线程池（或者信号量），当线程池占满，该依赖服务将会立即拒绝服务而不是排队等待 划分出成功、失败（抛出异常）、超时或者线程池占满四种请求依赖服务时可能出现的状态 引入『熔断器』机制，在依赖服务失效比例超过阈值时，手动或者自动地切断服务一段时间 当请求依赖服务时出现拒绝服务、超时或者短路（多个依赖服务顺序请求，前面的依赖服务请求失败，则后面的请求不会发出）时，执行该依赖服务的失败回退逻辑 近实时地提供监控和配置变更 当使用 Hystrix 包装了你的所有依赖服务的请求后，拓扑图如下 3、hystrix如何执行hystrix执行分为三种模式，分别为同步执行、异步执行、Reactive模式执行。 同步执行：若原方法返回参数非Future对象且非Observable对象则会构建该模式。使用command.execute()，阻塞，当依赖服务响应（或者抛出异常/超时）时，返回结果； 异步执行：若原方法返回参数为Future对象时构建该模式。使用command.queue()，返回Future对象，通过该对象异步得到返回结果； Reactive模式执行：若原方法返回参数为Observable对象时构建该模式。该模式又分observe()命令和toObservable()命令。observe()命令会立即发出请求，在依赖服务响应（或者抛出异常/超时）时，通过注册的 Subscriber得到返回结果。toObservable()命令只有在订阅该对象时，才会发出请求，然后在依赖服务响应（或者抛出异常/超时）时，通过注册的Subscriber得到返回结果。 在内部实现中，execute()是同步调用，内部会调用queue().get()方法。queue()内部会调用toObservable().toBlocking().toFuture()。也就是说，HystrixCommand 内部均通过一个Observable的实现来执行请求，即使这些命令本来是用来执行同步返回回应这样的简单逻辑。 构建HystrixCommand或者HystrixObservableCommand对象； 执行命令（execute()、queue()、observe()、toObservable()）； 如果请求结果缓存这个特性被启用，并且缓存命中，则缓存的回应会立即通过一个Observable对象的形式返回； 检查熔断器状态，确定请求线路是否是开路，如果请求线路是开路，Hystrix将不会执行这个命令，而是直接使用『失败回退逻辑』（即不会执行run()，直接执行getFallback()）； 如果和当前需要执行的命令相关联的线程池和请求队列（或者信号量，如果不使用线程池）满了，Hystrix 将不会执行这个命令，而是直接使用『失败回退逻辑』（即不会执行run()，直接执行getFallback()）； 执行HystrixCommand.run()或HystrixObservableCommand.construct()，如果这两个方法执行超时或者执行失败，则执行getFallback()；如果正常结束，Hystrix 在添加一些日志和监控数据采集之后，直接返回回应； Hystrix 会将请求成功，失败，被拒绝或超时信息报告给熔断器，熔断器维护一些用于统计数据用的计数器。 这些计数器产生的统计数据使得熔断器在特定的时刻，能短路某个依赖服务的后续请求，直到恢复期结束，若恢复期结束根据统计数据熔断器判定线路仍然未恢复健康，熔断器会再次关闭线路。 4、hystrix基本配置hystrix基本配置可以通过四种方式进行设置。 hystrix本身代码默认。这种是在以下三种都没有自定义的情况下使用，默认设置在hystrix-core下的HystrixCommandProperties和HystrixThreadPoolProperties 自定义默认配置。可以使用配置文件进行全局默认配置。例如：hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds 通过代码构造实例设置。 动态实例配置。根据实例的key值（commandKey或者threadPollKey）通过配置文件给特定实例进行配置。例如，一个实例的commandKey为commandTest，则为hystrix.command.commandTest.execution.isolation.thread.timeoutInMilliseconds 本文只介绍hystrix常用的command和ThreadPool配置，其余配置可以查看官网 command配置 execution.isolation.strategy：执行隔离策略. Thread是默认推荐的选择。THREAD为每次在一个线程中执行，并发请求数限制于线程池的线程数。SEMAPHORE为在调用线程中执行，并发请求数限制于semaphore信号量的值。 execution.isolation.thread.timeoutInMilliseconds：超时时间，默认1000ms。 execution.timeout.enabled：是否开启超时，默认true。 execution.isolation.thread.interruptOnTimeout：当超时的时候是否中断(interrupt) HystrixCommand.run()执行，默认：true。 fallback.enabled：是否开启fallback，默认：true。 circuitBreaker.enabled：是否开启熔断，默认true。 circuitBreaker.requestVolumeThreshold：设置一个滑动窗口内触发熔断的最少请求量，默认20。例如，如果这个值是20，一个滑动窗口内只有19个请求时，即使19个请求都失败了也不会触发熔断。 circuitBreaker.sleepWindowInMilliseconds：设置触发熔断后，拒绝请求后多长时间开始尝试再次执行。默认5000ms。 circuitBreaker.errorThresholdPercentage：设置触发熔断的错误比例。默认50，即50%。 metrics.rollingStats.timeInMilliseconds：设置滑动窗口的统计时间。熔断器使用这个时间。默认10s metrics.rollingStats.numBuckets：设置滑动统计的桶数量。默认10。metrics.rollingStats.timeInMilliseconds必须能被这个值整除。 threadPool配置 coreSize：设置线程池的core size,这是最大的并发执行数量。默认10。 maximumSize：设置线程池数量极大值，这是可以支持的最大并发量，一般情况下和coreSize是相等的。默认10。该值只有在allowMaximumSizeToDivergeFromCoreSize被设置时才能有效。 maxQueueSize：最大队列长度。设置BlockingQueue的最大长度。默认-1。 如果设置成-1，就会使用SynchronizeQueue。 如果其他正整数就会使用LinkedBlockingQueue。 queueSizeRejectionThreshold：设置拒绝请求的临界值。只有maxQueueSize为-1时才有效。设置设个值的原因是maxQueueSize值运行时不能改变，我们可以通过修改这个变量动态修改允许排队的长度。默认5。（注意：hystrix为每一个依赖服务维护一个线程池或者信号量，当线程池占满+queueSizeRejectionThreshold占满，该依赖服务将会立即拒绝服务而不是排队等待） keepAliveTimeMinutes：设置keep-live时间。默认1分钟。当coreSize==maximumSize时线程池是固定的。只有allowMaximumSizeToDivergeFromCoreSize值设置为true，coreSize和maximumSize才能分成两个部分。当coreSize &lt; maximumSize，该值控制一个线程多久没使用才被释放。 allowMaximumSizeToDivergeFromCoreSize：该值确认maximumSize是否起作用。默认false。 metrics.rollingStats.timeInMilliseconds：和command配置含义一样。 metrics.rollingStats.numBuckets：和command配置含义一样。 倘若使用配置文件进行配置，两种配置可以根据key的字符串进行区分。command都是hystrix.command.commandKey(or default).属性名，threadpool都是hystrix.threadpool.threadpoolKey(groupKey or default).属性名。 二、hystrix在spring mvc的使用hystrix在Spring cloud的使用非常简单，网上也有很多文档，在此就不多讲了。 为使熔断控制和现有代码解耦，hystrix官方采用了Aspect方式。现在介绍hystrix在spring mvc的使用。 1、添加依赖使用maven引入hystrix依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-javanica&lt;/artifactId&gt; &lt;version&gt;1.5.12&lt;/version&gt;&lt;/dependency&gt; 2、添加配置新建hystrix.properties文件（名字随意定，里面将定义项目所有hystrix配置信息） 新建一个类HystrixConfig 12345678910111213141516171819public class HystrixConfig&#123; public void init() &#123; Properties prop = new Properties(); InputStream in = null; try &#123; in = HystrixConfig.class.getClassLoader().getResourceAsStream(&quot;hystrix.properties&quot;); prop.load(in); in.close(); System.setProperties(prop); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在spring的配置文件添加内容： 1234&lt;!-- 添加了就不用加了 --&gt;&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot; /&gt;&lt;bean name=&quot;hystrixCommandAspect&quot; class=&quot;com.netflix.hystrix.contrib.javanica.aop.aspectj.HystrixCommandAspect&quot;/&gt;&lt;bean id=&quot;hystrixConfig&quot; class=&quot;包名.HystrixConfig&quot; init-method=&quot;init&quot;/&gt; 新建hystrixConfig bean主要是因为使用spring自带的context:property-placeholder配置加载器，hystrix无法读取。目前我只想到了通过System.setProperties的方式，若有其他方式欢迎指导。 3、hystrixCommand使用举个简单的例子(写成接口方式是方便测试，普通的方法效果是一样的)： 12345678910111213141516@ResponseBody@RequestMapping(&quot;/test.html&quot;)@HystrixCommandpublic String test(int s)&#123; logger.info(&quot;test.html start,s:&#123;&#125;&quot;, s); try &#123; Thread.sleep(s * 1000); &#125; catch (Exception e) &#123; logger.error(&quot;test.html error.&quot;, e); &#125; return &quot;OK&quot;;&#125; 根据例子，我们可以看到和其他方法相比就添加了个@HystrixCommand注解，方法执行后会被HystrixCommandAspect拦截，拦截后会根据方法的基本属性（所在类、方法名、返回类型等）和HystrixCommand属性生成HystrixInvokable，最后执行。例子中，因为HystrixCommand属性为空，所以其groupKey默认为类名，commandKey为方法名。 通过HystrixCommand源码来看下可以设置的属性： 1234567891011121314151617181920212223242526@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface HystrixCommand &#123; String groupKey() default &quot;&quot;; String commandKey() default &quot;&quot;; String threadPoolKey() default &quot;&quot;; String fallbackMethod() default &quot;&quot;; HystrixProperty[] commandProperties() default &#123;&#125;; HystrixProperty[] threadPoolProperties() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] ignoreExceptions() default &#123;&#125;; ObservableExecutionMode observableExecutionMode() default ObservableExecutionMode.EAGER; HystrixException[] raiseHystrixExceptions() default &#123;&#125;; String defaultFallback() default &quot;&quot;;&#125; 其中比较重要的是groupKey、commandKey、fallbackMethod（Fallback时调用的方法，一定要在同一个类中，且传参和返参要一致）。threadPoolKey一般可以不定义，线程池名会默认定义为groupKey。 再来看下HystrixCommandAspect是如何实现拦截的： 123456789101112131415161718192021222324252627282930313233343536@Pointcut(&quot;@annotation(com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand)&quot;)public void hystrixCommandAnnotationPointcut() &#123;&#125;@Pointcut(&quot;@annotation(com.netflix.hystrix.contrib.javanica.annotation.HystrixCollapser)&quot;)public void hystrixCollapserAnnotationPointcut() &#123;&#125;@Around(&quot;hystrixCommandAnnotationPointcut() || hystrixCollapserAnnotationPointcut()&quot;)public Object methodsAnnotatedWithHystrixCommand(final ProceedingJoinPoint joinPoint) throws Throwable &#123; Method method = getMethodFromTarget(joinPoint);//见步骤1 Validate.notNull(method, &quot;failed to get method from joinPoint: %s&quot;, joinPoint); if (method.isAnnotationPresent(HystrixCommand.class) &amp;&amp; method.isAnnotationPresent(HystrixCollapser.class)) &#123; throw new IllegalStateException(&quot;method cannot be annotated with HystrixCommand and HystrixCollapser &quot; + &quot;annotations at the same time&quot;); &#125; MetaHolderFactory metaHolderFactory = META_HOLDER_FACTORY_MAP.get(HystrixPointcutType.of(method));//见步骤2 MetaHolder metaHolder = metaHolderFactory.create(joinPoint);//见步骤3 HystrixInvokable invokable = HystrixCommandFactory.getInstance().create(metaHolder);//见步骤4 ExecutionType executionType = metaHolder.isCollapserAnnotationPresent() ? metaHolder.getCollapserExecutionType() : metaHolder.getExecutionType(); Object result; try &#123; if (!metaHolder.isObservable()) &#123; result = CommandExecutor.execute(invokable, executionType, metaHolder); &#125; else &#123; result = executeObservable(invokable, executionType, metaHolder);//见步骤5 &#125; &#125; catch (HystrixBadRequestException e) &#123; throw e.getCause() != null ? e.getCause() : e; &#125; catch (HystrixRuntimeException e) &#123; throw hystrixRuntimeExceptionToThrowable(metaHolder, e); &#125; return result;&#125; 步骤1：获取切入点方法； 步骤2：根据方法的注解HystrixCommand或者HystrixCollapser生成相应的CommandMetaHolderFactory或者CollapserMetaHolderFactory类。 步骤3：将原方法的属性set进metaHolder中； 步骤4：根据metaHolder生成相应的HystrixCommand，包含加载hystrix配置信息。commandProperties加载的优先级为前缀hystrix.command.commandKey &gt; hystrix.command.default &gt; defaultValue(原代码默认)；threadPool配置加载的优先级为 前缀hystrix.threadpool.groupKey.&gt; hystrix.threadpool.default.&gt; defaultValue(原代码默认). 步骤5：执行命令。 倘若需要给该方法指定groupKey和commandKey定义其fallback方法，则可通过添加注解属性来实现。如： 1234567891011121314151617181920@ResponseBody@RequestMapping(&quot;/test.html&quot;)@HystrixCommand(groupKey = &quot;groupTest&quot;, commandKey = &quot;commandTest&quot;, fallbackMethod = &quot;back&quot;)public String test(int s)&#123; try &#123; Thread.sleep(s * 1000); &#125; catch (Exception e) &#123; &#125; logger.info(&quot;test.html start&quot;); return &quot;OK&quot;;&#125;private String back(int s)&#123; return &quot;back&quot;;&#125; groupKey=”groupTest”是将该hystrix操作的组名定义为groupTest，该属性在读取threadPoolProperties时需要用到。读取的策略是先读取已groupTest为键值的配置缓存；若没有则读取已hystrix.threadpool.groupTest.为前缀的配置；若没有则读取hystrix.threadpool.为前缀的配置，最后才读取代码默认的值。 commandKey=”commandTest”是将hystrix操作的命令名定义为commandTest，该属性在读取commandProperties时需要用到。读取的策略与上面的一致，只是前缀由hystrix.threadpool变为hystrix.command。 fallbackMethod=”back”是给该hystrix操作定义一个回退方法，值为回退方法的方法名，并且要与回退方法在同一个类下、相同的参入参数和返回参数。fallbackMethod可级联。 如果要给该方法指定一些hystrix属性，可通过在hystrix.properties中添加一些配置来实现。如给上述方法添加一些hystrix属性，示例如下： 12345678#定义commandKey为commandTest的过期时间为3shystrix.command.commandTest.execution.isolation.thread.timeoutInMilliseconds=3000#定义所有的默认过期时间为5s，不再是默认是1s。优先级小于上面配置hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds=5000#定义threadPoolKey为groupTest的线程池大小为15hystrix.threadpool.groupTest.coreSize=15#定义所有的线程池大小为为5，不再是默认是10。优先级小于上面配置hystrix.threadpool.default.coreSize=5 其余的配置方式与例子中的相似，就不一一列举了。 至此，spring mvc就可以为每一个依赖随心添加依赖隔离了。 四、参考链接https://github.com/Netflix/Hystrix/tree/master/hystrix-contrib/hystrix-javanica https://github.com/Netflix/Hystrix/wiki https://github.com/Netflix/Hystrix/wiki/Dashboard https://github.com/Netflix/Turbine/wiki https://github.com/Netflix/Turbine/wiki/Configuration-(1.x)) http://youdang.github.io/categories/%E7%BF%BB%E8%AF%91/ http://www.cnblogs.com/java-zhao/p/5521233.html 1、构建一个HystrixCommand或者HystrixObservableCommand 一个HystrixCommand或一个HystrixObservableCommand对象，代表了对某个依赖服务发起的一次请求或者调用HystrixCommand主要用于仅仅会返回一个结果的调用HystrixObservableCommand主要用于可能会返回多条结果的调用 2、调用command的执行方法 执行Command就可以发起一次对依赖服务的调用 要执行Command，需要在4个方法中选择其中的一个：execute()，queue()，observe()，toObservable() 其中execute()和queue()仅仅对HystrixCommand适用 execute()：调用后直接block住，属于同步调用，直到依赖服务返回单条结果，或者抛出异常queue()：返回一个Future，属于异步调用，后面可以通过Future获取单条结果observe()：订阅一个Observable对象，Observable代表的是依赖服务返回的结果，获取到一个那个代表结果的Observable对象的拷贝对象toObservable()：返回一个Observable对象，如果我们订阅这个对象，就会执行command并且获取返回结果 execute()实际上会调用queue().get().queue()，接着会调用toObservable().toBlocking().toFuture() 也就是说，无论是哪种执行command的方式，最终都是依赖toObservable()去执行的 3、检查是否开启缓存 如果这个command开启了请求缓存，request cache，而且这个调用的结果在缓存中存在，那么直接从缓存中返回结果 4、检查是否开启了短路器 检查这个command对应的依赖服务是否开启了短路器 如果断路器被打开了，那么hystrix就不会执行这个command，而是直接去执行fallback降级机制 5、检查线程池/队列/semaphore是否已经满了 如果command对应的线程池/队列/semaphore已经满了，那么也不会执行command，而是直接去调用fallback降级机制 6、执行command 调用HystrixObservableCommand.construct()或HystrixCommand.run()来实际执行这个command HystrixCommand.run()是返回一个单条结果，或者抛出一个异常HystrixObservableCommand.construct()是返回一个Observable对象，可以获取多条结果 如果HystrixCommand.run()或HystrixObservableCommand.construct()的执行，超过了timeout时长的话，那么command所在的线程就会抛出一个TimeoutException 如果timeout了，也会去执行fallback降级机制，而且就不会管run()或construct()返回的值了 这里要注意的一点是，我们是不可能终止掉一个调用严重延迟的依赖服务的线程的，只能说给你抛出来一个TimeoutException，但是还是可能会因为严重延迟的调用线程占满整个线程池的 即使这个时候新来的流量都被限流了。。。 如果没有timeout的话，那么就会拿到一些调用依赖服务获取到的结果，然后hystrix会做一些logging记录和metric统计 7、短路健康检查 Hystrix会将每一个依赖服务的调用成功，失败，拒绝，超时，等事件，都会发送给circuit breaker断路器 短路器就会对调用成功/失败/拒绝/超时等事件的次数进行统计 短路器会根据这些统计次数来决定，是否要进行短路，如果打开了短路器，那么在一段时间内就会直接短路，然后如果在之后第一次检查发现调用成功了，就关闭断路器 8、调用fallback降级机制 在以下几种情况中，hystrix会调用fallback降级机制：run()或construct()抛出一个异常，短路器打开，线程池/队列/semaphore满了，command执行超时了 一般在降级机制中，都建议给出一些默认的返回值，比如静态的一些代码逻辑，或者从内存中的缓存中提取一些数据，尽量在这里不要再进行网络请求了 即使在降级中，一定要进行网络调用，也应该将那个调用放在一个HystrixCommand中，进行隔离 在HystrixCommand中，上线getFallback()方法，可以提供降级机制 在HystirxObservableCommand中，实现一个resumeWithFallback()方法，返回一个Observable对象，可以提供降级结果 如果fallback返回了结果，那么hystrix就会返回这个结果 对于HystrixCommand，会返回一个Observable对象，其中会发返回对应的结果对于HystrixObservableCommand，会返回一个原始的Observable对象 如果没有实现fallback，或者是fallback抛出了异常，Hystrix会返回一个Observable，但是不会返回任何数据 不同的command执行方式，其fallback为空或者异常时的返回结果不同 对于execute()，直接抛出异常对于queue()，返回一个Future，调用get()时抛出异常对于observe()，返回一个Observable对象，但是调用subscribe()方法订阅它时，理解抛出调用者的onError方法对于toObservable()，返回一个Observable对象，但是调用subscribe()方法订阅它时，理解抛出调用者的onError方法 9、不同的执行方式 execute()，获取一个Future.get()，然后拿到单个结果queue()，返回一个Futureobserver()，立即订阅Observable，然后启动8大执行步骤，返回一个拷贝的Observable，订阅时理解回调给你结果toObservable()，返回一个原始的Observable，必须手动订阅才会去执行8大步骤]]></content>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch入门]]></title>
    <url>%2F2019%2F05%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fes%2F</url>
    <content type="text"><![CDATA[Elasticsearch是一个分布式的文档系统，可以实现近实时的搜索，可以用来做全文检索，数据分析，日志检索等。基于lucene，lucene是最先进、功能最强大的搜索库，但是提供的api复杂，直接基于lucene开发比较麻烦。es提供了简单易用的restful api接口 es分布式 横向扩展机制好 可以近实时搜索 数据量的增加没有明显的变化 内存占用高 仅支持json文件格式 全文检索、数据分析以及分布式技术结合在一起 分片机制高可用 solr 支持的数据格式多 对已存在的数查询快 建立索引时搜索效率下降 数据量越大效率越低 不适合实时搜索的应用 听说github现在用es来做代码的检索代替solr 核心概念倒排索引：倒排索引的索引对象是文档集合中的单词 记录出现的文档id 在文档中出现的次数 位置等信息 倒排索引的结构 （1）包含这个关键词的document id list（2）包含这个关键词的所有document的数量：IDF（inverse document frequency）（3）这个关键词在每个document中出现的次数：TF（term frequency）（4）这个关键词在这个document中的次序（5）每个document的长度：length norm（6）包含这个关键词的所有document的平均长度 倒排索引不可变的好处 （1）不需要锁，提升并发能力，避免锁的问题（2）数据不变，一直保存在os cache中，只要cache内存足够（3）filter cache一直驻留在内存，因为数据不变（4）可以压缩，节省cpu和io开销 倒排索引不可变的坏处：每次都要重新构建整个索引 Index：索引，包含一堆居有相似结构的文档数据，相当于数据库库的概念Type：类型，相当于数据库表的概念（es6的版本不允许一个index下面有多个type）Document：文档，es中的最小数据单元，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。相当于数据库行的概念。应用系统的数据结构都是面向对象的，复杂的，对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦，ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能，es的document用json数据格式来表达shard：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个shard都是一个lucene index。replica：任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引10个shard，5个primary shard，5个replica shard 一个index包含多个shard，每个shard都承载部分数据，replica shard是primary shard的副本，负责容错，以及承担读请求负载（3）增减节点时，shard会自动在nodes中负载均衡 mapping：index的type的元数据，每个type都有一个自己的mapping，决定了数据类型，建立倒排索引的行为，还有进行搜索的行为 往es里面直接插入数据，es会自动建立索引，同时建立type以及对应的mapping，mapping中就自动定义了每个field的数据类型，不同的数据类型（比如说text和date），可能有的是exact value，有的是full text，exact value，在建立倒排索引的时候，分词的时候，是将整个值一起作为一个关键词建立到倒排索引中的；full text，会经历各种各样的处理，分词，normaliztion（时态转换，同义词转换，大小写转换），才会建立到倒排索引中，）同时呢，exact value和full text类型的field就决定了，在一个搜索过来的时候，对exact value field或者是full text field进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致；比如说exact value搜索的时候，就是直接按照整个值进行匹配，full text query string，也会进行分词和normalization再去倒排索引中去搜索，可以用es的dynamic mapping，让其自动建立mapping，包括自动设置数据类型；也可以提前手动创建index和type的mapping，自己对各个field进行设置，包括数据类型，包括索引行为，包括分词器，等等 1234567891011121314# 获取GET /shop/product/_mapping# 创建mappingPUT /索引库名/_mapping/类型名称&#123; &quot;properties&quot;: &#123; &quot;字段名&quot;: &#123; &quot;type&quot;: &quot;类型&quot;, # 可以是text、long、short、date、integer、object等text可分词 keyword不能 &quot;index&quot;: true， # 是否索引，默认为true，则可以用来进行搜索 &quot;store&quot;: true， # 是否存储，默认为false &quot;analyzer&quot;: &quot;分词器&quot; &#125; &#125;&#125; 工作原理写的工作原理 客户端发送请求到一个协调节点 协调节点对document根据doc id进行hash路由，将请求转发给对应的primary shard存在的节点 该节点上的primary shard处理请求，然后将数据同步到replica node 如果协调节点发现primary node和所有replica node都完成操作，就返回响应结果给客户端 doc id 可以手动指定，如果没有指定会自动生成doc id 读的工作原理 客户端发送请求到一个协调节点 协调节点对document根据doc id进行hash路由，将请求转发给对应的shard 用负载均衡算法从primary shard和replica shard中选择 接受请求的节点返回document给协调节点 协调节点返回document给客户端 搜索的工作原理 客户端发送请求到一个协调节点 协调节点将搜索请求转发到所有的shard(primary or replica) query :每个shard将自己的搜索结果(doc id)返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果 fetch :协调节点根据doc id去拉取document返回给客户端 （5）写数据底层原理 document写入buffer，同时写入translog日志文件，这时数据不能被搜索到 数据先写入buffer，在buffer里的时候数据是搜索不到的；同时将数据写入translog日志文件 如果buffer快满了，或者每隔一段时间（默认1秒），就会将buffer数据refresh到一个新的segment file中，但是此时数据不是直接进入segment file的磁盘文件的，而是先进入os cache的。这个过程就是refresh。数据写入磁盘文件之前，会先进入os cache，先进入操作系统级别的一个内存缓存中去。只要buffer中的数据被refresh操作，刷入os cache中，就代表这个数据就可以被搜索到了，所以是准实时的 。 默认每隔30分钟会自动执行一次commit，但是如果translog过大，也会触发commit操作。叫做flush操作，也可以手动执行 此时将对应的segment file数据从os cache fsync到磁盘 并清空translog日志文件 segment file会定期执行merge，或当segment file多到一定程度的时候也会定期执行merge，生成新的segment file 删除操作一开始是假删除 将document标记为delete 等到merge时真删除 translog:记录一段时间的写操作，机器挂掉buffer和oscache的数据就没了，重启机器es读取translog恢复这些数据，translog也是先写入os cache的，默认每隔5秒刷一次到磁盘中去，如果此时机器挂了，可能会丢失5秒钟的数据 性能优化 es其实性能并没有你想象中那么好的，数据量很大的情况下（数十亿级别）,跑个搜索怎么一下5秒~10秒，坑爹了。第一次搜索的时候，是5-10秒，后面反而就快了，可能就几百毫秒。 （1）性能优化的杀手锏——filesystem cache 写入es的数据量最好&lt;filesystem cache os cache，操作系统的缓存，如果数据都放在内存里查询性能就会非常高，所以es里只放被用用来搜索的字段，其他字段还放在其他数据库里，搜索拿到对应的doc id 再到其他库里查 （2）数据预热 内存不够，可以将热点数据留在内存中，自己搜索一下数据刷到os cache ，比如用storm统最近的热点数据 好像跟redis预热差不多 （3）es里复杂的关联查询比如join,nested,parent-child最好不要用，性能不好 （4）分页性能优化 deeping-paging 问题 搜索过深 会在协调节点，保存大量数据再排序，再取出自己需要的数据 每个节点使用一个优先级队列保存队列 最后一期merge得到ids 再mget请求拿到所有数据 ES为了避免深分页，不允许使用分页(from&amp;size)查询10000条以后的数据，因此如果要查询第10000条以后的数据，要使用ES提供的 scroll(游标) 来查询 假设取的页数较大时(深分页)，如请求第20页，Elasticsearch不得不取出所有分片上的第1页到第20页的所有文档，并做排序，最终再取出from后的size条结果作爲最终的返回值 假设你有16个分片，则需要在coordinate node彙总到 shards (from+size)条记录，即需要16(20+10)记录后做一次全局排序 所以，当索引非常非常大(千万或亿)，是无法使用from + size 做深分页的，分页越深则越容易OOM，即便不OOM，也很消耗CPU和内存资源 因此ES使用index.max_result_window:10000作爲保护措施 ，即默认 from + size 不能超过10000，虽然这个参数可以动态修改，也可以在配置文件配置，但是最好不要这麽做，应该改用ES游标来取得数据 scroll游标原理 可以把 scroll 理解爲关系型数据库里的 cursor，因此，scroll 并不适合用来做实时搜索，而更适用于后台批处理任务，比如群发 scroll 具体分爲初始化和遍历两步 初始化时将所有符合搜索条件的搜索结果缓存起来，可以想象成快照 在遍历时，从这个快照里取数据 也就是说，在初始化后对索引插入、删除、更新数据都不会影响遍历结果 游标可以增加性能的原因，是因为如果做深分页，每次搜索都必须重新排序，非常浪费，使用scroll就是一次把要用的数据都排完了，分批取出，因此比使用from+size还好 scoll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更，是不会让用户看到的 1234567891011121314GET /shop/product/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &quot;_doc&quot; ], &quot;size&quot;: 1&#125;GET /_search/scroll&#123; &quot;scroll&quot;: &quot;1m&quot;, &quot;scroll_id&quot; : &quot;上面查询返回的id&quot;&#125; 集群下的分页比如要求page=100,size=10 es的分页是较坑的，为啥呢？举个例子吧，假如你每页是10条数据，你现在要查询第100页，实际上是会把每个shard上存储的前1000条数据都查到一个协调节点上，如果你有个5个shard，那么就有5000条数据，接着协调节点对这500ye0条数据进行一些合并、处理，再获取到最终第100页的10条数据。 分布式的，你要查第100页的10条数据，你是不可能说从5个shard，每个shard就查2条数据？最后到协调节点合并成10条数据？你必须得从每个shard都查1000条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第100页的数据。 你翻页的时候，翻的越深，每个shard返回的数据就越多，而且协调节点处理的时间越长。非常坑爹。所以用es做分页的时候，翻的越深，性能就越差。所以不允许用户深度分页 我们之前也是遇到过这个问题，用es作分页，前几页就几十毫秒，翻到10页之后，几十页的时候，基本上就要5~10秒才能查出来一页数据了 如果是像下拉刷新那种可以用scroll api,scroll会一次性给你生成所有数据的一个快照，然后每次翻页就是通过游标移动，获取下一页下一页这样子，性能会比上面说的那种分页性能也高很多很多 不适用于随意跳到某一页的场景。同时这个scroll是要保留一段时间内的数据快照的 boost 设置权重 12345678910111213141516171819202122GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;boost&quot;: 2 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ] &#125; &#125;&#125; 彻底掌握IK中文分词器：彻底掌握，连源码的修改都讲过了，怎么基于mysql热加载你的词库 rest api新增文档 123456789# PUT /index/type/idPUT /shop/product/1&#123; &quot;name&quot; : &quot;candy&quot;, &quot;desc&quot; : &quot;made in china&quot;, &quot;price&quot; : 2, &quot;producer&quot; :&quot;big white rabbit&quot;, &quot;tags&quot;: [ &quot;sweet&quot;, &quot;china&quot; ]&#125; es会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索 检索文档 1GET /shop/product/1 修改文档 替换文档 12345678PUT /shop/product/1&#123; &quot;name&quot; : &quot;candy&quot;, &quot;desc&quot; : &quot;made in china&quot;, &quot;price&quot; : 4, &quot;producer&quot; :&quot;big white rabbit&quot;, &quot;tags&quot;: [ &quot;sweet&quot;, &quot;china&quot; ]&#125; 替换方式有一个不好，必须带上所有的field，才能去进行信息的修改，否则未带上的字段会消失 更新文档 123456POST /shop/product/1/_update&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;sweet candy&quot; &#125;&#125; 删除文档 1DELETE /shop/product/1 queryquery string search123GET /shop/product/_search# 搜索商品名称中包含candy的商品，而且按照价格升序排序GET /shop/product/_search?q=name:candy&amp;sort=price:asc 1GET /shop/product/_search?q=candy 任意一个field包含指定的关键字(candy)就可以搜索出来，在进行中搜索的时候，并不是对document中的每一个field都进行一次搜索。es在插入一条document，会自动将多个field的值，全部用字符串的方式串联起来，变成一个长的字符串，作为_all field的值，同时建立索引，如果在搜索的时候，没有对某个field指定搜索，就默认搜索_all field，其中是包含了所有field的值 query DSL (Domain Specified Language)可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293GET /索引库名/_search&#123; &quot;query&quot;:&#123; &quot;查询类型&quot;:&#123; #`match_all， match，term ， range 等等 &quot;查询条件&quot;:&quot;查询条件值&quot; &#125; &#125;&#125;# 查询所有的商品GET /shop/product/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;# 搜索商品名称中包含candy的商品，而且按照价格升序排序GET /shop/product/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;name&quot; : &quot;candy&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;price&quot;: &quot;asc&quot; &#125;, &#123; &quot;_score&quot;: &quot;desc&quot; &#125; ]&#125;# GET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;candy&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125;# 最小匹参数GET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&#123; &quot;query&quot;:&quot;小米曲面电视&quot;, &quot;minimum_should_match&quot;: &quot;75%&quot; &#125; &#125; &#125;&#125;# 多字段查询multi_matchGET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;小米&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;subTitle&quot; ] &#125; &#125;&#125;# 词条匹配 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些未分词的字符串GET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;price&quot;:2699.00 &#125; &#125;&#125;# 多词条精确匹配(terms)GET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;price&quot;:[2699.00,2899.00,3899.00] &#125; &#125;, &quot;_source&quot;: &#123; &quot;includes&quot;:[&quot;title&quot;,&quot;price&quot;], &quot;excludes&quot;: [&quot;images&quot;] &#125;&#125;# 分页查询 对结果进行过滤 指定要查询出来商品的名称和价格就可以GET /shop/product/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 0, &quot;size&quot;:1, &quot;_source&quot;:[&quot;name&quot;,&quot;price&quot;]&#125; 组合多个搜索条件 12345678910111213141516171819202122232425262728GET /shop/product/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;name&quot;:&quot;candy&quot; &#125; &#125; ], &quot;should&quot;:[ &#123; &quot;match&quot;:&#123; &quot;producer&quot;:&quot;china&quot; &#125; &#125; ], &quot;must_not&quot;:[ &#123; &quot;match&quot;:&#123; &quot;name&quot;:&quot;milk&quot; &#125; &#125; ] &#125; &#125;,&#125; “minimum_should_match”: 1 最小匹配度 query filter 搜索商品名称包含yagao，而且售价大于25元的商品 1234567891011121314151617181920212223242526272829303132333435363738GET /shop/product/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;name&quot;:&quot;candy&quot; &#125; &#125; ] &#125; &#125;, &quot;post_filter&quot;:&#123; &quot;range&quot;:&#123; &quot;price&quot;:&#123; &quot;gt&quot;:&quot;2&quot; &#125; &#125; &#125;&#125;GET /tvs/sales/_search &#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &quot;TCL&quot; &#125; &#125; &#125; &#125;&#125; term filter text需要建索引时指定为not_analyzed，才能用term query 相当于sql的where条件 得到分词结果 12345GET /item/_analyze&#123; &quot;field&quot;: &quot;all&quot;, &quot;text&quot;: &quot;华为 麦芒5 全网通 4GB+64GB版 手机 手机通讯 手机&quot;&#125; full-text search（全文检索）全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回，不是说单纯的只是匹配完整的一个值，而是可以对值进行拆分词语后（分词）进行匹配，也可以通过缩写、时态、大小写、同义词等进行匹配（1）缩写 vs. 全程：cn vs. china（2）格式转化：like liked likes（3）大小写：Tom vs tom（4）同义词：like vs love 12345678GET /shop/product/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;candy&quot; &#125; &#125;&#125; phrase search（短语搜索）要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回 12345678GET /shop/product/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;name&quot;: &quot;milk candy&quot; &#125; &#125;&#125; highlight search（高亮搜索）12345678910111213GET /shop/product/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;nat candy&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;name&quot;: &#123;&#125; &#125; &#125;&#125; fuzzy（误拼写模糊搜索）-自动将拼写错误的搜索文本，进行纠正，纠正以后去尝试匹配索引中的数据 12345678910111213//指定就错的次数GET /shop/product/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;cad&quot;, &quot;fuzziness&quot;: &quot;3&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 123456789101112GET /shop/product/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;cndy mil&quot;, &quot;fuzziness&quot;: &quot;AUTO&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 批量查询减少网络请求的性能开销 mget批量查询 123456789101112131415GET /_mget&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;shop&quot;, &quot;_type&quot; : &quot;product&quot;, &quot;_id&quot; : 1 &#125;, &#123; &quot;_index&quot; : &quot;seckillshop&quot;, &quot;_type&quot; : &quot;product&quot;, &quot;_id&quot; : 3 &#125; ]&#125; 如果查询的数据都在同一个index下的同一个type下 1234GET /shop/product/_mget&#123; &quot;ids&quot;:[1,2]&#125; filter与query对比 filter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序 所以是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；不希望一些搜索条件来影响你的document排序，那么就放在filter中 filter不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据. 倒排索引的结果 构成一个bitset 如果doc 有这个field就为1 否则为0 多个filter 会从稀疏的bitset开始过滤 query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果 bulk课程大纲 1、bulk语法 12&#123;&quot;action&quot;: &#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; POST /_bulk{ “delete”: { “_index”: “test_index”, “_type”: “test_type”, “_id”: “3” }}{ “create”: { “_index”: “test_index”, “_type”: “test_type”, “_id”: “12” }}{ “test_field”: “test12” }{ “index”: { “_index”: “test_index”, “_type”: “test_type”, “_id”: “2” }}{ “test_field”: “replaced test2” }{ “update”: { “_index”: “test_index”, “_type”: “test_type”, “_id”: “1”, “_retry_on_conflict” : 3} }{ “doc” : {“test_field2” : “bulk test1”} } 举例，比如你现在要创建一个文档，放bulk里面，看起来会是这样子的： {“index”: {“_index”: “test_index”, “_type”, “test_type”, “_id”: “1”}}{“test_field1”: “test1”, “test_field2”: “test2”} 有哪些类型的操作可以执行呢？（1）delete：删除一个文档，只要1个json串就可以了（2）create：PUT /index/type/id/_create，强制创建（3）index：普通的put操作，可以是创建文档，也可以是全量替换文档（4）update：执行的partial update操作 bulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志 2、bulk size最佳大小 bulk request会加载到内存里，如果太大的话，性能反而会下降，因此需要反复尝试一个最佳的bulk size。一般从1000~5000条数据开始，尝试逐渐增加。另外，如果看大小的话，最好是在5~15MB之间。 1、bulk中的每个操作都可能要转发到不同的node的shard去执行 2、如果采用比较良好的json数组格式 允许任意的换行，整个可读性非常棒，读起来很爽，es拿到那种标准格式的json串以后，要按照下述流程去进行处理 （1）将json数组解析为JSONArray对象，这个时候，整个数据，就会在内存中出现一份一模一样的拷贝，一份数据是json文本，一份数据是JSONArray对象（2）解析json数组里的每个json，对每个请求中的document进行路由（3）为路由到同一个shard上的多个请求，创建一个请求数组（4）将这个请求数组序列化（5）将序列化后的请求数组发送到对应的节点上去 3、耗费更多内存，更多的jvm gc开销 我们之前提到过bulk size最佳大小的那个问题，一般建议说在几千条那样，然后大小在10MB左右，所以说，可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB，然后每个请求的json都copy一份为jsonarray对象，此时内存中的占用就会翻倍，就会占用2GB的内存，甚至还不止。因为弄成jsonarray之后，还可能会多搞一些其他的数据结构，2GB+的内存占用。 占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，跟频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更多 4、现在的奇特格式 {“action”: {“meta”}}\n{“data”}\n{“action”: {“meta”}}\n{“data”}\n （1）不用将其转换为json对象，不会出现内存中的相同数据的拷贝，直接按照换行符切割json（2）对每两个一组的json，读取meta，进行document路由（3）直接将对应的json发送到node上去 5、最大的优势在于，不需要将json数组解析为一个JSONArray对象，形成一份大数据的拷贝，浪费内存空间，尽可能地保证性能 分析第一个分析需求：计算每个tag下的商品数量 GET /ecommerce/product/_search{ “aggs”: {​ “group_by_tags”: {​ “terms”: { “field”: “tags” }​ } }} 将文本field的fielddata属性设置为true PUT /ecommerce/_mapping/product{ “properties”: {​ “tags”: {​ “type”: “text”,​ “fielddata”: true​ } }} GET /ecommerce/product/_search{ “size”: 0, “aggs”: {​ “all_tags”: {​ “terms”: { “field”: “tags” }​ } }}{ “took”: 20, “timed_out”: false, “_shards”: {​ “total”: 5,​ “successful”: 5,​ “failed”: 0 }, “hits”: {​ “total”: 4,​ “max_score”: 0,​ “hits”: [] }, “aggregations”: {​ “group_by_tags”: {​ “doc_count_error_upper_bound”: 0,​ “sum_other_doc_count”: 0,​ “buckets”: [​ {​ “key”: “fangzhu”,​ “doc_count”: 2​ },​ {​ “key”: “meibai”,​ “doc_count”: 2​ },​ {​ “key”: “qingxin”,​ “doc_count”: 1​ }​ ]​ } }} 第二个聚合分析的需求：对名称中包含yagao的商品，计算每个tag下的商品数量 GET /ecommerce/product/_search{ “size”: 0, “query”: {​ “match”: {​ “name”: “yagao”​ } }, “aggs”: {​ “all_tags”: {​ “terms”: {​ “field”: “tags”​ }​ } }} 第三个聚合分析的需求：先分组，再算每组的平均值，计算每个tag下的商品的平均价格 GET /ecommerce/product/_search{​ “size”: 0,​ “aggs” : {​ “group_by_tags” : {​ “terms” : { “field” : “tags” },​ “aggs” : {​ “avg_price” : {​ “avg” : { “field” : “price” }​ }​ }​ }​ }} 第四个数据分析需求：计算每个tag下的商品的平均价格，并且按照平均价格降序排序 GET /ecommerce/product/_search{​ “size”: 0,​ “aggs” : {​ “all_tags” : {​ “terms” : { “field” : “tags”, “order”: { “avg_price”: “desc” } },​ “aggs” : {​ “avg_price” : {​ “avg” : { “field” : “price” }​ }​ }​ }​ }} 第五个数据分析需求：按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格 GET /ecommerce/product/_search{ “size”: 0, “aggs”: {​ “group_by_price”: {​ “range”: {​ “field”: “price”,​ “ranges”: [​ {​ “from”: 0,​ “to”: 20​ },​ {​ “from”: 20,​ “to”: 40​ },​ {​ “from”: 40,​ “to”: 50​ }​ ]​ },​ “aggs”: {​ “group_by_tags”: {​ “terms”: {​ “field”: “tags”​ },​ “aggs”: {​ “average_price”: {​ “avg”: {​ “field”: “price”​ }​ }​ }​ }​ }​ } }} 分词器课程大纲 1、什么是分词器 切分词语，normalization（提升recall召回率） 给你一段句子，然后将这段句子拆分成一个一个的单个的单词，同时对每个单词进行normalization（时态转换，单复数转换），分瓷器recall，召回率：搜索的时候，增加能够搜索到的结果的数量 character filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（hello –&gt; hello），&amp; –&gt; and（I&amp;you –&gt; I and you）tokenizer：分词，hello you and me –&gt; hello, you, and, metoken filter：lowercase，stop word，synonymom，dogs –&gt; dog，liked –&gt; like，Tom –&gt; tom，a/the/an –&gt; 干掉，mother –&gt; mom，small –&gt; little 一个分词器，很重要，将一段文本进行各种处理，最后处理好的结果才会拿去建立倒排索引 2、内置分词器的介绍 Set the shape to semi-transparent by calling set_trans(5) standard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard）simple analyzer：set, the, shape, to, semi, transparent, by, calling, set, transwhitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)language analyzer（特定的语言的分词器，比如说，english，英语分词器）：set, shape, semi, transpar, call, set_tran, 5 standard tokenizer：以单词边界进行切分standard token filter：什么都不做lowercase token filter：将所有字母转换为小写stop token filer（默认被禁用）：移除停用词，比如a the it等等 12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; ngramik分词器GET /my_index/_analyze{ “text”: “男子偷上万元发红包求交女友 被抓获时仍然单身”, “analyzer”: “ik_max_word”} 1、ik配置文件 ik配置文件地址：es/plugins/ik/config目录 IKAnalyzer.cfg.xml：用来配置自定义词库main.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起quantifier.dic：放了一些单位相关的词suffix.dic：放了一些后缀surname.dic：中国的姓氏stopword.dic：英文停用词 ik原生最重要的两个配置文件 main.dic：包含了原生的中文词语，会按照这个里面的词语去分词stopword.dic：包含了英文的停用词 停用词，stopword a the and at but 一般，像停用词，会在分词的时候，直接被干掉，不会建立在倒排索引中 2、自定义词库 （1）自己建立词库：每年都会涌现一些特殊的流行词，网红，蓝瘦香菇，喊麦，鬼畜，一般不会在ik的原生词典里 自己补充自己的最新的词语，到ik的词库里面去 IKAnalyzer.cfg.xml：ext_dict，custom/mydict.dic 补充自己的词语，然后需要重启es，才能生效 （2）自己建立停用词库：比如了，的，啥，么，我们可能并不想去建立索引，让人家搜索 custom/ext_stopword.dic，已经有了常用的中文停用词，可以补充自己的停用词，然后重启es 聚合数据分析bucket 数据分组相当于sql的group by metric 对一个数据分组执行的统计 比如说求平均值avg，求最大值max，求最小值min 123456789101112131415161718POST /tvs/sales/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 3000, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2016-05-18&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-07-02&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1200, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-08-19&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 8000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;三星&quot;, &quot;sold_date&quot; : &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2017-02-12&quot; &#125; 统计哪种颜色的电视销量最高 1234567891011GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;popular_colors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;color&quot; &#125; &#125; &#125;&#125; size：只获取聚合结果，而不要执行聚合的原始数据aggs：固定语法，要对一份数据执行分组聚合操作popular_colors：就是对每个aggs，都要起一个名字，这个名字是随机的，你随便取什么都okterms：根据字段的值进行分组field：根据指定的字段的值进行分组 默认的排序规则：按照doc_count降序排序 统计每种颜色电视平均价格,最高价 最低价 总价 12345678910111213141516171819202122232425262728293031323334GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;min_price&quot;: &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;max_price&quot;: &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; select avg(price) from sales group by color histogram(柱形图)：类似于terms，也是进行bucket分组操作，接收一个field，按照这个field的值的各个范围区间分段，进行bucket分组操作 “histogram”:{ “field”: “price”, “interval”: 2000}, interval：2000，划分范围，0~2000，2000~4000，4000~6000，6000~8000，8000~10000，buckets 12345678910111213141516171819GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;:&#123; &quot;price&quot;:&#123; &quot;histogram&quot;:&#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000 &#125;, &quot;aggs&quot;:&#123; &quot;revenue&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 对时间分组 用 data_historam month月 quarter季度 12345678910111213141516171819202122232425GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot;:&#123; &quot;daterange&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;month&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot; : 0, &quot;extended_bounds&quot; : &#123; &quot;min&quot; : &quot;2016-01-01&quot;, &quot;max&quot; : &quot;2017-12-31&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;revenue&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 求出每月销售了多少，销售总额 统计每年各个品牌的销售额 12345678910111213141516171819202122232425262728293031323334353637GET /tvs/sales/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_sold_date&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;year&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot;: 0, &quot;extended_bounds&quot;: &#123; &quot;min&quot;: &quot;2016-01-01&quot;, &quot;max&quot;: &quot;2017-12-31&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;total_sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 相关度评分TF/IDF算法相关度评分算法（relevance score），简单来说就是计算出，一个索引中的文本与搜索文本他们之间的关联匹配程度 Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法 Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关 Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关 Field-length norm：field长度，field越长，相关度越弱 data-elasticsearch自定义方法 Spring Data 的另一个强大功能，是根据方法名称自动实现功能。 比如：你的方法名叫做：findByTitle，那么它就知道你是根据title查询，然后自动帮你完成，无需写实现类。 当然，方法名称要符合一定的约定： Keyword Sample Elasticsearch Query String And findByNameAndPrice {&quot;bool&quot; : {&quot;must&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Or findByNameOrPrice {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Is findByName {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Not findByNameNot {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Between findByPriceBetween {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} LessThanEqual findByPriceLessThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} GreaterThanEqual findByPriceGreaterThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Before findByPriceBefore {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} After findByPriceAfter {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Like findByNameLike {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} StartingWith findByNameStartingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} EndingWith findByNameEndingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;*?&quot;,&quot;analyze_wildcard&quot; : true}}}}} Contains/Containing findByNameContaining {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;**?**&quot;,&quot;analyze_wildcard&quot; : true}}}}} In findByNameIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must&quot; : {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}} ]}}}} NotIn findByNameNotIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;bool&quot; : {&quot;should&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}}}} Near findByStoreNear Not Supported Yet ! True findByAvailableTrue {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} False findByAvailableFalse {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : false}}}} OrderBy findByAvailableTrueOrderByNameDesc {&quot;sort&quot; : [{ &quot;name&quot; : {&quot;order&quot; : &quot;desc&quot;} }],&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} bool：must，must_not，should，组合多个过滤条件（2）bool可以嵌套（3）相当于SQL中的多个and条件 123456789101112131415161718192021222324GET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;:[ &#123;&quot;term&quot;:&#123; &quot;name&quot;:&quot;candy&quot; &#125;&#125;, &#123;&quot;term&quot;:&#123; &quot;name&quot;:&quot;milk&quot; &#125;&#125; ], &quot;must_not&quot;:&#123; &quot;term&quot;:&#123; &quot;desc&quot;:&quot;japan&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; terms相当于 in 123456789101112GET /shop/product/_search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;tags&quot;: [&quot;japan&quot;,&quot;china&quot;] &#125; &#125; &#125; &#125;&#125; 小米 或 华为 12345678GET item/docs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;all&quot;: &quot;小米 华为&quot; &#125; &#125;&#125; 都要匹配 1234567891011GET item/docs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;all&quot;: &#123; &quot;query&quot;: &quot;小米 红米&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 必须至少匹配其中的多少个关键字 1234567891011GET item/docs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;all&quot;: &#123; &quot;query&quot;: &quot;小米 华为 电脑 通讯&quot;, &quot;minimum_should_match&quot;: &quot;75%&quot; &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171package com.example.eesb.es;import com.example.eesb.dao.ItemRepository;import com.example.eesb.model.Item;import org.elasticsearch.index.query.MatchQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.aggregations.Aggregation;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.aggregations.Aggregations;import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;import org.elasticsearch.search.aggregations.bucket.terms.DoubleTerms;import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;import org.elasticsearch.search.aggregations.metrics.avg.InternalAvg;import org.elasticsearch.search.sort.SortBuilders;import org.elasticsearch.search.sort.SortOrder;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Sort;import org.springframework.data.elasticsearch.core.ElasticsearchTemplate;import org.springframework.data.elasticsearch.core.aggregation.AggregatedPage;import org.springframework.data.elasticsearch.core.query.NativeSearchQueryBuilder;import org.springframework.stereotype.Service;import java.util.Arrays;import java.util.List;@Servicepublic class SearchService &#123; @Autowired ElasticsearchTemplate elasticsearchTemplate; @Autowired ItemRepository itemRepository; public void addDocument()&#123; Item item=new Item(); item.setId(1L); item.setTitle(&quot;big rabbit&quot;); itemRepository.save(item); &#125; public void updateDoc()&#123; Item item=new Item(); item.setId(1L); item.setTitle(&quot;大西瓜&quot;); itemRepository.save(item); &#125; public void addDocs()&#123; Item i=new Item(); i.setId(1L); i.setPrice(11.0); i.setBrand(&quot;良品铺子&quot;); i.setTitle(&quot;大西瓜&quot;); Item item=new Item(); item.setId(2L); item.setPrice(222.0); item.setBrand(&quot;三只松鼠&quot;); item.setTitle(&quot;ti小红本&quot;); Item item2=new Item(); item2.setId(3L); item2.setPrice(12.0); item2.setTitle(&quot;大白兔&quot;); item2.setBrand(&quot;良品铺子&quot;); List&lt;Item&gt; items = Arrays.asList(item, item2,i); itemRepository.saveAll(items); &#125; /** * 查找 */ public void search()&#123; Iterable&lt;Item&gt; id = itemRepository.findAll(Sort.by(Sort.Direction.DESC, &quot;id&quot;)); id.forEach(e-&gt;&#123; System.out.println(e); &#125;); &#125; /** * QueryBuilders提供了大量的静态方法，用于生成各种不同类型的查询对象，例如：词条、模糊、通配符等QueryBuilder对象。 */ public void searchBuilder()&#123; MatchQueryBuilder builder1 = new MatchQueryBuilder(&quot;title&quot;,&quot;小红本&quot;); Iterable&lt;Item&gt; search = itemRepository.search(builder1); search.forEach(e-&gt;&#123; System.out.println(e); &#125;); &#125; public void nativesearchquerybuilder()&#123; NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder(); nativeSearchQueryBuilder.withQuery(QueryBuilders.matchQuery(&quot;title&quot;,&quot;ti小红本&quot;)); //QueryBuilders.termQuery(); Page&lt;Item&gt; search = itemRepository.search(nativeSearchQueryBuilder.build()); System.out.println(search.getTotalElements()); System.out.println(search.getTotalElements()); search.forEach(System.out::println); &#125; /** * 分页 */ public void fenye()&#123; NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder(); nativeSearchQueryBuilder.withPageable(PageRequest.of(0,8)) .withSort(SortBuilders.fieldSort(&quot;price&quot;).order(SortOrder.ASC)); Page&lt;Item&gt; search = itemRepository.search(nativeSearchQueryBuilder.build()); System.out.println(search.getTotalElements()); System.out.println(search.getTotalElements()); System.out.println(search.getSize()); System.out.println(search.getNumber()); search.forEach(System.out::println); &#125; /** * 聚合 */ public void aggs()&#123; NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder(); nativeSearchQueryBuilder.addAggregation(AggregationBuilders.terms(&quot;brandagg&quot;).field(&quot;brand&quot;)); AggregatedPage&lt;Item&gt; search = (AggregatedPage&lt;Item&gt;)itemRepository.search(nativeSearchQueryBuilder.build()); StringTerms brandagg = (StringTerms)search.getAggregation(&quot;brandagg&quot;); List&lt;StringTerms.Bucket&gt; buckets = brandagg.getBuckets(); buckets.forEach(e-&gt;&#123; System.out.println(e.getDocCount()); System.out.println(e.getKey()); &#125;); &#125; public void aggsplus()&#123;// 解决办法很简单，在leyou-search的application.yml中添加一行配置，json处理时忽略空值：//// spring:// jackson:// default-property-inclusion: non_null # 配置json处理时忽略空值//// // 1、对key进行全文检索查询// queryBuilder.withQuery(QueryBuilders.matchQuery(&quot;all&quot;, key).operator(Operator.AND));//// // 2、通过sourceFilter设置返回的结果字段,我们只需要id、skus、subTitle// queryBuilder.withSourceFilter(new FetchSourceFilter(// new String[]&#123;&quot;id&quot;,&quot;skus&quot;,&quot;subTitle&quot;&#125;, null)); NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder(); nativeSearchQueryBuilder.addAggregation(AggregationBuilders.histogram(&quot;priceint&quot;).field(&quot;price&quot;).interval(100) .subAggregation(AggregationBuilders.avg(&quot;priceavg&quot;).field(&quot;price&quot;))); AggregatedPage&lt;Item&gt; search = (AggregatedPage&lt;Item&gt;)itemRepository.search(nativeSearchQueryBuilder.build()); InternalHistogram priceint = (InternalHistogram)search.getAggregation(&quot;priceint&quot;); priceint.getBuckets().forEach(e-&gt;&#123; System.out.println(e.getKeyAsString()); System.out.println(e.getDocCount()); InternalAvg priceavg = (InternalAvg)e.getAggregations().asMap().get(&quot;priceavg&quot;); System.out.println(priceavg.getValue()); &#125;); &#125;&#125; 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.leyou.order.mapper.OrderMapper&quot;&gt; &lt;resultMap id=&quot;OrderWithDetail&quot; type=&quot;com.leyou.order.pojo.Order&quot; autoMapping=&quot;true&quot;&gt; &lt;id column=&quot;order_id&quot; property=&quot;orderId&quot;&gt;&lt;/id&gt; &lt;/resultMap&gt; &lt;select id=&quot;queryOrderList&quot; resultMap=&quot;OrderWithDetail&quot;&gt; SELECT o.order_id,o.actual_pay, o.total_pay,o.create_time, os.status FROM tb_order o LEFT JOIN tb_order_status os ON os.order_id = o.order_id WHERE o.user_id = #&#123;userId&#125; &lt;if test=&quot;status != null and status != 0&quot;&gt; AND os.status = #&#123;status&#125; &lt;/if&gt; ORDER BY o.create_time DESC &lt;/select&gt;&lt;/mapper&gt; geo point地理位置数据类型package com.example.eesb.es; import com.example.eesb.dao.ItemRepository;import com.example.eesb.model.Item;import org.elasticsearch.index.query.MatchQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.aggregations.Aggregation;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.aggregations.Aggregations;import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;import org.elasticsearch.search.aggregations.bucket.terms.DoubleTerms;import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;import org.elasticsearch.search.aggregations.metrics.avg.InternalAvg;import org.elasticsearch.search.sort.SortBuilders;import org.elasticsearch.search.sort.SortOrder;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Sort;import org.springframework.data.elasticsearch.core.ElasticsearchTemplate;import org.springframework.data.elasticsearch.core.aggregation.AggregatedPage;import org.springframework.data.elasticsearch.core.query.NativeSearchQueryBuilder;import org.springframework.stereotype.Service; import java.util.Arrays;import java.util.List; @Servicepublic class SearchService {​ @Autowired​ ElasticsearchTemplate elasticsearchTemplate;​ @Autowired​ ItemRepository itemRepository; ​ public void addDocument(){ ​ Item item=new Item();​ item.setId(1L);​ item.setTitle(“big rabbit”);​ itemRepository.save(item);​ } ​ public void updateDoc(){​ Item item=new Item();​ item.setId(1L);​ item.setTitle(“大西瓜”);​ itemRepository.save(item);​ } ​ public void addDocs(){​ Item i=new Item();​ i.setId(1L);​ i.setPrice(11.0);​ i.setBrand(“良品铺子”);​ i.setTitle(“大西瓜”); ​ Item item=new Item();​ item.setId(2L);​ item.setPrice(222.0);​ item.setBrand(“三只松鼠”);​ item.setTitle(“ti小红本”); ​ Item item2=new Item();​ item2.setId(3L);​ item2.setPrice(12.0);​ item2.setTitle(“大白兔”);​ item2.setBrand(“良品铺子”);​ List items = Arrays.asList(item, item2,i);​ itemRepository.saveAll(items);​ } ​ /*​ 查找​ */​ ​ public void search(){ ​ Iterable id = itemRepository.findAll(Sort.by(Sort.Direction.DESC, “id”));​ id.forEach(e-&gt;{​ System.out.println(e);​ });​ } ​ /*​ QueryBuilders提供了大量的静态方法，用于生成各种不同类型的查询对象，例如：词条、模糊、通配符等QueryBuilder对象。​ */​ ​ public void searchBuilder(){​ ​ MatchQueryBuilder builder1 = new MatchQueryBuilder(“title”,”小红本”);​ ​ Iterable search = itemRepository.search(builder1);​ ​ search.forEach(e-&gt;{​ ​ System.out.println(e);​ ​ });​ ​ } ​ public void nativesearchquerybuilder(){ ​ NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder();​ nativeSearchQueryBuilder.withQuery(QueryBuilders.matchQuery(“title”,”ti小红本”));​ //QueryBuilders.termQuery();​ Page search = itemRepository.search(nativeSearchQueryBuilder.build());​ System.out.println(search.getTotalElements());​ System.out.println(search.getTotalElements());​ search.forEach(System.out::println); ​ } ​ /*​ 分页​ */​ ​ public void fenye(){​ ​ NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder();​ ​ nativeSearchQueryBuilder.withPageable(PageRequest.of(0,8))​ .withSort(SortBuilders.fieldSort(“price”).order(SortOrder.ASC));​ ​ Page search = itemRepository.search(nativeSearchQueryBuilder.build());​ ​ System.out.println(search.getTotalElements());​ ​ System.out.println(search.getTotalElements());​ ​ System.out.println(search.getSize());​ ​ System.out.println(search.getNumber());​ ​ search.forEach(System.out::println);​ ​ } ​ /*​ 聚合​ */​ ​ public void aggs(){​ ​ NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder();​ ​ nativeSearchQueryBuilder.addAggregation(AggregationBuilders.terms(“brandagg”).field(“brand”));​ ​ AggregatedPage search = (AggregatedPage)itemRepository.search(nativeSearchQueryBuilder.build());​ ​ StringTerms brandagg = (StringTerms)search.getAggregation(“brandagg”);​ ​ List&lt;StringTerms.Bucket&gt; buckets = brandagg.getBuckets();​ ​ buckets.forEach(e-&gt;{​ ​ System.out.println(e.getDocCount());​ ​ System.out.println(e.getKey());​ ​ });​ ​ } ​ public void aggsplus(){ // 解决办法很简单，在leyou-search的application.yml中添加一行配置，json处理时忽略空值：//// spring:// jackson:// default-property-inclusion: non_null # 配置json处理时忽略空值 //// // 1、对key进行全文检索查询// queryBuilder.withQuery(QueryBuilders.matchQuery(“all”, key).operator(Operator.AND));//// // 2、通过sourceFilter设置返回的结果字段,我们只需要id、skus、subTitle// queryBuilder.withSourceFilter(new FetchSourceFilter(// new String[]{“id”,”skus”,”subTitle”}, null)); ​ NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder();​ nativeSearchQueryBuilder.addAggregation(AggregationBuilders.histogram(“priceint”).field(“price”).interval(100)​ .subAggregation(AggregationBuilders.avg(“priceavg”).field(“price”)));​ AggregatedPage search = (AggregatedPage)itemRepository.search(nativeSearchQueryBuilder.build());​ InternalHistogram priceint = (InternalHistogram)search.getAggregation(“priceint”);​ priceint.getBuckets().forEach(e-&gt;{​ System.out.println(e.getKeyAsString());​ System.out.println(e.getDocCount());​ InternalAvg priceavg = (InternalAvg)e.getAggregations().asMap().get(“priceavg”);​ System.out.println(priceavg.getValue());​ });​ } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361、建立geo_point类型的mapping第一个地理位置的数据类型，就是geo_point，geo_point，说白了，就是一个地理位置坐标点，包含了一个经度，一个维度，经纬度，就可以唯一定位一个地球上的坐标PUT /my_index &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125; &#125; &#125;&#125;2、写入geo_point的3种方法PUT my_index/my_type/1&#123; &quot;text&quot;: &quot;Geo-point as an object&quot;, &quot;location&quot;: &#123; &quot;lat&quot;: 41.12, &quot;lon&quot;: -71.34 &#125;&#125;latitude：纬度longitude：经度3、根据地理位置进行查询最最简单的，根据地理位置查询一些点，比如说，下面geo_bounding_box查询，查询某个矩形的地理位置范围内的坐标点GET /my_index/my_type/_search &#123; &quot;query&quot;: &#123; &quot;geo_bounding_box&quot;: &#123; &quot;location&quot;: &#123; &quot;top_left&quot;: &#123; &quot;lat&quot;: 42, &quot;lon&quot;: -72 &#125;, &quot;bottom_right&quot;: &#123; &quot;lat&quot;: 40, &quot;lon&quot;: -74 &#125; &#125; &#125; &#125;&#125;// 多边形范围GET /hotel_app/hotels/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125; &#125; ], &quot;filter&quot;: &#123; &quot;geo_polygon&quot;: &#123; &quot;pin.location&quot;: &#123; &quot;points&quot;: [ &#123;&quot;lat&quot; : 40.73, &quot;lon&quot; : -74.1&#125;, &#123;&quot;lat&quot; : 40.01, &quot;lon&quot; : -71.12&#125;, &#123;&quot;lat&quot; : 50.56, &quot;lon&quot; : -90.58&#125; ] &#125; &#125; &#125; &#125; &#125;&#125;// 距离多少GET /hotel_app/hotels/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125; &#125; ], &quot;filter&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;distance&quot;: &quot;200km&quot;, &quot;pin.location&quot;: &#123; &quot;lat&quot;: 40, &quot;lon&quot;: -70 &#125; &#125; &#125; &#125; &#125;&#125;0~100m有几个酒店，100m~300m有几个酒店，300m以上有几个酒店GET /hotel_app/hotels/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;agg_by_distance_range&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;field&quot;: &quot;pin.location&quot;, &quot;origin&quot;: &#123; &quot;lat&quot;: 40, &quot;lon&quot;: -70 &#125;, &quot;unit&quot;: &quot;mi&quot;, &quot;ranges&quot;: [ &#123; &quot;to&quot;: 100 &#125;, &#123; &quot;from&quot;: 100, &quot;to&quot;: 300 &#125;, &#123; &quot;from&quot;: 300 &#125; ] &#125; &#125; &#125;&#125; 资料 《Elasticsearch索引原理》 死磕es elasticsearch资源汇总 awesome-elasticsearch 插件汇总 ES权威指南中文 ES权威指南英文 ES安全search-guard elasticsearch相关介绍 solr和elasticsearch对比 将 ELASTICSEARCH 写入速度优化到极限 企业使用案例 使用Akka、Kafka和ElasticSearch等构建分析引擎 用Elasticsearch构建电商搜索平台，一个极有代表性的基础技术架构和算法实践案例 用Elasticsearch+Redis构建投诉监控系统，看Airbnb如何保证用户持续增长 基于Elasticsearch构建千亿流量日志搜索平台实战 Elasticsearch作为时间序列数据库 索引原理 ES原理 docvalues 介绍 ElasticSearch存储文件解析 问题解决 如何防止elasticsearch的脑裂问题 jar conflic 通过maven-shade-plugin 解决Elasticsearch与hbase的jar包冲突问题 rally性能测试 nested 和parent使用介绍 sql操作es 《从零开始搭建一个ELKB日志收集系统》]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
</search>
